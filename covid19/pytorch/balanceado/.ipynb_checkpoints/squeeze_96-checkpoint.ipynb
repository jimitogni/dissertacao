{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.5.0\n",
      "Torchvision Version:  0.6.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy \n",
    "import random\n",
    "import shutil, sys\n",
    "import pandas as pd\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = '/home/jimi/dissertacao/covid19/datasets/dataset_bal/'\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"squeezenet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 3\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 8\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 200\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files per classes\n",
      "----------------------------------------\n",
      "normal :  505\n",
      "pneumonia :  505\n",
      "covid :  505\n",
      "--------------------\n",
      "Train, test, validation\n",
      "--------------------\n",
      "len_train_dir :  969\n",
      "len_test_dir :  303\n",
      "len_val_dir :  243\n"
     ]
    }
   ],
   "source": [
    "# Path to data\n",
    "data_dir = '/home/jimi/dissertacao/covid19/datasets/dataset_bal/'\n",
    "train_dir = data_dir+'train/'\n",
    "test_dir = data_dir+'test/'\n",
    "val_dir = data_dir+'val/'\n",
    "\n",
    "normal_dir = data_dir+'normal/'\n",
    "pneumonia_dir = data_dir+'pneumonia/'\n",
    "covid_dir = data_dir+'covid/'\n",
    "\n",
    "len_covid = len([iq for iq in os.scandir(normal_dir)])\n",
    "len_normal = len([iq for iq in os.scandir(pneumonia_dir)])\n",
    "len_pneumonia = len([iq for iq in os.scandir(covid_dir)])\n",
    "\n",
    "len_train_dir = len([iq for iq in os.scandir(train_dir+'covid/')]) + len([iq for iq in os.scandir(train_dir+'normal/')]) + len([iq for iq in os.scandir(train_dir+'pneumonia/')])\n",
    "len_test_dir = len([iq for iq in os.scandir(test_dir+'covid/')]) + len([iq for iq in os.scandir(test_dir+'normal/')]) + len([iq for iq in os.scandir(test_dir+'pneumonia/')])\n",
    "len_val_dir = len([iq for iq in os.scandir(val_dir+'covid/')]) + len([iq for iq in os.scandir(val_dir+'normal/')]) + len([iq for iq in os.scandir(val_dir+'pneumonia/')])\n",
    "\n",
    "print('Files per classes')\n",
    "print(\"----\"*10)\n",
    "print(\"normal : \", len_covid)\n",
    "print(\"pneumonia : \", len_normal)\n",
    "print(\"covid : \", len_pneumonia)\n",
    "print(\"-\"*20)\n",
    "print('Train, test, validation')\n",
    "print(\"-\"*20)\n",
    "print(\"len_train_dir : \", len_train_dir)\n",
    "print(\"len_test_dir : \", len_test_dir)\n",
    "print(\"len_val_dir : \", len_val_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SqueezeNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (3): Fire(\n",
      "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Fire(\n",
      "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Fire(\n",
      "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (7): Fire(\n",
      "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (8): Fire(\n",
      "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (9): Fire(\n",
      "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (10): Fire(\n",
      "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (12): Fire(\n",
      "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Conv2d(512, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "#transforms.RandomRotation(degrees=(-5, 5)),\n",
    "#transforms.ColorJitter(brightness=.02),\n",
    "    \n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t classifier.1.weight\n",
      "\t classifier.1.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Squeezenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 0.8399 Acc: 0.6254\n",
      "val Loss: 0.3795 Acc: 0.8930\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 0.5654 Acc: 0.7699\n",
      "val Loss: 0.2710 Acc: 0.9259\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 0.4659 Acc: 0.8194\n",
      "val Loss: 0.2726 Acc: 0.9259\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 0.3845 Acc: 0.8555\n",
      "val Loss: 0.3999 Acc: 0.8724\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 0.3527 Acc: 0.8658\n",
      "val Loss: 0.3947 Acc: 0.8971\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 0.3664 Acc: 0.8483\n",
      "val Loss: 0.3523 Acc: 0.9218\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 0.3485 Acc: 0.8576\n",
      "val Loss: 0.2888 Acc: 0.9383\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 0.2638 Acc: 0.9082\n",
      "val Loss: 0.2531 Acc: 0.9465\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 0.3411 Acc: 0.8669\n",
      "val Loss: 0.2487 Acc: 0.9259\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 0.2962 Acc: 0.8968\n",
      "val Loss: 0.2302 Acc: 0.9218\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 0.3251 Acc: 0.8782\n",
      "val Loss: 0.3381 Acc: 0.9218\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 0.3078 Acc: 0.8793\n",
      "val Loss: 0.2582 Acc: 0.9342\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 0.2859 Acc: 0.8947\n",
      "val Loss: 0.2662 Acc: 0.9424\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 0.2620 Acc: 0.9092\n",
      "val Loss: 0.3159 Acc: 0.9095\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 0.2501 Acc: 0.9040\n",
      "val Loss: 0.2607 Acc: 0.8971\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 0.2506 Acc: 0.9030\n",
      "val Loss: 0.2189 Acc: 0.9177\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 0.3170 Acc: 0.8772\n",
      "val Loss: 0.2222 Acc: 0.9383\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 0.2570 Acc: 0.9164\n",
      "val Loss: 0.3044 Acc: 0.9259\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 0.2676 Acc: 0.8968\n",
      "val Loss: 0.3258 Acc: 0.9095\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 0.2659 Acc: 0.9040\n",
      "val Loss: 0.3491 Acc: 0.9053\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 0.2680 Acc: 0.8968\n",
      "val Loss: 0.3776 Acc: 0.9095\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 0.2472 Acc: 0.9143\n",
      "val Loss: 0.2188 Acc: 0.9259\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 0.2574 Acc: 0.9143\n",
      "val Loss: 0.2510 Acc: 0.9383\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 0.2464 Acc: 0.9102\n",
      "val Loss: 0.3415 Acc: 0.9012\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 0.2373 Acc: 0.9071\n",
      "val Loss: 0.3781 Acc: 0.8724\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 0.2732 Acc: 0.8978\n",
      "val Loss: 0.4247 Acc: 0.8642\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 0.2549 Acc: 0.9092\n",
      "val Loss: 0.2111 Acc: 0.9506\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 0.2379 Acc: 0.9143\n",
      "val Loss: 0.2686 Acc: 0.9383\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 0.2435 Acc: 0.9112\n",
      "val Loss: 0.3925 Acc: 0.8477\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 0.2535 Acc: 0.9040\n",
      "val Loss: 0.2486 Acc: 0.9424\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 0.2430 Acc: 0.9133\n",
      "val Loss: 0.2972 Acc: 0.9259\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 0.2399 Acc: 0.9195\n",
      "val Loss: 0.2561 Acc: 0.9383\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 0.2398 Acc: 0.9174\n",
      "val Loss: 0.1731 Acc: 0.9465\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 0.2242 Acc: 0.9071\n",
      "val Loss: 0.1915 Acc: 0.9506\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 0.2423 Acc: 0.9133\n",
      "val Loss: 0.3215 Acc: 0.9095\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 0.2542 Acc: 0.9061\n",
      "val Loss: 0.2755 Acc: 0.9177\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 0.2577 Acc: 0.8865\n",
      "val Loss: 0.3434 Acc: 0.9053\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 0.2931 Acc: 0.8875\n",
      "val Loss: 0.3262 Acc: 0.9218\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 0.2431 Acc: 0.9185\n",
      "val Loss: 0.2597 Acc: 0.9383\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 0.2379 Acc: 0.9164\n",
      "val Loss: 0.3755 Acc: 0.8930\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 0.2422 Acc: 0.9030\n",
      "val Loss: 0.1835 Acc: 0.9465\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 0.2324 Acc: 0.9123\n",
      "val Loss: 0.2981 Acc: 0.9300\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 0.2255 Acc: 0.9112\n",
      "val Loss: 0.3729 Acc: 0.9095\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 0.2612 Acc: 0.8958\n",
      "val Loss: 0.3079 Acc: 0.9259\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 0.2482 Acc: 0.9061\n",
      "val Loss: 0.2712 Acc: 0.9383\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 0.2386 Acc: 0.9133\n",
      "val Loss: 0.2099 Acc: 0.9424\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 0.2613 Acc: 0.9051\n",
      "val Loss: 0.2867 Acc: 0.9300\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 0.2195 Acc: 0.9164\n",
      "val Loss: 0.3203 Acc: 0.9053\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 0.2147 Acc: 0.9226\n",
      "val Loss: 0.2352 Acc: 0.9424\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 0.2251 Acc: 0.9185\n",
      "val Loss: 0.2046 Acc: 0.9465\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 0.2255 Acc: 0.9236\n",
      "val Loss: 0.2758 Acc: 0.9383\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 0.2222 Acc: 0.9226\n",
      "val Loss: 0.2547 Acc: 0.9424\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 0.1884 Acc: 0.9288\n",
      "val Loss: 0.2045 Acc: 0.9506\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 0.2264 Acc: 0.9247\n",
      "val Loss: 0.2621 Acc: 0.9300\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 0.2424 Acc: 0.9092\n",
      "val Loss: 0.2762 Acc: 0.9218\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 0.2047 Acc: 0.9247\n",
      "val Loss: 0.1810 Acc: 0.9465\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 0.2248 Acc: 0.9236\n",
      "val Loss: 0.2564 Acc: 0.9383\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 0.1932 Acc: 0.9267\n",
      "val Loss: 0.1735 Acc: 0.9547\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 0.2214 Acc: 0.9164\n",
      "val Loss: 0.3054 Acc: 0.9300\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 0.2065 Acc: 0.9195\n",
      "val Loss: 0.1795 Acc: 0.9506\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 0.2549 Acc: 0.9040\n",
      "val Loss: 0.2348 Acc: 0.9506\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 0.2265 Acc: 0.9205\n",
      "val Loss: 0.2051 Acc: 0.9506\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 0.2282 Acc: 0.9205\n",
      "val Loss: 0.2546 Acc: 0.9383\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 0.2046 Acc: 0.9247\n",
      "val Loss: 0.3191 Acc: 0.9136\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 0.2465 Acc: 0.9123\n",
      "val Loss: 0.1990 Acc: 0.9506\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 0.2009 Acc: 0.9216\n",
      "val Loss: 0.2950 Acc: 0.9383\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 0.2057 Acc: 0.9278\n",
      "val Loss: 0.3985 Acc: 0.8848\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 0.2335 Acc: 0.9133\n",
      "val Loss: 0.1677 Acc: 0.9547\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 0.2315 Acc: 0.9195\n",
      "val Loss: 0.2390 Acc: 0.9424\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 0.2262 Acc: 0.9112\n",
      "val Loss: 0.2609 Acc: 0.9465\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 0.2239 Acc: 0.9123\n",
      "val Loss: 0.2597 Acc: 0.9383\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 0.2212 Acc: 0.9133\n",
      "val Loss: 0.2662 Acc: 0.9424\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 0.1982 Acc: 0.9236\n",
      "val Loss: 0.2803 Acc: 0.9300\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 0.2420 Acc: 0.9040\n",
      "val Loss: 0.4395 Acc: 0.8683\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 0.2203 Acc: 0.9174\n",
      "val Loss: 0.1955 Acc: 0.9547\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 0.2223 Acc: 0.9143\n",
      "val Loss: 0.3778 Acc: 0.9053\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 0.2514 Acc: 0.8999\n",
      "val Loss: 0.2296 Acc: 0.9383\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 0.2341 Acc: 0.9071\n",
      "val Loss: 0.3377 Acc: 0.9218\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 0.2143 Acc: 0.9154\n",
      "val Loss: 0.2618 Acc: 0.9424\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 0.2441 Acc: 0.9133\n",
      "val Loss: 0.2282 Acc: 0.9383\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 0.1887 Acc: 0.9340\n",
      "val Loss: 0.2835 Acc: 0.9300\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 0.2240 Acc: 0.9102\n",
      "val Loss: 0.2200 Acc: 0.9424\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 0.2216 Acc: 0.9143\n",
      "val Loss: 0.2813 Acc: 0.9342\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 0.1949 Acc: 0.9278\n",
      "val Loss: 0.2561 Acc: 0.9424\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 0.2181 Acc: 0.9133\n",
      "val Loss: 0.2638 Acc: 0.9383\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 0.2056 Acc: 0.9226\n",
      "val Loss: 0.2486 Acc: 0.9342\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 0.2437 Acc: 0.9174\n",
      "val Loss: 0.4447 Acc: 0.8683\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 0.2065 Acc: 0.9205\n",
      "val Loss: 0.2377 Acc: 0.9424\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 0.2296 Acc: 0.9061\n",
      "val Loss: 0.2247 Acc: 0.9506\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 0.2004 Acc: 0.9143\n",
      "val Loss: 0.4249 Acc: 0.8807\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 0.2083 Acc: 0.9143\n",
      "val Loss: 0.1946 Acc: 0.9588\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 0.1876 Acc: 0.9236\n",
      "val Loss: 0.2420 Acc: 0.9547\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 0.2051 Acc: 0.9247\n",
      "val Loss: 0.2303 Acc: 0.9506\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 0.2189 Acc: 0.9205\n",
      "val Loss: 0.3202 Acc: 0.9300\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 0.1802 Acc: 0.9381\n",
      "val Loss: 0.1729 Acc: 0.9630\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 0.1981 Acc: 0.9267\n",
      "val Loss: 0.2386 Acc: 0.9342\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 0.1966 Acc: 0.9185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.3771 Acc: 0.9012\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 0.2036 Acc: 0.9143\n",
      "val Loss: 0.2437 Acc: 0.9465\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 0.2027 Acc: 0.9185\n",
      "val Loss: 0.2368 Acc: 0.9383\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 0.1982 Acc: 0.9360\n",
      "val Loss: 0.2598 Acc: 0.9465\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 0.1648 Acc: 0.9329\n",
      "val Loss: 0.1923 Acc: 0.9547\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 0.2262 Acc: 0.9216\n",
      "val Loss: 0.2072 Acc: 0.9465\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 0.1879 Acc: 0.9216\n",
      "val Loss: 0.2112 Acc: 0.9547\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 0.2407 Acc: 0.9102\n",
      "val Loss: 0.2415 Acc: 0.9424\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 0.2151 Acc: 0.9267\n",
      "val Loss: 0.2904 Acc: 0.9342\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 0.2222 Acc: 0.9216\n",
      "val Loss: 0.2389 Acc: 0.9506\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 0.2241 Acc: 0.9205\n",
      "val Loss: 0.2555 Acc: 0.9259\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 0.2080 Acc: 0.9236\n",
      "val Loss: 0.2099 Acc: 0.9506\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 0.2071 Acc: 0.9288\n",
      "val Loss: 0.1649 Acc: 0.9547\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 0.1906 Acc: 0.9412\n",
      "val Loss: 0.2458 Acc: 0.9300\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 0.1823 Acc: 0.9309\n",
      "val Loss: 0.2207 Acc: 0.9465\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 0.2369 Acc: 0.9082\n",
      "val Loss: 0.3165 Acc: 0.9300\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 0.1992 Acc: 0.9226\n",
      "val Loss: 0.3511 Acc: 0.9218\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 0.2203 Acc: 0.9205\n",
      "val Loss: 0.2331 Acc: 0.9465\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 0.2088 Acc: 0.9236\n",
      "val Loss: 0.2594 Acc: 0.9342\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 0.2083 Acc: 0.9216\n",
      "val Loss: 0.1899 Acc: 0.9547\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 0.2082 Acc: 0.9236\n",
      "val Loss: 0.3002 Acc: 0.9053\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 0.2191 Acc: 0.9154\n",
      "val Loss: 0.3410 Acc: 0.9136\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 0.2007 Acc: 0.9360\n",
      "val Loss: 0.2136 Acc: 0.9547\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 0.2126 Acc: 0.9267\n",
      "val Loss: 0.2527 Acc: 0.9424\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 0.2182 Acc: 0.9216\n",
      "val Loss: 0.1427 Acc: 0.9630\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 0.2388 Acc: 0.9123\n",
      "val Loss: 0.2167 Acc: 0.9506\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 0.1923 Acc: 0.9309\n",
      "val Loss: 0.1581 Acc: 0.9588\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 0.1942 Acc: 0.9216\n",
      "val Loss: 0.2436 Acc: 0.9424\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 0.2294 Acc: 0.9195\n",
      "val Loss: 0.2675 Acc: 0.9342\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 0.2053 Acc: 0.9195\n",
      "val Loss: 0.2214 Acc: 0.9465\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 0.1927 Acc: 0.9236\n",
      "val Loss: 0.2119 Acc: 0.9547\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 0.1976 Acc: 0.9288\n",
      "val Loss: 0.2502 Acc: 0.9506\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 0.2158 Acc: 0.9174\n",
      "val Loss: 0.1879 Acc: 0.9506\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 0.2070 Acc: 0.9195\n",
      "val Loss: 0.2522 Acc: 0.9383\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 0.1819 Acc: 0.9350\n",
      "val Loss: 0.1986 Acc: 0.9547\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 0.1800 Acc: 0.9236\n",
      "val Loss: 0.2483 Acc: 0.9383\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 0.1878 Acc: 0.9226\n",
      "val Loss: 0.2205 Acc: 0.9547\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 0.1856 Acc: 0.9350\n",
      "val Loss: 0.2065 Acc: 0.9547\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 0.2202 Acc: 0.9185\n",
      "val Loss: 0.2726 Acc: 0.9218\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 0.1947 Acc: 0.9278\n",
      "val Loss: 0.3210 Acc: 0.9259\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 0.1978 Acc: 0.9381\n",
      "val Loss: 0.2366 Acc: 0.9506\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 0.2249 Acc: 0.9143\n",
      "val Loss: 0.2026 Acc: 0.9547\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 0.1979 Acc: 0.9288\n",
      "val Loss: 0.2471 Acc: 0.9424\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 0.2214 Acc: 0.9143\n",
      "val Loss: 0.1630 Acc: 0.9630\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 0.2027 Acc: 0.9247\n",
      "val Loss: 0.2437 Acc: 0.9547\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 0.1925 Acc: 0.9288\n",
      "val Loss: 0.2443 Acc: 0.9506\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 0.1895 Acc: 0.9309\n",
      "val Loss: 0.1930 Acc: 0.9465\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 0.2173 Acc: 0.9195\n",
      "val Loss: 0.2746 Acc: 0.9300\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 0.2224 Acc: 0.9123\n",
      "val Loss: 0.1721 Acc: 0.9465\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 0.2231 Acc: 0.9216\n",
      "val Loss: 0.1572 Acc: 0.9588\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 0.2167 Acc: 0.9247\n",
      "val Loss: 0.2407 Acc: 0.9547\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 0.1936 Acc: 0.9236\n",
      "val Loss: 0.1746 Acc: 0.9547\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 0.1841 Acc: 0.9360\n",
      "val Loss: 0.2527 Acc: 0.9383\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 0.2124 Acc: 0.9216\n",
      "val Loss: 0.2254 Acc: 0.9465\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 0.2169 Acc: 0.9278\n",
      "val Loss: 0.1375 Acc: 0.9671\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 0.1883 Acc: 0.9236\n",
      "val Loss: 0.2142 Acc: 0.9547\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 0.2023 Acc: 0.9195\n",
      "val Loss: 0.1669 Acc: 0.9506\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 0.1883 Acc: 0.9267\n",
      "val Loss: 0.2352 Acc: 0.9465\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 0.1917 Acc: 0.9319\n",
      "val Loss: 0.2532 Acc: 0.9424\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 0.2011 Acc: 0.9267\n",
      "val Loss: 0.2006 Acc: 0.9588\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 0.2091 Acc: 0.9164\n",
      "val Loss: 0.2047 Acc: 0.9424\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 0.2164 Acc: 0.9185\n",
      "val Loss: 0.3112 Acc: 0.9136\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 0.2168 Acc: 0.9185\n",
      "val Loss: 0.2027 Acc: 0.9547\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 0.1714 Acc: 0.9360\n",
      "val Loss: 0.2250 Acc: 0.9506\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 0.1804 Acc: 0.9319\n",
      "val Loss: 0.1850 Acc: 0.9506\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 0.1808 Acc: 0.9236\n",
      "val Loss: 0.2152 Acc: 0.9424\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 0.1841 Acc: 0.9298\n",
      "val Loss: 0.1898 Acc: 0.9506\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 0.2311 Acc: 0.9174\n",
      "val Loss: 0.1852 Acc: 0.9547\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 0.1911 Acc: 0.9381\n",
      "val Loss: 0.2136 Acc: 0.9588\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 0.1789 Acc: 0.9267\n",
      "val Loss: 0.2393 Acc: 0.9424\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 0.1807 Acc: 0.9340\n",
      "val Loss: 0.2480 Acc: 0.9218\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 0.1706 Acc: 0.9401\n",
      "val Loss: 0.2154 Acc: 0.9506\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 0.1898 Acc: 0.9340\n",
      "val Loss: 0.1760 Acc: 0.9547\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 0.1789 Acc: 0.9412\n",
      "val Loss: 0.2680 Acc: 0.9424\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 0.1670 Acc: 0.9340\n",
      "val Loss: 0.3931 Acc: 0.9053\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 0.1905 Acc: 0.9247\n",
      "val Loss: 0.2244 Acc: 0.9547\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 0.1818 Acc: 0.9381\n",
      "val Loss: 0.2604 Acc: 0.9342\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 0.1692 Acc: 0.9381\n",
      "val Loss: 0.1817 Acc: 0.9506\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 0.2306 Acc: 0.9071\n",
      "val Loss: 0.2128 Acc: 0.9465\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 0.1809 Acc: 0.9278\n",
      "val Loss: 0.2162 Acc: 0.9547\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 0.2085 Acc: 0.9288\n",
      "val Loss: 0.2184 Acc: 0.9547\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 0.1985 Acc: 0.9205\n",
      "val Loss: 0.1775 Acc: 0.9547\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 0.1748 Acc: 0.9309\n",
      "val Loss: 0.1781 Acc: 0.9506\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 0.2344 Acc: 0.9174\n",
      "val Loss: 0.1964 Acc: 0.9588\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 0.2013 Acc: 0.9133\n",
      "val Loss: 0.2635 Acc: 0.9424\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 0.1908 Acc: 0.9340\n",
      "val Loss: 0.2411 Acc: 0.9383\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 0.1917 Acc: 0.9319\n",
      "val Loss: 0.2613 Acc: 0.9465\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 0.1757 Acc: 0.9381\n",
      "val Loss: 0.2473 Acc: 0.9383\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 0.1919 Acc: 0.9370\n",
      "val Loss: 0.2237 Acc: 0.9506\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 0.1776 Acc: 0.9432\n",
      "val Loss: 0.2527 Acc: 0.9383\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 0.1960 Acc: 0.9278\n",
      "val Loss: 0.1797 Acc: 0.9506\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 0.2094 Acc: 0.9226\n",
      "val Loss: 0.1861 Acc: 0.9506\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 0.2128 Acc: 0.9216\n",
      "val Loss: 0.1958 Acc: 0.9506\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 0.2035 Acc: 0.9278\n",
      "val Loss: 0.1427 Acc: 0.9506\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 0.1961 Acc: 0.9247\n",
      "val Loss: 0.2683 Acc: 0.9424\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 0.1877 Acc: 0.9236\n",
      "val Loss: 0.2427 Acc: 0.9300\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2021 Acc: 0.9154\n",
      "val Loss: 0.3153 Acc: 0.9218\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 0.1732 Acc: 0.9370\n",
      "val Loss: 0.2234 Acc: 0.9465\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 0.1834 Acc: 0.9298\n",
      "val Loss: 0.2272 Acc: 0.9547\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 0.1562 Acc: 0.9422\n",
      "val Loss: 0.2346 Acc: 0.9547\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 0.1793 Acc: 0.9360\n",
      "val Loss: 0.1449 Acc: 0.9547\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 0.2327 Acc: 0.9174\n",
      "val Loss: 0.1849 Acc: 0.9465\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 0.2322 Acc: 0.9154\n",
      "val Loss: 0.2255 Acc: 0.9506\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 0.2062 Acc: 0.9236\n",
      "val Loss: 0.1511 Acc: 0.9506\n",
      "\n",
      "Training complete in 113m 39s\n",
      "Best val Acc: 0.967078\n"
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, \n",
    "                             optimizer_ft, num_epochs=num_epochs, \n",
    "                             is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
