{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.5.0\n",
      "Torchvision Version:  0.6.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy \n",
    "import random\n",
    "import shutil, sys\n",
    "import pandas as pd\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = '/home/jimi/dissertacao/covid19/datasets/dataset_bal/'\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"vgg\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 3\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 8\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 200\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files per classes\n",
      "----------------------------------------\n",
      "normal :  505\n",
      "pneumonia :  505\n",
      "covid :  505\n",
      "--------------------\n",
      "Train, test, validation\n",
      "--------------------\n",
      "len_train_dir :  969\n",
      "len_test_dir :  303\n",
      "len_val_dir :  243\n"
     ]
    }
   ],
   "source": [
    "# Path to data\n",
    "data_dir = '/home/jimi/dissertacao/covid19/datasets/dataset_bal/'\n",
    "train_dir = data_dir+'train/'\n",
    "test_dir = data_dir+'test/'\n",
    "val_dir = data_dir+'val/'\n",
    "\n",
    "normal_dir = data_dir+'normal/'\n",
    "pneumonia_dir = data_dir+'pneumonia/'\n",
    "covid_dir = data_dir+'covid/'\n",
    "\n",
    "len_covid = len([iq for iq in os.scandir(normal_dir)])\n",
    "len_normal = len([iq for iq in os.scandir(pneumonia_dir)])\n",
    "len_pneumonia = len([iq for iq in os.scandir(covid_dir)])\n",
    "\n",
    "len_train_dir = len([iq for iq in os.scandir(train_dir+'covid/')]) + len([iq for iq in os.scandir(train_dir+'normal/')]) + len([iq for iq in os.scandir(train_dir+'pneumonia/')])\n",
    "len_test_dir = len([iq for iq in os.scandir(test_dir+'covid/')]) + len([iq for iq in os.scandir(test_dir+'normal/')]) + len([iq for iq in os.scandir(test_dir+'pneumonia/')])\n",
    "len_val_dir = len([iq for iq in os.scandir(val_dir+'covid/')]) + len([iq for iq in os.scandir(val_dir+'normal/')]) + len([iq for iq in os.scandir(val_dir+'pneumonia/')])\n",
    "\n",
    "print('Files per classes')\n",
    "print(\"----\"*10)\n",
    "print(\"normal : \", len_covid)\n",
    "print(\"pneumonia : \", len_normal)\n",
    "print(\"covid : \", len_pneumonia)\n",
    "print(\"-\"*20)\n",
    "print('Train, test, validation')\n",
    "print(\"-\"*20)\n",
    "print(\"len_train_dir : \", len_train_dir)\n",
    "print(\"len_test_dir : \", len_test_dir)\n",
    "print(\"len_val_dir : \", len_val_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (24): ReLU(inplace=True)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "#transforms.RandomRotation(degrees=(-5, 5)),\n",
    "#transforms.ColorJitter(brightness=.02),\n",
    "    \n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Squeezenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 0.6530 Acc: 0.6945\n",
      "val Loss: 0.3714 Acc: 0.8683\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 0.4475 Acc: 0.8101\n",
      "val Loss: 0.3303 Acc: 0.8848\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 0.4610 Acc: 0.8256\n",
      "val Loss: 0.2740 Acc: 0.9218\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 0.3790 Acc: 0.8524\n",
      "val Loss: 0.2883 Acc: 0.9012\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 0.4563 Acc: 0.8173\n",
      "val Loss: 0.2580 Acc: 0.9342\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 0.4195 Acc: 0.8328\n",
      "val Loss: 0.2692 Acc: 0.9218\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 0.4302 Acc: 0.8256\n",
      "val Loss: 0.2318 Acc: 0.9259\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 0.3948 Acc: 0.8380\n",
      "val Loss: 0.2211 Acc: 0.9259\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 0.3965 Acc: 0.8349\n",
      "val Loss: 0.2683 Acc: 0.9259\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 0.4042 Acc: 0.8421\n",
      "val Loss: 0.2224 Acc: 0.9300\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 0.3998 Acc: 0.8442\n",
      "val Loss: 0.2230 Acc: 0.9259\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 0.4084 Acc: 0.8586\n",
      "val Loss: 0.2608 Acc: 0.9300\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 0.4212 Acc: 0.8400\n",
      "val Loss: 0.2400 Acc: 0.9095\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 0.3521 Acc: 0.8679\n",
      "val Loss: 0.2139 Acc: 0.9218\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 0.3742 Acc: 0.8514\n",
      "val Loss: 0.1966 Acc: 0.9259\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 0.4021 Acc: 0.8514\n",
      "val Loss: 0.2083 Acc: 0.9383\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 0.3980 Acc: 0.8421\n",
      "val Loss: 0.2669 Acc: 0.9259\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 0.3689 Acc: 0.8555\n",
      "val Loss: 0.2660 Acc: 0.9177\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 0.3733 Acc: 0.8524\n",
      "val Loss: 0.2011 Acc: 0.9259\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 0.3921 Acc: 0.8648\n",
      "val Loss: 0.2214 Acc: 0.9136\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 0.3751 Acc: 0.8638\n",
      "val Loss: 0.1799 Acc: 0.9465\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 0.3686 Acc: 0.8596\n",
      "val Loss: 0.2233 Acc: 0.9177\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 0.3607 Acc: 0.8648\n",
      "val Loss: 0.2261 Acc: 0.9012\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 0.4013 Acc: 0.8607\n",
      "val Loss: 0.2123 Acc: 0.9342\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 0.3957 Acc: 0.8504\n",
      "val Loss: 0.1504 Acc: 0.9342\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 0.3636 Acc: 0.8596\n",
      "val Loss: 0.1913 Acc: 0.9300\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 0.4163 Acc: 0.8411\n",
      "val Loss: 0.2035 Acc: 0.9012\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 0.3965 Acc: 0.8308\n",
      "val Loss: 0.2148 Acc: 0.9383\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 0.3997 Acc: 0.8483\n",
      "val Loss: 0.2396 Acc: 0.9218\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 0.4192 Acc: 0.8462\n",
      "val Loss: 0.1966 Acc: 0.9300\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 0.3552 Acc: 0.8669\n",
      "val Loss: 0.1854 Acc: 0.9259\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 0.3407 Acc: 0.8751\n",
      "val Loss: 0.1693 Acc: 0.9300\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 0.3959 Acc: 0.8483\n",
      "val Loss: 0.1974 Acc: 0.9342\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 0.4451 Acc: 0.8297\n",
      "val Loss: 0.2161 Acc: 0.9300\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 0.4230 Acc: 0.8421\n",
      "val Loss: 0.2159 Acc: 0.9053\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 0.4167 Acc: 0.8452\n",
      "val Loss: 0.1842 Acc: 0.9342\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 0.4092 Acc: 0.8442\n",
      "val Loss: 0.1877 Acc: 0.9300\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 0.3692 Acc: 0.8483\n",
      "val Loss: 0.1907 Acc: 0.9342\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 0.3703 Acc: 0.8679\n",
      "val Loss: 0.2173 Acc: 0.9383\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 0.4088 Acc: 0.8504\n",
      "val Loss: 0.2325 Acc: 0.9177\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 0.4018 Acc: 0.8535\n",
      "val Loss: 0.2740 Acc: 0.9259\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 0.4047 Acc: 0.8535\n",
      "val Loss: 0.2052 Acc: 0.9300\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 0.3907 Acc: 0.8514\n",
      "val Loss: 0.2487 Acc: 0.9218\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 0.4060 Acc: 0.8493\n",
      "val Loss: 0.2073 Acc: 0.9383\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 0.3912 Acc: 0.8535\n",
      "val Loss: 0.1910 Acc: 0.9424\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 0.3881 Acc: 0.8483\n",
      "val Loss: 0.1917 Acc: 0.9383\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 0.4186 Acc: 0.8514\n",
      "val Loss: 0.1726 Acc: 0.9383\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 0.3537 Acc: 0.8700\n",
      "val Loss: 0.1965 Acc: 0.9218\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 0.4088 Acc: 0.8483\n",
      "val Loss: 0.1750 Acc: 0.9383\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 0.3556 Acc: 0.8731\n",
      "val Loss: 0.2544 Acc: 0.9218\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 0.3808 Acc: 0.8638\n",
      "val Loss: 0.1761 Acc: 0.9383\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 0.3678 Acc: 0.8648\n",
      "val Loss: 0.1604 Acc: 0.9424\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 0.4260 Acc: 0.8421\n",
      "val Loss: 0.2201 Acc: 0.9300\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 0.4172 Acc: 0.8411\n",
      "val Loss: 0.1927 Acc: 0.9383\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 0.3973 Acc: 0.8483\n",
      "val Loss: 0.2138 Acc: 0.9424\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 0.4092 Acc: 0.8504\n",
      "val Loss: 0.1969 Acc: 0.9342\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 0.3832 Acc: 0.8607\n",
      "val Loss: 0.1460 Acc: 0.9383\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 0.3817 Acc: 0.8576\n",
      "val Loss: 0.2392 Acc: 0.9177\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 0.3786 Acc: 0.8669\n",
      "val Loss: 0.1434 Acc: 0.9424\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 0.3761 Acc: 0.8596\n",
      "val Loss: 0.1890 Acc: 0.9424\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 0.3473 Acc: 0.8710\n",
      "val Loss: 0.1461 Acc: 0.9465\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 0.3799 Acc: 0.8627\n",
      "val Loss: 0.1352 Acc: 0.9465\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 0.4029 Acc: 0.8617\n",
      "val Loss: 0.2399 Acc: 0.9259\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 0.3368 Acc: 0.8782\n",
      "val Loss: 0.1948 Acc: 0.9342\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 0.4011 Acc: 0.8586\n",
      "val Loss: 0.2087 Acc: 0.9300\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 0.3619 Acc: 0.8700\n",
      "val Loss: 0.2326 Acc: 0.9259\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 0.4257 Acc: 0.8390\n",
      "val Loss: 0.2116 Acc: 0.9342\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 0.4242 Acc: 0.8411\n",
      "val Loss: 0.1946 Acc: 0.9383\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 0.3632 Acc: 0.8700\n",
      "val Loss: 0.2109 Acc: 0.9342\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 0.3996 Acc: 0.8514\n",
      "val Loss: 0.2326 Acc: 0.9342\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 0.3853 Acc: 0.8658\n",
      "val Loss: 0.2758 Acc: 0.9218\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 0.3962 Acc: 0.8586\n",
      "val Loss: 0.1891 Acc: 0.9424\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 0.3767 Acc: 0.8535\n",
      "val Loss: 0.2805 Acc: 0.9177\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 0.3471 Acc: 0.8710\n",
      "val Loss: 0.2087 Acc: 0.9383\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 0.4050 Acc: 0.8452\n",
      "val Loss: 0.1557 Acc: 0.9424\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 0.4003 Acc: 0.8586\n",
      "val Loss: 0.2325 Acc: 0.9342\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 0.3941 Acc: 0.8576\n",
      "val Loss: 0.1844 Acc: 0.9465\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 0.3839 Acc: 0.8658\n",
      "val Loss: 0.1886 Acc: 0.9342\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 0.3962 Acc: 0.8566\n",
      "val Loss: 0.1946 Acc: 0.9342\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 0.3694 Acc: 0.8576\n",
      "val Loss: 0.1488 Acc: 0.9424\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 0.3858 Acc: 0.8380\n",
      "val Loss: 0.1732 Acc: 0.9342\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 0.3573 Acc: 0.8607\n",
      "val Loss: 0.1523 Acc: 0.9506\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 0.4124 Acc: 0.8473\n",
      "val Loss: 0.2084 Acc: 0.9342\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 0.3768 Acc: 0.8658\n",
      "val Loss: 0.2755 Acc: 0.9177\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 0.3956 Acc: 0.8462\n",
      "val Loss: 0.1438 Acc: 0.9506\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 0.3767 Acc: 0.8627\n",
      "val Loss: 0.1925 Acc: 0.9465\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 0.4029 Acc: 0.8483\n",
      "val Loss: 0.2248 Acc: 0.9342\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 0.3876 Acc: 0.8566\n",
      "val Loss: 0.2076 Acc: 0.9383\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 0.3659 Acc: 0.8514\n",
      "val Loss: 0.1799 Acc: 0.9465\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 0.4467 Acc: 0.8380\n",
      "val Loss: 0.1406 Acc: 0.9547\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 0.3665 Acc: 0.8617\n",
      "val Loss: 0.2484 Acc: 0.9259\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 0.4397 Acc: 0.8493\n",
      "val Loss: 0.2322 Acc: 0.9300\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 0.3686 Acc: 0.8566\n",
      "val Loss: 0.1683 Acc: 0.9465\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 0.3767 Acc: 0.8586\n",
      "val Loss: 0.1610 Acc: 0.9506\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 0.3909 Acc: 0.8493\n",
      "val Loss: 0.2575 Acc: 0.9300\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 0.4536 Acc: 0.8369\n",
      "val Loss: 0.2471 Acc: 0.9218\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 0.3481 Acc: 0.8586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2459 Acc: 0.9342\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 0.4214 Acc: 0.8359\n",
      "val Loss: 0.2038 Acc: 0.9506\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 0.4205 Acc: 0.8421\n",
      "val Loss: 0.2176 Acc: 0.9342\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 0.3878 Acc: 0.8586\n",
      "val Loss: 0.2413 Acc: 0.9300\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 0.3547 Acc: 0.8627\n",
      "val Loss: 0.2091 Acc: 0.9383\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 0.3802 Acc: 0.8545\n",
      "val Loss: 0.2918 Acc: 0.9300\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 0.3893 Acc: 0.8751\n",
      "val Loss: 0.1690 Acc: 0.9383\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 0.3746 Acc: 0.8710\n",
      "val Loss: 0.1713 Acc: 0.9506\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 0.3516 Acc: 0.8731\n",
      "val Loss: 0.2345 Acc: 0.9342\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 0.3332 Acc: 0.8617\n",
      "val Loss: 0.1770 Acc: 0.9424\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 0.3393 Acc: 0.8772\n",
      "val Loss: 0.1924 Acc: 0.9424\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 0.3525 Acc: 0.8772\n",
      "val Loss: 0.2093 Acc: 0.9383\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 0.3243 Acc: 0.8854\n",
      "val Loss: 0.2377 Acc: 0.9300\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 0.4025 Acc: 0.8349\n",
      "val Loss: 0.2006 Acc: 0.9465\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 0.3993 Acc: 0.8596\n",
      "val Loss: 0.2041 Acc: 0.9383\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 0.4067 Acc: 0.8452\n",
      "val Loss: 0.1516 Acc: 0.9424\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 0.4077 Acc: 0.8576\n",
      "val Loss: 0.2342 Acc: 0.9424\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 0.4108 Acc: 0.8576\n",
      "val Loss: 0.2241 Acc: 0.9342\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 0.3639 Acc: 0.8679\n",
      "val Loss: 0.2074 Acc: 0.9259\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 0.3482 Acc: 0.8700\n",
      "val Loss: 0.2644 Acc: 0.9259\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 0.3711 Acc: 0.8731\n",
      "val Loss: 0.1857 Acc: 0.9342\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 0.3628 Acc: 0.8627\n",
      "val Loss: 0.2143 Acc: 0.9383\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 0.3596 Acc: 0.8555\n",
      "val Loss: 0.2845 Acc: 0.9259\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 0.3648 Acc: 0.8720\n",
      "val Loss: 0.2529 Acc: 0.9300\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 0.4751 Acc: 0.8256\n",
      "val Loss: 0.2659 Acc: 0.9259\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 0.4180 Acc: 0.8524\n",
      "val Loss: 0.2146 Acc: 0.9300\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 0.4000 Acc: 0.8586\n",
      "val Loss: 0.1742 Acc: 0.9383\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 0.3657 Acc: 0.8617\n",
      "val Loss: 0.1583 Acc: 0.9465\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 0.3715 Acc: 0.8535\n",
      "val Loss: 0.1894 Acc: 0.9465\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 0.3663 Acc: 0.8596\n",
      "val Loss: 0.2004 Acc: 0.9383\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 0.4064 Acc: 0.8566\n",
      "val Loss: 0.1747 Acc: 0.9424\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 0.4153 Acc: 0.8504\n",
      "val Loss: 0.1561 Acc: 0.9465\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 0.3929 Acc: 0.8596\n",
      "val Loss: 0.1891 Acc: 0.9383\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 0.3619 Acc: 0.8524\n",
      "val Loss: 0.1514 Acc: 0.9300\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 0.3713 Acc: 0.8596\n",
      "val Loss: 0.1925 Acc: 0.9383\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 0.3824 Acc: 0.8669\n",
      "val Loss: 0.2237 Acc: 0.9300\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 0.3307 Acc: 0.8803\n",
      "val Loss: 0.2051 Acc: 0.9465\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 0.3725 Acc: 0.8607\n",
      "val Loss: 0.1941 Acc: 0.9424\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 0.3756 Acc: 0.8586\n",
      "val Loss: 0.2052 Acc: 0.9383\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 0.3927 Acc: 0.8545\n",
      "val Loss: 0.1901 Acc: 0.9300\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 0.3616 Acc: 0.8731\n",
      "val Loss: 0.2191 Acc: 0.9053\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 0.2812 Acc: 0.8978\n",
      "val Loss: 0.1733 Acc: 0.9383\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 0.3308 Acc: 0.8762\n",
      "val Loss: 0.1500 Acc: 0.9506\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 0.4140 Acc: 0.8524\n",
      "val Loss: 0.2343 Acc: 0.9218\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 0.3973 Acc: 0.8493\n",
      "val Loss: 0.1281 Acc: 0.9588\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 0.3560 Acc: 0.8627\n",
      "val Loss: 0.1842 Acc: 0.9342\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 0.4323 Acc: 0.8483\n",
      "val Loss: 0.1587 Acc: 0.9424\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 0.3892 Acc: 0.8751\n",
      "val Loss: 0.1848 Acc: 0.9465\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 0.3884 Acc: 0.8782\n",
      "val Loss: 0.1861 Acc: 0.9300\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 0.3340 Acc: 0.8803\n",
      "val Loss: 0.2104 Acc: 0.9218\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 0.3880 Acc: 0.8617\n",
      "val Loss: 0.2129 Acc: 0.9342\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 0.3992 Acc: 0.8669\n",
      "val Loss: 0.2130 Acc: 0.9342\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 0.4098 Acc: 0.8566\n",
      "val Loss: 0.2076 Acc: 0.9424\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 0.3742 Acc: 0.8627\n",
      "val Loss: 0.1895 Acc: 0.9465\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 0.3450 Acc: 0.8731\n",
      "val Loss: 0.2147 Acc: 0.9383\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 0.3570 Acc: 0.8885\n",
      "val Loss: 0.2360 Acc: 0.9300\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 0.3655 Acc: 0.8607\n",
      "val Loss: 0.2271 Acc: 0.9424\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 0.3778 Acc: 0.8658\n",
      "val Loss: 0.2992 Acc: 0.9342\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 0.4041 Acc: 0.8390\n",
      "val Loss: 0.2186 Acc: 0.9424\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 0.3591 Acc: 0.8844\n",
      "val Loss: 0.1702 Acc: 0.9547\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 0.3888 Acc: 0.8576\n",
      "val Loss: 0.1618 Acc: 0.9465\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 0.3593 Acc: 0.8720\n",
      "val Loss: 0.2302 Acc: 0.9300\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 0.3211 Acc: 0.8813\n",
      "val Loss: 0.1641 Acc: 0.9465\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 0.3674 Acc: 0.8689\n",
      "val Loss: 0.2144 Acc: 0.9383\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 0.3260 Acc: 0.8793\n",
      "val Loss: 0.1949 Acc: 0.9300\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 0.3911 Acc: 0.8514\n",
      "val Loss: 0.2009 Acc: 0.9259\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 0.4168 Acc: 0.8473\n",
      "val Loss: 0.2530 Acc: 0.9300\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 0.3462 Acc: 0.8731\n",
      "val Loss: 0.1694 Acc: 0.9547\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 0.3535 Acc: 0.8731\n",
      "val Loss: 0.2102 Acc: 0.9342\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 0.3933 Acc: 0.8514\n",
      "val Loss: 0.2993 Acc: 0.9218\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 0.3524 Acc: 0.8617\n",
      "val Loss: 0.1935 Acc: 0.9465\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 0.3963 Acc: 0.8483\n",
      "val Loss: 0.3263 Acc: 0.9053\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 0.3680 Acc: 0.8854\n",
      "val Loss: 0.1871 Acc: 0.9342\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 0.3982 Acc: 0.8586\n",
      "val Loss: 0.3132 Acc: 0.9218\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 0.3962 Acc: 0.8638\n",
      "val Loss: 0.2512 Acc: 0.9424\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 0.3385 Acc: 0.8689\n",
      "val Loss: 0.2416 Acc: 0.9383\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 0.3596 Acc: 0.8648\n",
      "val Loss: 0.2530 Acc: 0.9547\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 0.3731 Acc: 0.8607\n",
      "val Loss: 0.2419 Acc: 0.9383\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 0.3736 Acc: 0.8638\n",
      "val Loss: 0.2872 Acc: 0.9300\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 0.4032 Acc: 0.8524\n",
      "val Loss: 0.2745 Acc: 0.9218\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 0.3980 Acc: 0.8576\n",
      "val Loss: 0.1913 Acc: 0.9424\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 0.3481 Acc: 0.8710\n",
      "val Loss: 0.1933 Acc: 0.9424\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 0.3586 Acc: 0.8638\n",
      "val Loss: 0.1946 Acc: 0.9424\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 0.3820 Acc: 0.8617\n",
      "val Loss: 0.2660 Acc: 0.9342\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 0.3765 Acc: 0.8566\n",
      "val Loss: 0.1482 Acc: 0.9465\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 0.3808 Acc: 0.8617\n",
      "val Loss: 0.1617 Acc: 0.9424\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 0.3973 Acc: 0.8535\n",
      "val Loss: 0.1829 Acc: 0.9547\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 0.4068 Acc: 0.8493\n",
      "val Loss: 0.1727 Acc: 0.9506\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 0.3430 Acc: 0.8700\n",
      "val Loss: 0.2152 Acc: 0.9342\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 0.3616 Acc: 0.8813\n",
      "val Loss: 0.2711 Acc: 0.9383\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 0.4284 Acc: 0.8524\n",
      "val Loss: 0.1458 Acc: 0.9424\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 0.3699 Acc: 0.8648\n",
      "val Loss: 0.2232 Acc: 0.9342\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 0.4021 Acc: 0.8555\n",
      "val Loss: 0.2028 Acc: 0.9424\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 0.3287 Acc: 0.8648\n",
      "val Loss: 0.3015 Acc: 0.9177\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 0.3424 Acc: 0.8813\n",
      "val Loss: 0.1933 Acc: 0.9465\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 0.3910 Acc: 0.8638\n",
      "val Loss: 0.2247 Acc: 0.9424\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3509 Acc: 0.8782\n",
      "val Loss: 0.2360 Acc: 0.9136\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 0.4119 Acc: 0.8421\n",
      "val Loss: 0.1827 Acc: 0.9259\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 0.3542 Acc: 0.8793\n",
      "val Loss: 0.2129 Acc: 0.9383\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 0.3686 Acc: 0.8648\n",
      "val Loss: 0.1833 Acc: 0.9383\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 0.3519 Acc: 0.8720\n",
      "val Loss: 0.1882 Acc: 0.9506\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 0.3509 Acc: 0.8762\n",
      "val Loss: 0.1705 Acc: 0.9465\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 0.4149 Acc: 0.8607\n",
      "val Loss: 0.2959 Acc: 0.9218\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 0.3897 Acc: 0.8627\n",
      "val Loss: 0.1724 Acc: 0.9506\n",
      "\n",
      "Training complete in 113m 45s\n",
      "Best val Acc: 0.958848\n"
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, \n",
    "                             optimizer_ft, num_epochs=num_epochs, \n",
    "                             is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
