{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimi/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jimi/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jimi/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jimi/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jimi/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jimi/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy \n",
    "import random\n",
    "import shutil, sys\n",
    "import pandas as pd\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = '/home/jimi/dissertacao/covid19/datasets/dataset_aug/'\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "#model_name = \"densenet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "#num_classes = 3\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 8\n",
    "\n",
    "# Number of epochs to train for\n",
    "\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True\n",
    "\n",
    "#num_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "#transforms.RandomRotation(degrees=(-5, 5)),\n",
    "#transforms.ColorJitter(brightness=.02),\n",
    "    \n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x]) for x in ['train', 'val']}\n",
    "\n",
    "# Create training and validation dataloaders\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
    "                    batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files per classes\n",
      "----------------------------------------\n",
      "normal :  4023\n",
      "pneumonia :  4035\n",
      "covid :  4105\n",
      "--------------------\n",
      "Train, test, validation\n",
      "--------------------\n",
      "len_train_dir :  7784\n",
      "len_test_dir :  2433\n",
      "len_val_dir :  1946\n"
     ]
    }
   ],
   "source": [
    "# Path to data\n",
    "data_dir = '/home/jimi/dissertacao/covid19/datasets/dataset_aug/'\n",
    "train_dir = data_dir+'train/'\n",
    "test_dir = data_dir+'test/'\n",
    "val_dir = data_dir+'val/'\n",
    "\n",
    "normal_dir = data_dir+'normal/'\n",
    "pneumonia_dir = data_dir+'pneumonia/'\n",
    "covid_dir = data_dir+'covid/'\n",
    "\n",
    "len_covid = len([iq for iq in os.scandir(normal_dir)])\n",
    "len_normal = len([iq for iq in os.scandir(pneumonia_dir)])\n",
    "len_pneumonia = len([iq for iq in os.scandir(covid_dir)])\n",
    "\n",
    "len_train_dir = len([iq for iq in os.scandir(train_dir+'covid/')]) + len([iq for iq in os.scandir(train_dir+'normal/')]) + len([iq for iq in os.scandir(train_dir+'pneumonia/')])\n",
    "len_test_dir = len([iq for iq in os.scandir(test_dir+'covid/')]) + len([iq for iq in os.scandir(test_dir+'normal/')]) + len([iq for iq in os.scandir(test_dir+'pneumonia/')])\n",
    "len_val_dir = len([iq for iq in os.scandir(val_dir+'covid/')]) + len([iq for iq in os.scandir(val_dir+'normal/')]) + len([iq for iq in os.scandir(val_dir+'pneumonia/')])\n",
    "print(\"<br>\")\n",
    "print('Files per classes')\n",
    "print(\"<br>\")\n",
    "print(\"----\"*10)\n",
    "print(\"<br>\")\n",
    "print(\"normal : \", len_covid)\n",
    "print(\"<br>\")\n",
    "print(\"pneumonia : \", len_normal)\n",
    "print(\"<br>\")\n",
    "print(\"covid : \", len_pneumonia)\n",
    "print(\"<br>\")\n",
    "print(\"-\"*20)\n",
    "print(\"<br>\")\n",
    "print('Train, test, validation')\n",
    "print(\"<br>\")\n",
    "print(\"-\"*20)\n",
    "print(\"<br>\")\n",
    "print(\"len_train_dir : \", len_train_dir)\n",
    "print(\"<br>\")\n",
    "print(\"len_test_dir : \", len_test_dir)\n",
    "print(\"<br>\")\n",
    "print(\"len_val_dir : \", len_val_dir)\n",
    "print(\"<br>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs, model_name, lr, batch_size):\n",
    "    since = time.time()\n",
    "    is_inception = False\n",
    "    \n",
    "    #tensorboard\n",
    "    writer = SummaryWriter(f'runs/dg_{model_name}_lr={lr}_epoch={num_epochs}_batch_size={batch_size}')\n",
    "    step = 0\n",
    "\n",
    "    val_acc_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print(\"<br>\")\n",
    "        print('-' * 10)\n",
    "        print(\"<br>\")\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            print(\"<br>\")\n",
    "\n",
    "            writer.add_scalar('training loss', loss, global_step=step)\n",
    "            writer.add_scalar('training accuracy', epoch_acc, global_step=step)\n",
    "            step += 1\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "        \n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print()\n",
    "    print(\"<br>\")\n",
    "    print('#'*30)\n",
    "    print(\"<br>\")\n",
    "    print('------ Summary ------')\n",
    "    print(\"<br>\")\n",
    "    print(f'model -> {_model}')\n",
    "    print(\"<br>\")\n",
    "    print(f'epochs -> {_epochs}')\n",
    "    print(\"<br>\")\n",
    "    print(f'lr -> {_lrs}')\n",
    "    print(\"<br>\")\n",
    "    print(f'batch size -> {_batch}')\n",
    "    print(\"<br>\")\n",
    "    print()\n",
    "    print(\"<br>\")\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print(\"<br>\")\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    print(\"<br>\")\n",
    "    print('#'*30)\n",
    "    print(\"<br>\")\n",
    "    print()\n",
    "    print(\"<br>\")\n",
    "    print('==== END ====')\n",
    "    print(\"<br>\")\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=4):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 3, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "==== INITIALIZING WITH PARAMETERS: ====\n",
      "\n",
      "model -> squeezenet\n",
      "epochs -> 500\n",
      "lr -> 0.01\n",
      "batch size -> 8\n",
      "\n",
      "--------------------\n",
      "Params to learn:\n",
      "\t classifier.1.weight\n",
      "\t classifier.1.bias\n",
      "\n",
      "--------------------\n",
      "\n",
      "== Epochs ==\n",
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.4036 Acc: 0.8394\n",
      "val Loss: 0.3905 Acc: 0.8607\n",
      "\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.2226 Acc: 0.9221\n",
      "val Loss: 0.1953 Acc: 0.9399\n",
      "\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.1882 Acc: 0.9354\n",
      "val Loss: 0.1710 Acc: 0.9481\n",
      "\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.1943 Acc: 0.9351\n",
      "val Loss: 0.1934 Acc: 0.9358\n",
      "\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.1703 Acc: 0.9417\n",
      "val Loss: 0.2079 Acc: 0.9260\n",
      "\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.1646 Acc: 0.9423\n",
      "val Loss: 0.1818 Acc: 0.9399\n",
      "\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.1626 Acc: 0.9441\n",
      "val Loss: 0.1858 Acc: 0.9383\n",
      "\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.1599 Acc: 0.9455\n",
      "val Loss: 0.1757 Acc: 0.9435\n",
      "\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.1587 Acc: 0.9460\n",
      "val Loss: 0.1557 Acc: 0.9460\n",
      "\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.1550 Acc: 0.9481\n",
      "val Loss: 0.2118 Acc: 0.9152\n",
      "\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.1637 Acc: 0.9437\n",
      "val Loss: 0.1673 Acc: 0.9430\n",
      "\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.1494 Acc: 0.9457\n",
      "val Loss: 0.2759 Acc: 0.8952\n",
      "\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.1485 Acc: 0.9496\n",
      "val Loss: 0.2027 Acc: 0.9219\n",
      "\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.1569 Acc: 0.9450\n",
      "val Loss: 0.1791 Acc: 0.9306\n",
      "\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.1483 Acc: 0.9476\n",
      "val Loss: 0.1500 Acc: 0.9471\n",
      "\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.1472 Acc: 0.9489\n",
      "val Loss: 0.1697 Acc: 0.9399\n",
      "\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.1372 Acc: 0.9514\n"
     ]
    }
   ],
   "source": [
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "num_classes = 3\n",
    "\n",
    "_models = ['squeezenet', 'densenet', 'resnet', 'alexnet', 'vgg']\n",
    "lrs = [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "_epoch = [500]\n",
    "batch_sizes = [8, 16, 32]\n",
    "\n",
    "for _model in _models:\n",
    "    for _epochs in _epoch:\n",
    "        for _lrs in lrs:\n",
    "            for _batch in batch_sizes:\n",
    "                               \n",
    "                print()\n",
    "                print()\n",
    "                print()\n",
    "                print(\"<br>\")\n",
    "                print('='*60)\n",
    "                print(\"<br>\")\n",
    "                print('==== INITIALIZING WITH PARAMETERS: ====')\n",
    "                print(\"<br>\")\n",
    "                print()\n",
    "                print(\"<br>\")\n",
    "                print(f'model -> {_model}')\n",
    "                print(\"<br>\")\n",
    "                print(f'epochs -> {_epochs}')\n",
    "                print(\"<br>\")\n",
    "                print(f'lr -> {_lrs}')\n",
    "                print(\"<br>\")\n",
    "                print(f'batch size -> {_batch}')\n",
    "                print(\"<br>\")\n",
    "                print()\n",
    "                print(\"<br>\")\n",
    "\n",
    "                feature_extract = True\n",
    "                \n",
    "                model_ft, input_size = initialize_model(_model, num_classes, \n",
    "                                                        feature_extract, use_pretrained=True)\n",
    "                \n",
    "                # Send the model to GPU\n",
    "                model_ft = model_ft.to(device)\n",
    "\n",
    "                print('-'*20)\n",
    "                params_to_update = model_ft.parameters()\n",
    "                print(\"Params to learn:\")\n",
    "                if feature_extract:\n",
    "                    params_to_update = []\n",
    "                    for name,param in model_ft.named_parameters():\n",
    "                        if param.requires_grad == True:\n",
    "                            params_to_update.append(param)\n",
    "                            print(\"\\t\",name)\n",
    "                            print(\"<br>\")\n",
    "                else:\n",
    "                    for name,param in model_ft.named_parameters():\n",
    "                        if param.requires_grad == True:\n",
    "                            print(\"\\t\",name)\n",
    "                            print(\"<br>\")\n",
    "\n",
    "                print()\n",
    "                print(\"<br>\")\n",
    "                print('-'*20)\n",
    "                \n",
    "                print()\n",
    "                print('== Epochs ==')\n",
    "                \n",
    "                optimizer_ft = optim.SGD(params_to_update, _lrs, momentum=0.9)\n",
    "                exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)\n",
    "\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "                model_ft, hist = train_model(model_ft, dataloaders, criterion, optimizer_ft,\n",
    "                                             num_epochs=_epochs, model_name=_model, lr=_lrs, batch_size=_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
