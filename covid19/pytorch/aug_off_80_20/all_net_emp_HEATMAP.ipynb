{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hatmap\n",
    "- Melhorar o plot de imagens\n",
    "- Propor algo novo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy \n",
    "import cv2\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import shutil, sys\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = '/home/jimi/dissertacao/covid19/datasets/80-20/'\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "#model_name = \"densenet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "#num_classes = 3\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 16\n",
    "\n",
    "# Number of epochs to train for\n",
    "\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True\n",
    "\n",
    "#num_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "#transforms.RandomRotation(degrees=(-5, 5)),\n",
    "#transforms.ColorJitter(brightness=.02),\n",
    "    \n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x]) for x in ['train', 'val', 'test']}\n",
    "\n",
    "# Create training and validation dataloaders\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
    "                    batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "\n",
    "# trans = ['train','val','test']\n",
    "# categories = ['train','val','test']\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files per classes\n",
      "----------------------------------------\n",
      "normal :  4023\n",
      "pneumonia :  4035\n",
      "covid :  4105\n",
      "--------------------\n",
      "Train, test, validation\n",
      "--------------------\n",
      "len_train_dir :  9632\n",
      "len_test_dir :  31\n",
      "len_val_dir :  2409\n"
     ]
    }
   ],
   "source": [
    "# Path to data\n",
    "data_dir = '/home/jimi/dissertacao/covid19/datasets/80-20/'\n",
    "train_dir = data_dir+'train/'\n",
    "test_dir = data_dir+'test/'\n",
    "val_dir = data_dir+'val/'\n",
    "\n",
    "normal_dir = data_dir+'normal/'\n",
    "pneumonia_dir = data_dir+'pneumonia/'\n",
    "covid_dir = data_dir+'covid/'\n",
    "\n",
    "len_covid = len([iq for iq in os.scandir(normal_dir)])\n",
    "len_normal = len([iq for iq in os.scandir(pneumonia_dir)])\n",
    "len_pneumonia = len([iq for iq in os.scandir(covid_dir)])\n",
    "\n",
    "len_train_dir = len([iq for iq in os.scandir(train_dir+'covid/')]) + len([iq for iq in os.scandir(train_dir+'normal/')]) + len([iq for iq in os.scandir(train_dir+'pneumonia/')])\n",
    "len_test_dir = len([iq for iq in os.scandir(test_dir+'covid/')]) + len([iq for iq in os.scandir(test_dir+'normal/')]) + len([iq for iq in os.scandir(test_dir+'pneumonia/')])\n",
    "len_val_dir = len([iq for iq in os.scandir(val_dir+'covid/')]) + len([iq for iq in os.scandir(val_dir+'normal/')]) + len([iq for iq in os.scandir(val_dir+'pneumonia/')])\n",
    "\n",
    "print('Files per classes')\n",
    "print(\"----\"*10)\n",
    "print(\"normal : \", len_covid)\n",
    "\n",
    "print(\"pneumonia : \", len_normal)\n",
    "\n",
    "print(\"covid : \", len_pneumonia)\n",
    "\n",
    "print(\"-\"*20)\n",
    "\n",
    "print('Train, test, validation')\n",
    "\n",
    "print(\"-\"*20)\n",
    "\n",
    "print(\"len_train_dir : \", len_train_dir)\n",
    "\n",
    "print(\"len_test_dir : \", len_test_dir)\n",
    "\n",
    "print(\"len_val_dir : \", len_val_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs, \n",
    "                model_name, lr, batch_size, opt_name, crt_name):\n",
    "    since = time.time()\n",
    "    is_inception = False\n",
    "    \n",
    "    #tensorboard\n",
    "    writer = SummaryWriter(f'runs/dg_{model_name}_lr={lr}_epoch={num_epochs}_batch_size={batch_size}')\n",
    "    step = 0\n",
    "\n",
    "    val_acc_history = []\n",
    "    val_loss_history = []\n",
    "    \n",
    "    train_acc_history = []\n",
    "    train_loss_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        \n",
    "        print('-' * 10)\n",
    "        \n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "\n",
    "            writer.add_scalar('training loss', loss, global_step=step)\n",
    "            writer.add_scalar('training accuracy', epoch_acc, global_step=step)\n",
    "            step += 1\n",
    "            \n",
    "            #only to plot the graph\n",
    "            if phase == 'train':\n",
    "                train_acc_history.append(epoch_acc)\n",
    "                train_loss_history.append(epoch_loss)\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "                val_loss_history.append(epoch_loss)\n",
    "\n",
    "        print()\n",
    "        \n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print()\n",
    "    print('#'*30)\n",
    "    print('------ Summary ------')\n",
    "    print(f'model -> {_model}')\n",
    "    print(f'epochs -> {_epochs}')\n",
    "    print(f'lr -> {_lrs}')\n",
    "    print(f'batch size -> {_batch}')\n",
    "    print(f'optimizer -> {opt_name}'), \n",
    "    print(f'criteriun -> {crt_name}')\n",
    "    print()\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    print('#'*30)\n",
    "    \n",
    "    plt.figure(figsize=(13, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(val_acc_history, label=\"Validation Accuracy\")\n",
    "    plt.plot(train_acc_history, label=\"Validation Loss\")\n",
    "    plt.title('Accuracy and Loss in Validation Dataset')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_loss_history, label=\"val_loss_history\")\n",
    "    plt.plot(train_loss_history, label=\"train_loss_history\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('hist_'+_model+'_opt_'+opt_name+'_crt_'+crt_name+'.png')\n",
    "    plt.show()\n",
    "\n",
    "    print()\n",
    "\n",
    "    print('==== END ====')\n",
    "\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=4):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 3, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools \n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig('cm_'+title+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "==== INITIALIZING WITH PARAMETERS: ====\n",
      "model -> squeezenet\n",
      "epochs -> 300\n",
      "lr -> 0.0001\n",
      "batch size -> 8\n",
      "optimizer -> 1\n",
      "criteriun -> 1\n",
      "\n",
      "--------------------\n",
      "Params to learn:\n",
      "\t classifier.1.weight\n",
      "\t classifier.1.bias\n",
      "\n",
      "--------------------\n",
      "\n",
      "== Epochs ==\n",
      "Epoch 0/299\n",
      "----------\n",
      "train Loss: 0.5567 Acc: 0.7990\n",
      "val Loss: 0.3143 Acc: 0.9132\n",
      "\n",
      "Epoch 1/299\n",
      "----------\n",
      "train Loss: 0.3074 Acc: 0.9016\n",
      "val Loss: 0.2554 Acc: 0.9286\n",
      "\n",
      "Epoch 2/299\n",
      "----------\n",
      "train Loss: 0.2613 Acc: 0.9170\n",
      "val Loss: 0.2161 Acc: 0.9394\n",
      "\n",
      "Epoch 3/299\n",
      "----------\n",
      "train Loss: 0.2377 Acc: 0.9222\n",
      "val Loss: 0.1965 Acc: 0.9423\n",
      "\n",
      "Epoch 4/299\n",
      "----------\n",
      "train Loss: 0.2197 Acc: 0.9301\n",
      "val Loss: 0.1828 Acc: 0.9469\n",
      "\n",
      "Epoch 5/299\n",
      "----------\n",
      "train Loss: 0.2093 Acc: 0.9340\n",
      "val Loss: 0.1759 Acc: 0.9494\n",
      "\n",
      "Epoch 6/299\n",
      "----------\n",
      "train Loss: 0.2044 Acc: 0.9302\n",
      "val Loss: 0.1689 Acc: 0.9527\n",
      "\n",
      "Epoch 7/299\n",
      "----------\n",
      "train Loss: 0.1984 Acc: 0.9351\n",
      "val Loss: 0.1668 Acc: 0.9523\n",
      "\n",
      "Epoch 8/299\n",
      "----------\n",
      "train Loss: 0.1857 Acc: 0.9410\n",
      "val Loss: 0.1760 Acc: 0.9485\n",
      "\n",
      "Epoch 9/299\n",
      "----------\n",
      "train Loss: 0.1793 Acc: 0.9403\n",
      "val Loss: 0.1618 Acc: 0.9510\n",
      "\n",
      "Epoch 10/299\n",
      "----------\n",
      "train Loss: 0.1784 Acc: 0.9429\n",
      "val Loss: 0.1472 Acc: 0.9548\n",
      "\n",
      "Epoch 11/299\n",
      "----------\n",
      "train Loss: 0.1734 Acc: 0.9431\n",
      "val Loss: 0.1426 Acc: 0.9577\n",
      "\n",
      "Epoch 12/299\n",
      "----------\n",
      "train Loss: 0.1714 Acc: 0.9420\n",
      "val Loss: 0.1420 Acc: 0.9548\n",
      "\n",
      "Epoch 13/299\n",
      "----------\n",
      "train Loss: 0.1683 Acc: 0.9441\n",
      "val Loss: 0.1395 Acc: 0.9593\n",
      "\n",
      "Epoch 14/299\n",
      "----------\n",
      "train Loss: 0.1651 Acc: 0.9475\n",
      "val Loss: 0.1375 Acc: 0.9572\n",
      "\n",
      "Epoch 15/299\n",
      "----------\n",
      "train Loss: 0.1623 Acc: 0.9463\n",
      "val Loss: 0.1327 Acc: 0.9601\n",
      "\n",
      "Epoch 16/299\n",
      "----------\n",
      "train Loss: 0.1612 Acc: 0.9477\n",
      "val Loss: 0.1305 Acc: 0.9589\n",
      "\n",
      "Epoch 17/299\n",
      "----------\n",
      "train Loss: 0.1588 Acc: 0.9466\n",
      "val Loss: 0.1271 Acc: 0.9614\n",
      "\n",
      "Epoch 18/299\n",
      "----------\n",
      "train Loss: 0.1579 Acc: 0.9454\n",
      "val Loss: 0.1304 Acc: 0.9593\n",
      "\n",
      "Epoch 19/299\n",
      "----------\n",
      "train Loss: 0.1581 Acc: 0.9459\n",
      "val Loss: 0.1246 Acc: 0.9610\n",
      "\n",
      "Epoch 20/299\n",
      "----------\n",
      "train Loss: 0.1547 Acc: 0.9481\n",
      "val Loss: 0.1264 Acc: 0.9622\n",
      "\n",
      "Epoch 21/299\n",
      "----------\n",
      "train Loss: 0.1507 Acc: 0.9501\n",
      "val Loss: 0.1238 Acc: 0.9626\n",
      "\n",
      "Epoch 22/299\n",
      "----------\n",
      "train Loss: 0.1514 Acc: 0.9477\n",
      "val Loss: 0.1204 Acc: 0.9622\n",
      "\n",
      "Epoch 23/299\n",
      "----------\n",
      "train Loss: 0.1499 Acc: 0.9513\n",
      "val Loss: 0.1194 Acc: 0.9626\n",
      "\n",
      "Epoch 24/299\n",
      "----------\n",
      "train Loss: 0.1465 Acc: 0.9499\n",
      "val Loss: 0.1338 Acc: 0.9564\n",
      "\n",
      "Epoch 25/299\n",
      "----------\n",
      "train Loss: 0.1472 Acc: 0.9496\n",
      "val Loss: 0.1291 Acc: 0.9585\n",
      "\n",
      "Epoch 26/299\n",
      "----------\n",
      "train Loss: 0.1454 Acc: 0.9492\n",
      "val Loss: 0.1166 Acc: 0.9622\n",
      "\n",
      "Epoch 27/299\n",
      "----------\n",
      "train Loss: 0.1464 Acc: 0.9496\n",
      "val Loss: 0.1168 Acc: 0.9635\n",
      "\n",
      "Epoch 28/299\n",
      "----------\n",
      "train Loss: 0.1436 Acc: 0.9516\n",
      "val Loss: 0.1169 Acc: 0.9631\n",
      "\n",
      "Epoch 29/299\n",
      "----------\n",
      "train Loss: 0.1379 Acc: 0.9531\n",
      "val Loss: 0.1140 Acc: 0.9618\n",
      "\n",
      "Epoch 30/299\n",
      "----------\n",
      "train Loss: 0.1412 Acc: 0.9532\n",
      "val Loss: 0.1128 Acc: 0.9622\n",
      "\n",
      "Epoch 31/299\n",
      "----------\n",
      "train Loss: 0.1374 Acc: 0.9540\n",
      "val Loss: 0.1176 Acc: 0.9618\n",
      "\n",
      "Epoch 32/299\n",
      "----------\n",
      "train Loss: 0.1406 Acc: 0.9541\n",
      "val Loss: 0.1178 Acc: 0.9618\n",
      "\n",
      "Epoch 33/299\n",
      "----------\n",
      "train Loss: 0.1397 Acc: 0.9517\n",
      "val Loss: 0.1101 Acc: 0.9651\n",
      "\n",
      "Epoch 34/299\n",
      "----------\n",
      "train Loss: 0.1422 Acc: 0.9509\n",
      "val Loss: 0.1097 Acc: 0.9651\n",
      "\n",
      "Epoch 35/299\n",
      "----------\n",
      "train Loss: 0.1352 Acc: 0.9539\n",
      "val Loss: 0.1098 Acc: 0.9643\n",
      "\n",
      "Epoch 36/299\n",
      "----------\n",
      "train Loss: 0.1344 Acc: 0.9554\n",
      "val Loss: 0.1083 Acc: 0.9647\n",
      "\n",
      "Epoch 37/299\n",
      "----------\n",
      "train Loss: 0.1338 Acc: 0.9539\n",
      "val Loss: 0.1087 Acc: 0.9639\n",
      "\n",
      "Epoch 38/299\n",
      "----------\n",
      "train Loss: 0.1369 Acc: 0.9544\n",
      "val Loss: 0.1093 Acc: 0.9631\n",
      "\n",
      "Epoch 39/299\n",
      "----------\n",
      "train Loss: 0.1353 Acc: 0.9526\n",
      "val Loss: 0.1084 Acc: 0.9635\n",
      "\n",
      "Epoch 40/299\n",
      "----------\n",
      "train Loss: 0.1335 Acc: 0.9546\n",
      "val Loss: 0.1071 Acc: 0.9647\n",
      "\n",
      "Epoch 41/299\n",
      "----------\n",
      "train Loss: 0.1334 Acc: 0.9545\n",
      "val Loss: 0.1099 Acc: 0.9647\n",
      "\n",
      "Epoch 42/299\n",
      "----------\n",
      "train Loss: 0.1284 Acc: 0.9558\n",
      "val Loss: 0.1076 Acc: 0.9639\n",
      "\n",
      "Epoch 43/299\n",
      "----------\n",
      "train Loss: 0.1321 Acc: 0.9549\n",
      "val Loss: 0.1090 Acc: 0.9647\n",
      "\n",
      "Epoch 44/299\n",
      "----------\n",
      "train Loss: 0.1326 Acc: 0.9559\n",
      "val Loss: 0.1047 Acc: 0.9668\n",
      "\n",
      "Epoch 45/299\n",
      "----------\n",
      "train Loss: 0.1283 Acc: 0.9581\n",
      "val Loss: 0.1049 Acc: 0.9651\n",
      "\n",
      "Epoch 46/299\n",
      "----------\n",
      "train Loss: 0.1296 Acc: 0.9563\n",
      "val Loss: 0.1045 Acc: 0.9647\n",
      "\n",
      "Epoch 47/299\n",
      "----------\n",
      "train Loss: 0.1286 Acc: 0.9560\n",
      "val Loss: 0.1027 Acc: 0.9668\n",
      "\n",
      "Epoch 48/299\n",
      "----------\n",
      "train Loss: 0.1297 Acc: 0.9569\n",
      "val Loss: 0.1035 Acc: 0.9676\n",
      "\n",
      "Epoch 49/299\n",
      "----------\n",
      "train Loss: 0.1294 Acc: 0.9549\n",
      "val Loss: 0.1045 Acc: 0.9655\n",
      "\n",
      "Epoch 50/299\n",
      "----------\n",
      "train Loss: 0.1251 Acc: 0.9569\n",
      "val Loss: 0.1035 Acc: 0.9651\n",
      "\n",
      "Epoch 51/299\n",
      "----------\n",
      "train Loss: 0.1272 Acc: 0.9567\n",
      "val Loss: 0.1023 Acc: 0.9664\n",
      "\n",
      "Epoch 52/299\n",
      "----------\n",
      "train Loss: 0.1269 Acc: 0.9591\n",
      "val Loss: 0.1016 Acc: 0.9647\n",
      "\n",
      "Epoch 53/299\n",
      "----------\n",
      "train Loss: 0.1319 Acc: 0.9547\n",
      "val Loss: 0.1028 Acc: 0.9660\n",
      "\n",
      "Epoch 54/299\n",
      "----------\n",
      "train Loss: 0.1279 Acc: 0.9573\n",
      "val Loss: 0.1008 Acc: 0.9664\n",
      "\n",
      "Epoch 55/299\n",
      "----------\n",
      "train Loss: 0.1250 Acc: 0.9583\n",
      "val Loss: 0.1010 Acc: 0.9664\n",
      "\n",
      "Epoch 56/299\n",
      "----------\n",
      "train Loss: 0.1270 Acc: 0.9562\n",
      "val Loss: 0.1006 Acc: 0.9660\n",
      "\n",
      "Epoch 57/299\n",
      "----------\n",
      "train Loss: 0.1258 Acc: 0.9577\n",
      "val Loss: 0.1005 Acc: 0.9672\n",
      "\n",
      "Epoch 58/299\n",
      "----------\n",
      "train Loss: 0.1230 Acc: 0.9589\n",
      "val Loss: 0.0999 Acc: 0.9668\n",
      "\n",
      "Epoch 59/299\n",
      "----------\n",
      "train Loss: 0.1245 Acc: 0.9549\n",
      "val Loss: 0.0989 Acc: 0.9668\n",
      "\n",
      "Epoch 60/299\n",
      "----------\n",
      "train Loss: 0.1214 Acc: 0.9582\n",
      "val Loss: 0.0994 Acc: 0.9680\n",
      "\n",
      "Epoch 61/299\n",
      "----------\n",
      "train Loss: 0.1227 Acc: 0.9568\n",
      "val Loss: 0.1184 Acc: 0.9593\n",
      "\n",
      "Epoch 62/299\n",
      "----------\n",
      "train Loss: 0.1222 Acc: 0.9578\n",
      "val Loss: 0.1027 Acc: 0.9664\n",
      "\n",
      "Epoch 63/299\n",
      "----------\n",
      "train Loss: 0.1218 Acc: 0.9581\n",
      "val Loss: 0.0979 Acc: 0.9672\n",
      "\n",
      "Epoch 64/299\n",
      "----------\n",
      "train Loss: 0.1234 Acc: 0.9575\n",
      "val Loss: 0.0971 Acc: 0.9668\n",
      "\n",
      "Epoch 65/299\n",
      "----------\n",
      "train Loss: 0.1198 Acc: 0.9605\n",
      "val Loss: 0.1005 Acc: 0.9676\n",
      "\n",
      "Epoch 66/299\n",
      "----------\n",
      "train Loss: 0.1195 Acc: 0.9587\n",
      "val Loss: 0.0964 Acc: 0.9693\n",
      "\n",
      "Epoch 67/299\n",
      "----------\n",
      "train Loss: 0.1206 Acc: 0.9585\n",
      "val Loss: 0.0963 Acc: 0.9672\n",
      "\n",
      "Epoch 68/299\n",
      "----------\n",
      "train Loss: 0.1220 Acc: 0.9588\n",
      "val Loss: 0.0967 Acc: 0.9668\n",
      "\n",
      "Epoch 69/299\n",
      "----------\n",
      "train Loss: 0.1196 Acc: 0.9594\n",
      "val Loss: 0.0963 Acc: 0.9680\n",
      "\n",
      "Epoch 70/299\n",
      "----------\n",
      "train Loss: 0.1206 Acc: 0.9580\n",
      "val Loss: 0.0961 Acc: 0.9680\n",
      "\n",
      "Epoch 71/299\n",
      "----------\n",
      "train Loss: 0.1222 Acc: 0.9584\n",
      "val Loss: 0.1107 Acc: 0.9618\n",
      "\n",
      "Epoch 72/299\n",
      "----------\n",
      "train Loss: 0.1219 Acc: 0.9582\n",
      "val Loss: 0.0941 Acc: 0.9689\n",
      "\n",
      "Epoch 73/299\n",
      "----------\n",
      "train Loss: 0.1200 Acc: 0.9595\n",
      "val Loss: 0.0990 Acc: 0.9680\n",
      "\n",
      "Epoch 74/299\n",
      "----------\n",
      "train Loss: 0.1199 Acc: 0.9577\n",
      "val Loss: 0.0961 Acc: 0.9689\n",
      "\n",
      "Epoch 75/299\n",
      "----------\n",
      "train Loss: 0.1184 Acc: 0.9601\n",
      "val Loss: 0.0947 Acc: 0.9672\n",
      "\n",
      "Epoch 76/299\n",
      "----------\n",
      "train Loss: 0.1171 Acc: 0.9618\n",
      "val Loss: 0.0936 Acc: 0.9685\n",
      "\n",
      "Epoch 77/299\n",
      "----------\n",
      "train Loss: 0.1156 Acc: 0.9618\n",
      "val Loss: 0.0942 Acc: 0.9697\n",
      "\n",
      "Epoch 78/299\n",
      "----------\n",
      "train Loss: 0.1203 Acc: 0.9584\n",
      "val Loss: 0.0933 Acc: 0.9697\n",
      "\n",
      "Epoch 79/299\n",
      "----------\n",
      "train Loss: 0.1127 Acc: 0.9596\n",
      "val Loss: 0.0934 Acc: 0.9689\n",
      "\n",
      "Epoch 80/299\n",
      "----------\n",
      "train Loss: 0.1146 Acc: 0.9615\n",
      "val Loss: 0.0937 Acc: 0.9705\n",
      "\n",
      "Epoch 81/299\n",
      "----------\n",
      "train Loss: 0.1203 Acc: 0.9571\n",
      "val Loss: 0.1000 Acc: 0.9660\n",
      "\n",
      "Epoch 82/299\n",
      "----------\n",
      "train Loss: 0.1176 Acc: 0.9602\n",
      "val Loss: 0.0922 Acc: 0.9701\n",
      "\n",
      "Epoch 83/299\n",
      "----------\n",
      "train Loss: 0.1150 Acc: 0.9619\n",
      "val Loss: 0.0923 Acc: 0.9705\n",
      "\n",
      "Epoch 84/299\n",
      "----------\n",
      "train Loss: 0.1168 Acc: 0.9608\n",
      "val Loss: 0.0925 Acc: 0.9705\n",
      "\n",
      "Epoch 85/299\n",
      "----------\n",
      "train Loss: 0.1143 Acc: 0.9612\n",
      "val Loss: 0.0924 Acc: 0.9689\n",
      "\n",
      "Epoch 86/299\n",
      "----------\n",
      "train Loss: 0.1163 Acc: 0.9586\n",
      "val Loss: 0.0919 Acc: 0.9693\n",
      "\n",
      "Epoch 87/299\n",
      "----------\n",
      "train Loss: 0.1159 Acc: 0.9612\n",
      "val Loss: 0.0941 Acc: 0.9685\n",
      "\n",
      "Epoch 88/299\n",
      "----------\n",
      "train Loss: 0.1148 Acc: 0.9605\n",
      "val Loss: 0.0914 Acc: 0.9714\n",
      "\n",
      "Epoch 89/299\n",
      "----------\n",
      "train Loss: 0.1170 Acc: 0.9595\n",
      "val Loss: 0.0955 Acc: 0.9705\n",
      "\n",
      "Epoch 90/299\n",
      "----------\n",
      "train Loss: 0.1129 Acc: 0.9618\n",
      "val Loss: 0.0959 Acc: 0.9676\n",
      "\n",
      "Epoch 91/299\n",
      "----------\n",
      "train Loss: 0.1137 Acc: 0.9625\n",
      "val Loss: 0.0901 Acc: 0.9714\n",
      "\n",
      "Epoch 92/299\n",
      "----------\n",
      "train Loss: 0.1121 Acc: 0.9623\n",
      "val Loss: 0.0902 Acc: 0.9714\n",
      "\n",
      "Epoch 93/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1182 Acc: 0.9590\n",
      "val Loss: 0.0920 Acc: 0.9680\n",
      "\n",
      "Epoch 94/299\n",
      "----------\n",
      "train Loss: 0.1111 Acc: 0.9617\n",
      "val Loss: 0.0927 Acc: 0.9693\n",
      "\n",
      "Epoch 95/299\n",
      "----------\n",
      "train Loss: 0.1149 Acc: 0.9618\n",
      "val Loss: 0.0897 Acc: 0.9714\n",
      "\n",
      "Epoch 96/299\n",
      "----------\n",
      "train Loss: 0.1142 Acc: 0.9607\n",
      "val Loss: 0.0897 Acc: 0.9718\n",
      "\n",
      "Epoch 97/299\n",
      "----------\n",
      "train Loss: 0.1134 Acc: 0.9618\n",
      "val Loss: 0.0894 Acc: 0.9709\n",
      "\n",
      "Epoch 98/299\n",
      "----------\n",
      "train Loss: 0.1098 Acc: 0.9631\n",
      "val Loss: 0.0890 Acc: 0.9709\n",
      "\n",
      "Epoch 99/299\n",
      "----------\n",
      "train Loss: 0.1097 Acc: 0.9627\n",
      "val Loss: 0.0889 Acc: 0.9722\n",
      "\n",
      "Epoch 100/299\n",
      "----------\n",
      "train Loss: 0.1102 Acc: 0.9597\n",
      "val Loss: 0.0891 Acc: 0.9693\n",
      "\n",
      "Epoch 101/299\n",
      "----------\n",
      "train Loss: 0.1101 Acc: 0.9621\n",
      "val Loss: 0.0923 Acc: 0.9693\n",
      "\n",
      "Epoch 102/299\n",
      "----------\n",
      "train Loss: 0.1139 Acc: 0.9601\n",
      "val Loss: 0.0896 Acc: 0.9689\n",
      "\n",
      "Epoch 103/299\n",
      "----------\n",
      "train Loss: 0.1098 Acc: 0.9614\n",
      "val Loss: 0.0886 Acc: 0.9701\n",
      "\n",
      "Epoch 104/299\n",
      "----------\n",
      "train Loss: 0.1154 Acc: 0.9589\n",
      "val Loss: 0.0881 Acc: 0.9726\n",
      "\n",
      "Epoch 105/299\n",
      "----------\n",
      "train Loss: 0.1107 Acc: 0.9611\n",
      "val Loss: 0.0903 Acc: 0.9693\n",
      "\n",
      "Epoch 106/299\n",
      "----------\n",
      "train Loss: 0.1117 Acc: 0.9617\n",
      "val Loss: 0.0895 Acc: 0.9726\n",
      "\n",
      "Epoch 107/299\n",
      "----------\n",
      "train Loss: 0.1123 Acc: 0.9615\n",
      "val Loss: 0.0922 Acc: 0.9672\n",
      "\n",
      "Epoch 108/299\n",
      "----------\n",
      "train Loss: 0.1113 Acc: 0.9615\n",
      "val Loss: 0.0885 Acc: 0.9709\n",
      "\n",
      "Epoch 109/299\n",
      "----------\n",
      "train Loss: 0.1110 Acc: 0.9616\n",
      "val Loss: 0.0961 Acc: 0.9660\n",
      "\n",
      "Epoch 110/299\n",
      "----------\n",
      "train Loss: 0.1104 Acc: 0.9615\n",
      "val Loss: 0.0875 Acc: 0.9726\n",
      "\n",
      "Epoch 111/299\n",
      "----------\n",
      "train Loss: 0.1124 Acc: 0.9609\n",
      "val Loss: 0.0914 Acc: 0.9689\n",
      "\n",
      "Epoch 112/299\n",
      "----------\n",
      "train Loss: 0.1098 Acc: 0.9635\n",
      "val Loss: 0.0915 Acc: 0.9722\n",
      "\n",
      "Epoch 113/299\n",
      "----------\n",
      "train Loss: 0.1063 Acc: 0.9654\n",
      "val Loss: 0.0878 Acc: 0.9709\n",
      "\n",
      "Epoch 114/299\n",
      "----------\n",
      "train Loss: 0.1111 Acc: 0.9632\n",
      "val Loss: 0.0903 Acc: 0.9730\n",
      "\n",
      "Epoch 115/299\n",
      "----------\n",
      "train Loss: 0.1082 Acc: 0.9634\n",
      "val Loss: 0.0888 Acc: 0.9714\n",
      "\n",
      "Epoch 116/299\n",
      "----------\n",
      "train Loss: 0.1140 Acc: 0.9609\n",
      "val Loss: 0.0870 Acc: 0.9726\n",
      "\n",
      "Epoch 117/299\n",
      "----------\n",
      "train Loss: 0.1093 Acc: 0.9621\n",
      "val Loss: 0.0863 Acc: 0.9722\n",
      "\n",
      "Epoch 118/299\n",
      "----------\n",
      "train Loss: 0.1080 Acc: 0.9641\n",
      "val Loss: 0.0887 Acc: 0.9726\n",
      "\n",
      "Epoch 119/299\n",
      "----------\n",
      "train Loss: 0.1085 Acc: 0.9619\n",
      "val Loss: 0.0891 Acc: 0.9718\n",
      "\n",
      "Epoch 120/299\n",
      "----------\n",
      "train Loss: 0.1081 Acc: 0.9625\n",
      "val Loss: 0.0984 Acc: 0.9672\n",
      "\n",
      "Epoch 121/299\n",
      "----------\n",
      "train Loss: 0.1074 Acc: 0.9631\n",
      "val Loss: 0.0867 Acc: 0.9734\n",
      "\n",
      "Epoch 122/299\n",
      "----------\n",
      "train Loss: 0.1120 Acc: 0.9614\n",
      "val Loss: 0.0875 Acc: 0.9709\n",
      "\n",
      "Epoch 123/299\n",
      "----------\n",
      "train Loss: 0.1112 Acc: 0.9617\n",
      "val Loss: 0.0904 Acc: 0.9685\n",
      "\n",
      "Epoch 124/299\n",
      "----------\n",
      "train Loss: 0.1089 Acc: 0.9626\n",
      "val Loss: 0.0887 Acc: 0.9730\n",
      "\n",
      "Epoch 125/299\n",
      "----------\n",
      "train Loss: 0.1052 Acc: 0.9627\n",
      "val Loss: 0.0862 Acc: 0.9718\n",
      "\n",
      "Epoch 126/299\n",
      "----------\n",
      "train Loss: 0.1069 Acc: 0.9640\n",
      "val Loss: 0.0851 Acc: 0.9722\n",
      "\n",
      "Epoch 127/299\n",
      "----------\n",
      "train Loss: 0.1058 Acc: 0.9626\n",
      "val Loss: 0.0865 Acc: 0.9709\n",
      "\n",
      "Epoch 128/299\n",
      "----------\n",
      "train Loss: 0.1051 Acc: 0.9631\n",
      "val Loss: 0.0856 Acc: 0.9714\n",
      "\n",
      "Epoch 129/299\n",
      "----------\n",
      "train Loss: 0.1056 Acc: 0.9641\n",
      "val Loss: 0.0863 Acc: 0.9714\n",
      "\n",
      "Epoch 130/299\n",
      "----------\n",
      "train Loss: 0.1053 Acc: 0.9652\n",
      "val Loss: 0.0882 Acc: 0.9689\n",
      "\n",
      "Epoch 131/299\n",
      "----------\n",
      "train Loss: 0.1044 Acc: 0.9641\n",
      "val Loss: 0.0851 Acc: 0.9718\n",
      "\n",
      "Epoch 132/299\n",
      "----------\n",
      "train Loss: 0.1080 Acc: 0.9614\n",
      "val Loss: 0.0853 Acc: 0.9730\n",
      "\n",
      "Epoch 133/299\n",
      "----------\n",
      "train Loss: 0.1030 Acc: 0.9650\n",
      "val Loss: 0.0862 Acc: 0.9709\n",
      "\n",
      "Epoch 134/299\n",
      "----------\n",
      "train Loss: 0.1071 Acc: 0.9630\n",
      "val Loss: 0.0853 Acc: 0.9726\n",
      "\n",
      "Epoch 135/299\n",
      "----------\n",
      "train Loss: 0.1066 Acc: 0.9639\n",
      "val Loss: 0.0850 Acc: 0.9730\n",
      "\n",
      "Epoch 136/299\n",
      "----------\n",
      "train Loss: 0.1082 Acc: 0.9619\n",
      "val Loss: 0.0843 Acc: 0.9734\n",
      "\n",
      "Epoch 137/299\n",
      "----------\n",
      "train Loss: 0.1062 Acc: 0.9634\n",
      "val Loss: 0.0861 Acc: 0.9730\n",
      "\n",
      "Epoch 138/299\n",
      "----------\n",
      "train Loss: 0.1078 Acc: 0.9616\n",
      "val Loss: 0.0850 Acc: 0.9730\n",
      "\n",
      "Epoch 139/299\n",
      "----------\n",
      "train Loss: 0.1056 Acc: 0.9625\n",
      "val Loss: 0.0862 Acc: 0.9709\n",
      "\n",
      "Epoch 140/299\n",
      "----------\n",
      "train Loss: 0.1062 Acc: 0.9635\n",
      "val Loss: 0.0860 Acc: 0.9714\n",
      "\n",
      "Epoch 141/299\n",
      "----------\n",
      "train Loss: 0.1099 Acc: 0.9623\n",
      "val Loss: 0.0858 Acc: 0.9705\n",
      "\n",
      "Epoch 142/299\n",
      "----------\n",
      "train Loss: 0.1035 Acc: 0.9639\n",
      "val Loss: 0.0916 Acc: 0.9709\n",
      "\n",
      "Epoch 143/299\n",
      "----------\n",
      "train Loss: 0.1071 Acc: 0.9642\n",
      "val Loss: 0.0863 Acc: 0.9701\n",
      "\n",
      "Epoch 144/299\n",
      "----------\n",
      "train Loss: 0.1078 Acc: 0.9641\n",
      "val Loss: 0.0856 Acc: 0.9718\n",
      "\n",
      "Epoch 145/299\n",
      "----------\n",
      "train Loss: 0.1073 Acc: 0.9642\n",
      "val Loss: 0.0862 Acc: 0.9747\n",
      "\n",
      "Epoch 146/299\n",
      "----------\n",
      "train Loss: 0.1033 Acc: 0.9636\n",
      "val Loss: 0.0837 Acc: 0.9734\n",
      "\n",
      "Epoch 147/299\n",
      "----------\n",
      "train Loss: 0.1034 Acc: 0.9628\n",
      "val Loss: 0.0846 Acc: 0.9714\n",
      "\n",
      "Epoch 148/299\n",
      "----------\n",
      "train Loss: 0.1070 Acc: 0.9632\n",
      "val Loss: 0.0870 Acc: 0.9730\n",
      "\n",
      "Epoch 149/299\n",
      "----------\n",
      "train Loss: 0.1068 Acc: 0.9638\n",
      "val Loss: 0.0830 Acc: 0.9734\n",
      "\n",
      "Epoch 150/299\n",
      "----------\n",
      "train Loss: 0.1061 Acc: 0.9635\n",
      "val Loss: 0.0918 Acc: 0.9676\n",
      "\n",
      "Epoch 151/299\n",
      "----------\n",
      "train Loss: 0.1034 Acc: 0.9641\n",
      "val Loss: 0.0834 Acc: 0.9730\n",
      "\n",
      "Epoch 152/299\n",
      "----------\n",
      "train Loss: 0.1052 Acc: 0.9636\n",
      "val Loss: 0.0829 Acc: 0.9734\n",
      "\n",
      "Epoch 153/299\n",
      "----------\n",
      "train Loss: 0.1046 Acc: 0.9636\n",
      "val Loss: 0.0903 Acc: 0.9722\n",
      "\n",
      "Epoch 154/299\n",
      "----------\n",
      "train Loss: 0.1049 Acc: 0.9643\n",
      "val Loss: 0.0858 Acc: 0.9726\n",
      "\n",
      "Epoch 155/299\n",
      "----------\n",
      "train Loss: 0.1036 Acc: 0.9650\n",
      "val Loss: 0.0847 Acc: 0.9714\n",
      "\n",
      "Epoch 156/299\n",
      "----------\n",
      "train Loss: 0.1086 Acc: 0.9590\n",
      "val Loss: 0.0854 Acc: 0.9701\n",
      "\n",
      "Epoch 157/299\n",
      "----------\n",
      "train Loss: 0.1049 Acc: 0.9637\n",
      "val Loss: 0.0839 Acc: 0.9714\n",
      "\n",
      "Epoch 158/299\n",
      "----------\n",
      "train Loss: 0.1066 Acc: 0.9626\n",
      "val Loss: 0.0847 Acc: 0.9755\n",
      "\n",
      "Epoch 159/299\n",
      "----------\n",
      "train Loss: 0.1045 Acc: 0.9638\n",
      "val Loss: 0.0863 Acc: 0.9689\n",
      "\n",
      "Epoch 160/299\n",
      "----------\n",
      "train Loss: 0.1039 Acc: 0.9655\n",
      "val Loss: 0.0851 Acc: 0.9701\n",
      "\n",
      "Epoch 161/299\n",
      "----------\n",
      "train Loss: 0.1021 Acc: 0.9630\n",
      "val Loss: 0.0833 Acc: 0.9722\n",
      "\n",
      "Epoch 162/299\n",
      "----------\n",
      "train Loss: 0.1038 Acc: 0.9656\n",
      "val Loss: 0.0833 Acc: 0.9743\n",
      "\n",
      "Epoch 163/299\n",
      "----------\n",
      "train Loss: 0.1063 Acc: 0.9638\n",
      "val Loss: 0.0844 Acc: 0.9709\n",
      "\n",
      "Epoch 164/299\n",
      "----------\n",
      "train Loss: 0.1040 Acc: 0.9646\n",
      "val Loss: 0.0844 Acc: 0.9738\n",
      "\n",
      "Epoch 165/299\n",
      "----------\n",
      "train Loss: 0.1041 Acc: 0.9651\n",
      "val Loss: 0.0889 Acc: 0.9722\n",
      "\n",
      "Epoch 166/299\n",
      "----------\n",
      "train Loss: 0.1022 Acc: 0.9659\n",
      "val Loss: 0.0904 Acc: 0.9685\n",
      "\n",
      "Epoch 167/299\n",
      "----------\n",
      "train Loss: 0.1040 Acc: 0.9639\n",
      "val Loss: 0.0832 Acc: 0.9722\n",
      "\n",
      "Epoch 168/299\n",
      "----------\n",
      "train Loss: 0.1019 Acc: 0.9644\n",
      "val Loss: 0.0894 Acc: 0.9722\n",
      "\n",
      "Epoch 169/299\n",
      "----------\n",
      "train Loss: 0.1068 Acc: 0.9626\n",
      "val Loss: 0.0830 Acc: 0.9730\n",
      "\n",
      "Epoch 170/299\n",
      "----------\n",
      "train Loss: 0.1007 Acc: 0.9661\n",
      "val Loss: 0.0858 Acc: 0.9697\n",
      "\n",
      "Epoch 171/299\n",
      "----------\n",
      "train Loss: 0.0988 Acc: 0.9666\n",
      "val Loss: 0.0837 Acc: 0.9709\n",
      "\n",
      "Epoch 172/299\n",
      "----------\n",
      "train Loss: 0.1016 Acc: 0.9659\n",
      "val Loss: 0.0824 Acc: 0.9730\n",
      "\n",
      "Epoch 173/299\n",
      "----------\n",
      "train Loss: 0.1027 Acc: 0.9638\n",
      "val Loss: 0.0823 Acc: 0.9759\n",
      "\n",
      "Epoch 174/299\n",
      "----------\n",
      "train Loss: 0.1009 Acc: 0.9653\n",
      "val Loss: 0.0832 Acc: 0.9743\n",
      "\n",
      "Epoch 175/299\n",
      "----------\n",
      "train Loss: 0.1051 Acc: 0.9635\n",
      "val Loss: 0.0847 Acc: 0.9747\n",
      "\n",
      "Epoch 176/299\n",
      "----------\n",
      "train Loss: 0.1039 Acc: 0.9649\n",
      "val Loss: 0.0888 Acc: 0.9685\n",
      "\n",
      "Epoch 177/299\n",
      "----------\n",
      "train Loss: 0.1009 Acc: 0.9644\n",
      "val Loss: 0.0849 Acc: 0.9705\n",
      "\n",
      "Epoch 178/299\n",
      "----------\n",
      "train Loss: 0.1053 Acc: 0.9644\n",
      "val Loss: 0.0892 Acc: 0.9689\n",
      "\n",
      "Epoch 179/299\n",
      "----------\n",
      "train Loss: 0.1056 Acc: 0.9622\n",
      "val Loss: 0.0829 Acc: 0.9751\n",
      "\n",
      "Epoch 180/299\n",
      "----------\n",
      "train Loss: 0.1021 Acc: 0.9641\n",
      "val Loss: 0.0823 Acc: 0.9747\n",
      "\n",
      "Epoch 181/299\n",
      "----------\n",
      "train Loss: 0.1018 Acc: 0.9652\n",
      "val Loss: 0.0859 Acc: 0.9689\n",
      "\n",
      "Epoch 182/299\n",
      "----------\n",
      "train Loss: 0.1030 Acc: 0.9648\n",
      "val Loss: 0.0846 Acc: 0.9738\n",
      "\n",
      "Epoch 183/299\n",
      "----------\n",
      "train Loss: 0.1018 Acc: 0.9648\n",
      "val Loss: 0.0846 Acc: 0.9697\n",
      "\n",
      "Epoch 184/299\n",
      "----------\n",
      "train Loss: 0.1046 Acc: 0.9638\n",
      "val Loss: 0.0806 Acc: 0.9747\n",
      "\n",
      "Epoch 185/299\n",
      "----------\n",
      "train Loss: 0.1019 Acc: 0.9642\n",
      "val Loss: 0.0830 Acc: 0.9747\n",
      "\n",
      "Epoch 186/299\n",
      "----------\n",
      "train Loss: 0.1046 Acc: 0.9635\n",
      "val Loss: 0.0828 Acc: 0.9705\n",
      "\n",
      "Epoch 187/299\n",
      "----------\n",
      "train Loss: 0.1024 Acc: 0.9639\n",
      "val Loss: 0.0884 Acc: 0.9718\n",
      "\n",
      "Epoch 188/299\n",
      "----------\n",
      "train Loss: 0.0999 Acc: 0.9665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0826 Acc: 0.9714\n",
      "\n",
      "Epoch 189/299\n",
      "----------\n",
      "train Loss: 0.1013 Acc: 0.9653\n",
      "val Loss: 0.0815 Acc: 0.9759\n",
      "\n",
      "Epoch 190/299\n",
      "----------\n",
      "train Loss: 0.1011 Acc: 0.9658\n",
      "val Loss: 0.0814 Acc: 0.9734\n",
      "\n",
      "Epoch 191/299\n",
      "----------\n",
      "train Loss: 0.0998 Acc: 0.9644\n",
      "val Loss: 0.0830 Acc: 0.9755\n",
      "\n",
      "Epoch 192/299\n",
      "----------\n",
      "train Loss: 0.0964 Acc: 0.9663\n",
      "val Loss: 0.0831 Acc: 0.9709\n",
      "\n",
      "Epoch 193/299\n",
      "----------\n",
      "train Loss: 0.1016 Acc: 0.9651\n",
      "val Loss: 0.0827 Acc: 0.9705\n",
      "\n",
      "Epoch 194/299\n",
      "----------\n",
      "train Loss: 0.1014 Acc: 0.9646\n",
      "val Loss: 0.0870 Acc: 0.9693\n",
      "\n",
      "Epoch 195/299\n",
      "----------\n",
      "train Loss: 0.1037 Acc: 0.9641\n",
      "val Loss: 0.0819 Acc: 0.9730\n",
      "\n",
      "Epoch 196/299\n",
      "----------\n",
      "train Loss: 0.1010 Acc: 0.9650\n",
      "val Loss: 0.0854 Acc: 0.9701\n",
      "\n",
      "Epoch 197/299\n",
      "----------\n",
      "train Loss: 0.1034 Acc: 0.9627\n",
      "val Loss: 0.0821 Acc: 0.9718\n",
      "\n",
      "Epoch 198/299\n",
      "----------\n",
      "train Loss: 0.1019 Acc: 0.9654\n",
      "val Loss: 0.0805 Acc: 0.9747\n",
      "\n",
      "Epoch 199/299\n",
      "----------\n",
      "train Loss: 0.0982 Acc: 0.9663\n",
      "val Loss: 0.0803 Acc: 0.9743\n",
      "\n",
      "Epoch 200/299\n",
      "----------\n",
      "train Loss: 0.1020 Acc: 0.9637\n",
      "val Loss: 0.0799 Acc: 0.9743\n",
      "\n",
      "Epoch 201/299\n",
      "----------\n",
      "train Loss: 0.1036 Acc: 0.9631\n",
      "val Loss: 0.0798 Acc: 0.9751\n",
      "\n",
      "Epoch 202/299\n",
      "----------\n",
      "train Loss: 0.1001 Acc: 0.9646\n",
      "val Loss: 0.0809 Acc: 0.9726\n",
      "\n",
      "Epoch 203/299\n",
      "----------\n",
      "train Loss: 0.0994 Acc: 0.9651\n",
      "val Loss: 0.0844 Acc: 0.9689\n",
      "\n",
      "Epoch 204/299\n",
      "----------\n",
      "train Loss: 0.1023 Acc: 0.9654\n",
      "val Loss: 0.0802 Acc: 0.9730\n",
      "\n",
      "Epoch 205/299\n",
      "----------\n",
      "train Loss: 0.1039 Acc: 0.9618\n",
      "val Loss: 0.0792 Acc: 0.9747\n",
      "\n",
      "Epoch 206/299\n",
      "----------\n",
      "train Loss: 0.0990 Acc: 0.9640\n",
      "val Loss: 0.0860 Acc: 0.9730\n",
      "\n",
      "Epoch 207/299\n",
      "----------\n",
      "train Loss: 0.1024 Acc: 0.9647\n",
      "val Loss: 0.0793 Acc: 0.9751\n",
      "\n",
      "Epoch 208/299\n",
      "----------\n",
      "train Loss: 0.0999 Acc: 0.9647\n",
      "val Loss: 0.0807 Acc: 0.9730\n",
      "\n",
      "Epoch 209/299\n",
      "----------\n",
      "train Loss: 0.1015 Acc: 0.9662\n",
      "val Loss: 0.0789 Acc: 0.9747\n",
      "\n",
      "Epoch 210/299\n",
      "----------\n",
      "train Loss: 0.1003 Acc: 0.9649\n",
      "val Loss: 0.0800 Acc: 0.9734\n",
      "\n",
      "Epoch 211/299\n",
      "----------\n",
      "train Loss: 0.0996 Acc: 0.9648\n",
      "val Loss: 0.0836 Acc: 0.9693\n",
      "\n",
      "Epoch 212/299\n",
      "----------\n",
      "train Loss: 0.0989 Acc: 0.9665\n",
      "val Loss: 0.0792 Acc: 0.9747\n",
      "\n",
      "Epoch 213/299\n",
      "----------\n",
      "train Loss: 0.0966 Acc: 0.9653\n",
      "val Loss: 0.0787 Acc: 0.9747\n",
      "\n",
      "Epoch 214/299\n",
      "----------\n",
      "train Loss: 0.1002 Acc: 0.9646\n",
      "val Loss: 0.0795 Acc: 0.9759\n",
      "\n",
      "Epoch 215/299\n",
      "----------\n",
      "train Loss: 0.0958 Acc: 0.9667\n",
      "val Loss: 0.0788 Acc: 0.9738\n",
      "\n",
      "Epoch 216/299\n",
      "----------\n",
      "train Loss: 0.0990 Acc: 0.9673\n",
      "val Loss: 0.0794 Acc: 0.9738\n",
      "\n",
      "Epoch 217/299\n",
      "----------\n",
      "train Loss: 0.0991 Acc: 0.9648\n",
      "val Loss: 0.0808 Acc: 0.9709\n",
      "\n",
      "Epoch 218/299\n",
      "----------\n",
      "train Loss: 0.0989 Acc: 0.9648\n",
      "val Loss: 0.0835 Acc: 0.9738\n",
      "\n",
      "Epoch 219/299\n",
      "----------\n",
      "train Loss: 0.1000 Acc: 0.9659\n",
      "val Loss: 0.0813 Acc: 0.9759\n",
      "\n",
      "Epoch 220/299\n",
      "----------\n",
      "train Loss: 0.0968 Acc: 0.9676\n",
      "val Loss: 0.0787 Acc: 0.9755\n",
      "\n",
      "Epoch 221/299\n",
      "----------\n",
      "train Loss: 0.0980 Acc: 0.9675\n",
      "val Loss: 0.0785 Acc: 0.9747\n",
      "\n",
      "Epoch 222/299\n",
      "----------\n",
      "train Loss: 0.0988 Acc: 0.9651\n",
      "val Loss: 0.0814 Acc: 0.9722\n",
      "\n",
      "Epoch 223/299\n",
      "----------\n",
      "train Loss: 0.1006 Acc: 0.9642\n",
      "val Loss: 0.0790 Acc: 0.9743\n",
      "\n",
      "Epoch 224/299\n",
      "----------\n",
      "train Loss: 0.0979 Acc: 0.9654\n",
      "val Loss: 0.0824 Acc: 0.9751\n",
      "\n",
      "Epoch 225/299\n",
      "----------\n",
      "train Loss: 0.1009 Acc: 0.9643\n",
      "val Loss: 0.0785 Acc: 0.9751\n",
      "\n",
      "Epoch 226/299\n",
      "----------\n",
      "train Loss: 0.0966 Acc: 0.9647\n",
      "val Loss: 0.0814 Acc: 0.9714\n",
      "\n",
      "Epoch 227/299\n",
      "----------\n",
      "train Loss: 0.0991 Acc: 0.9656\n",
      "val Loss: 0.0797 Acc: 0.9734\n",
      "\n",
      "Epoch 228/299\n",
      "----------\n",
      "train Loss: 0.1001 Acc: 0.9636\n",
      "val Loss: 0.0797 Acc: 0.9726\n",
      "\n",
      "Epoch 229/299\n",
      "----------\n",
      "train Loss: 0.0954 Acc: 0.9672\n",
      "val Loss: 0.0794 Acc: 0.9726\n",
      "\n",
      "Epoch 230/299\n",
      "----------\n",
      "train Loss: 0.1006 Acc: 0.9638\n",
      "val Loss: 0.0777 Acc: 0.9755\n",
      "\n",
      "Epoch 231/299\n",
      "----------\n",
      "train Loss: 0.0982 Acc: 0.9638\n",
      "val Loss: 0.0785 Acc: 0.9759\n",
      "\n",
      "Epoch 232/299\n",
      "----------\n",
      "train Loss: 0.1014 Acc: 0.9644\n",
      "val Loss: 0.0793 Acc: 0.9743\n",
      "\n",
      "Epoch 233/299\n",
      "----------\n",
      "train Loss: 0.0996 Acc: 0.9649\n",
      "val Loss: 0.0812 Acc: 0.9714\n",
      "\n",
      "Epoch 234/299\n",
      "----------\n",
      "train Loss: 0.0989 Acc: 0.9662\n",
      "val Loss: 0.0785 Acc: 0.9768\n",
      "\n",
      "Epoch 235/299\n",
      "----------\n",
      "train Loss: 0.0968 Acc: 0.9670\n",
      "val Loss: 0.0779 Acc: 0.9759\n",
      "\n",
      "Epoch 236/299\n",
      "----------\n",
      "train Loss: 0.0985 Acc: 0.9652\n",
      "val Loss: 0.0814 Acc: 0.9709\n",
      "\n",
      "Epoch 237/299\n",
      "----------\n",
      "train Loss: 0.0982 Acc: 0.9677\n",
      "val Loss: 0.0827 Acc: 0.9697\n",
      "\n",
      "Epoch 238/299\n",
      "----------\n",
      "train Loss: 0.0969 Acc: 0.9656\n",
      "val Loss: 0.0798 Acc: 0.9718\n",
      "\n",
      "Epoch 239/299\n",
      "----------\n",
      "train Loss: 0.0964 Acc: 0.9667\n",
      "val Loss: 0.0893 Acc: 0.9676\n",
      "\n",
      "Epoch 240/299\n",
      "----------\n",
      "train Loss: 0.1011 Acc: 0.9654\n",
      "val Loss: 0.0805 Acc: 0.9709\n",
      "\n",
      "Epoch 241/299\n",
      "----------\n",
      "train Loss: 0.0980 Acc: 0.9659\n",
      "val Loss: 0.0843 Acc: 0.9734\n",
      "\n",
      "Epoch 242/299\n",
      "----------\n",
      "train Loss: 0.0967 Acc: 0.9661\n",
      "val Loss: 0.0798 Acc: 0.9734\n",
      "\n",
      "Epoch 243/299\n",
      "----------\n",
      "train Loss: 0.0977 Acc: 0.9651\n",
      "val Loss: 0.0787 Acc: 0.9759\n",
      "\n",
      "Epoch 244/299\n",
      "----------\n",
      "train Loss: 0.0952 Acc: 0.9677\n",
      "val Loss: 0.0797 Acc: 0.9763\n",
      "\n",
      "Epoch 245/299\n",
      "----------\n",
      "train Loss: 0.1000 Acc: 0.9661\n",
      "val Loss: 0.0785 Acc: 0.9747\n",
      "\n",
      "Epoch 246/299\n",
      "----------\n",
      "train Loss: 0.0974 Acc: 0.9657\n",
      "val Loss: 0.0775 Acc: 0.9755\n",
      "\n",
      "Epoch 247/299\n",
      "----------\n",
      "train Loss: 0.0965 Acc: 0.9680\n",
      "val Loss: 0.0907 Acc: 0.9668\n",
      "\n",
      "Epoch 248/299\n",
      "----------\n",
      "train Loss: 0.1011 Acc: 0.9639\n",
      "val Loss: 0.0775 Acc: 0.9751\n",
      "\n",
      "Epoch 249/299\n",
      "----------\n",
      "train Loss: 0.0973 Acc: 0.9661\n",
      "val Loss: 0.0777 Acc: 0.9763\n",
      "\n",
      "Epoch 250/299\n",
      "----------\n",
      "train Loss: 0.0960 Acc: 0.9654\n",
      "val Loss: 0.0779 Acc: 0.9768\n",
      "\n",
      "Epoch 251/299\n",
      "----------\n",
      "train Loss: 0.0971 Acc: 0.9659\n",
      "val Loss: 0.0774 Acc: 0.9755\n",
      "\n",
      "Epoch 252/299\n",
      "----------\n",
      "train Loss: 0.0977 Acc: 0.9657\n",
      "val Loss: 0.0782 Acc: 0.9751\n",
      "\n",
      "Epoch 253/299\n",
      "----------\n",
      "train Loss: 0.0982 Acc: 0.9651\n",
      "val Loss: 0.0853 Acc: 0.9747\n",
      "\n",
      "Epoch 254/299\n",
      "----------\n",
      "train Loss: 0.1009 Acc: 0.9659\n",
      "val Loss: 0.0770 Acc: 0.9768\n",
      "\n",
      "Epoch 255/299\n",
      "----------\n",
      "train Loss: 0.0992 Acc: 0.9666\n",
      "val Loss: 0.0766 Acc: 0.9759\n",
      "\n",
      "Epoch 256/299\n",
      "----------\n",
      "train Loss: 0.0986 Acc: 0.9646\n",
      "val Loss: 0.0769 Acc: 0.9759\n",
      "\n",
      "Epoch 257/299\n",
      "----------\n",
      "train Loss: 0.1010 Acc: 0.9641\n",
      "val Loss: 0.0778 Acc: 0.9755\n",
      "\n",
      "Epoch 258/299\n",
      "----------\n",
      "train Loss: 0.0995 Acc: 0.9652\n",
      "val Loss: 0.0773 Acc: 0.9759\n",
      "\n",
      "Epoch 259/299\n",
      "----------\n",
      "train Loss: 0.0974 Acc: 0.9646\n",
      "val Loss: 0.0775 Acc: 0.9759\n",
      "\n",
      "Epoch 260/299\n",
      "----------\n",
      "train Loss: 0.0974 Acc: 0.9667\n",
      "val Loss: 0.0780 Acc: 0.9763\n",
      "\n",
      "Epoch 261/299\n",
      "----------\n",
      "train Loss: 0.0951 Acc: 0.9655\n",
      "val Loss: 0.0776 Acc: 0.9768\n",
      "\n",
      "Epoch 262/299\n",
      "----------\n",
      "train Loss: 0.0976 Acc: 0.9639\n",
      "val Loss: 0.0770 Acc: 0.9759\n",
      "\n",
      "Epoch 263/299\n",
      "----------\n",
      "train Loss: 0.0974 Acc: 0.9669\n",
      "val Loss: 0.0859 Acc: 0.9738\n",
      "\n",
      "Epoch 264/299\n",
      "----------\n",
      "train Loss: 0.0966 Acc: 0.9666\n",
      "val Loss: 0.0764 Acc: 0.9743\n",
      "\n",
      "Epoch 265/299\n",
      "----------\n",
      "train Loss: 0.0969 Acc: 0.9668\n",
      "val Loss: 0.0788 Acc: 0.9763\n",
      "\n",
      "Epoch 266/299\n",
      "----------\n",
      "train Loss: 0.0965 Acc: 0.9646\n",
      "val Loss: 0.0771 Acc: 0.9763\n",
      "\n",
      "Epoch 267/299\n",
      "----------\n",
      "train Loss: 0.0962 Acc: 0.9664\n",
      "val Loss: 0.0772 Acc: 0.9743\n",
      "\n",
      "Epoch 268/299\n",
      "----------\n",
      "train Loss: 0.0920 Acc: 0.9649\n",
      "val Loss: 0.0780 Acc: 0.9772\n",
      "\n",
      "Epoch 269/299\n",
      "----------\n",
      "train Loss: 0.0976 Acc: 0.9675\n",
      "val Loss: 0.0760 Acc: 0.9747\n",
      "\n",
      "Epoch 270/299\n",
      "----------\n",
      "train Loss: 0.0997 Acc: 0.9646\n",
      "val Loss: 0.0790 Acc: 0.9714\n",
      "\n",
      "Epoch 271/299\n",
      "----------\n",
      "train Loss: 0.0975 Acc: 0.9668\n",
      "val Loss: 0.0776 Acc: 0.9743\n",
      "\n",
      "Epoch 272/299\n",
      "----------\n",
      "train Loss: 0.0953 Acc: 0.9667\n",
      "val Loss: 0.0776 Acc: 0.9772\n",
      "\n",
      "Epoch 273/299\n",
      "----------\n",
      "train Loss: 0.0931 Acc: 0.9690\n",
      "val Loss: 0.0777 Acc: 0.9743\n",
      "\n",
      "Epoch 274/299\n",
      "----------\n",
      "train Loss: 0.0976 Acc: 0.9654\n",
      "val Loss: 0.0784 Acc: 0.9726\n",
      "\n",
      "Epoch 275/299\n",
      "----------\n",
      "train Loss: 0.0975 Acc: 0.9642\n",
      "val Loss: 0.0773 Acc: 0.9743\n",
      "\n",
      "Epoch 276/299\n",
      "----------\n",
      "train Loss: 0.0984 Acc: 0.9668\n",
      "val Loss: 0.0767 Acc: 0.9751\n",
      "\n",
      "Epoch 277/299\n",
      "----------\n",
      "train Loss: 0.0971 Acc: 0.9638\n",
      "val Loss: 0.0764 Acc: 0.9755\n",
      "\n",
      "Epoch 278/299\n",
      "----------\n",
      "train Loss: 0.0964 Acc: 0.9669\n",
      "val Loss: 0.0766 Acc: 0.9759\n",
      "\n",
      "Epoch 279/299\n",
      "----------\n",
      "train Loss: 0.0935 Acc: 0.9662\n",
      "val Loss: 0.0785 Acc: 0.9714\n",
      "\n",
      "Epoch 280/299\n",
      "----------\n",
      "train Loss: 0.0987 Acc: 0.9651\n",
      "val Loss: 0.0775 Acc: 0.9743\n",
      "\n",
      "Epoch 281/299\n",
      "----------\n",
      "train Loss: 0.0965 Acc: 0.9653\n",
      "val Loss: 0.0765 Acc: 0.9759\n",
      "\n",
      "Epoch 282/299\n",
      "----------\n",
      "train Loss: 0.0935 Acc: 0.9666\n",
      "val Loss: 0.0852 Acc: 0.9685\n",
      "\n",
      "Epoch 283/299\n",
      "----------\n",
      "train Loss: 0.0939 Acc: 0.9655\n",
      "val Loss: 0.0763 Acc: 0.9763\n",
      "\n",
      "Epoch 284/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0967 Acc: 0.9654\n",
      "val Loss: 0.0760 Acc: 0.9759\n",
      "\n",
      "Epoch 285/299\n",
      "----------\n",
      "train Loss: 0.0932 Acc: 0.9663\n",
      "val Loss: 0.0813 Acc: 0.9709\n",
      "\n",
      "Epoch 286/299\n",
      "----------\n",
      "train Loss: 0.0959 Acc: 0.9648\n",
      "val Loss: 0.0792 Acc: 0.9763\n",
      "\n",
      "Epoch 287/299\n",
      "----------\n",
      "train Loss: 0.0966 Acc: 0.9654\n",
      "val Loss: 0.0837 Acc: 0.9689\n",
      "\n",
      "Epoch 288/299\n",
      "----------\n",
      "train Loss: 0.0962 Acc: 0.9657\n",
      "val Loss: 0.0764 Acc: 0.9763\n",
      "\n",
      "Epoch 289/299\n",
      "----------\n",
      "train Loss: 0.0938 Acc: 0.9673\n",
      "val Loss: 0.0753 Acc: 0.9763\n",
      "\n",
      "Epoch 290/299\n",
      "----------\n",
      "train Loss: 0.0983 Acc: 0.9673\n",
      "val Loss: 0.0755 Acc: 0.9751\n",
      "\n",
      "Epoch 291/299\n",
      "----------\n",
      "train Loss: 0.0983 Acc: 0.9648\n",
      "val Loss: 0.0754 Acc: 0.9755\n",
      "\n",
      "Epoch 292/299\n",
      "----------\n",
      "train Loss: 0.0953 Acc: 0.9651\n",
      "val Loss: 0.0813 Acc: 0.9747\n",
      "\n",
      "Epoch 293/299\n",
      "----------\n",
      "train Loss: 0.0964 Acc: 0.9653\n",
      "val Loss: 0.0881 Acc: 0.9672\n",
      "\n",
      "Epoch 294/299\n",
      "----------\n",
      "train Loss: 0.0984 Acc: 0.9659\n",
      "val Loss: 0.0766 Acc: 0.9768\n",
      "\n",
      "Epoch 295/299\n",
      "----------\n",
      "train Loss: 0.0975 Acc: 0.9678\n",
      "val Loss: 0.0777 Acc: 0.9738\n",
      "\n",
      "Epoch 296/299\n",
      "----------\n",
      "train Loss: 0.0961 Acc: 0.9659\n",
      "val Loss: 0.0848 Acc: 0.9685\n",
      "\n",
      "Epoch 297/299\n",
      "----------\n",
      "train Loss: 0.0937 Acc: 0.9677\n",
      "val Loss: 0.0838 Acc: 0.9726\n",
      "\n",
      "Epoch 298/299\n",
      "----------\n",
      "train Loss: 0.0983 Acc: 0.9640\n",
      "val Loss: 0.0756 Acc: 0.9751\n",
      "\n",
      "Epoch 299/299\n",
      "----------\n",
      "train Loss: 0.0911 Acc: 0.9676\n",
      "val Loss: 0.0786 Acc: 0.9772\n",
      "\n",
      "\n",
      "##############################\n",
      "------ Summary ------\n",
      "model -> squeezenet\n",
      "epochs -> 300\n",
      "lr -> 0.0001\n",
      "batch size -> 8\n",
      "optimizer -> SGD\n",
      "criteriun -> CrossEntropyLoss\n",
      "\n",
      "Training complete in 152m 46s\n",
      "Best val Acc: 0.977169\n",
      "##############################\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== END ====\n",
      "[[791   0   0]\n",
      " [  2 770  27]\n",
      " [  6  20 793]]\n",
      "\n",
      "Sensitivity or recall total\n",
      "0.9771689497716894\n",
      "\n",
      "Sensitivity or recall per classes\n",
      "[1.   0.96 0.97]\n",
      "\n",
      "Precision\n",
      "[0.99 0.97 0.97]\n",
      "\n",
      "F1 Score\n",
      "[0.99 0.97 0.97]\n",
      "Confusion matrix, without normalization\n",
      "\n",
      "============================================================\n",
      "==== INITIALIZING WITH PARAMETERS: ====\n",
      "model -> squeezenet\n",
      "epochs -> 300\n",
      "lr -> 0.0001\n",
      "batch size -> 8\n",
      "optimizer -> 1\n",
      "criteriun -> 2\n",
      "\n",
      "--------------------\n",
      "Params to learn:\n",
      "\t classifier.1.weight\n",
      "\t classifier.1.bias\n",
      "\n",
      "--------------------\n",
      "\n",
      "== Epochs ==\n",
      "Epoch 0/299\n",
      "----------\n",
      "train Loss: -285.7825 Acc: 0.4383\n",
      "val Loss: -590.8180 Acc: 0.3404\n",
      "\n",
      "Epoch 1/299\n",
      "----------\n",
      "train Loss: -890.0951 Acc: 0.3435\n",
      "val Loss: -1198.0694 Acc: 0.3416\n",
      "\n",
      "Epoch 2/299\n",
      "----------\n",
      "train Loss: -1494.6983 Acc: 0.3429\n",
      "val Loss: -1805.0861 Acc: 0.3421\n",
      "\n",
      "Epoch 3/299\n",
      "----------\n",
      "train Loss: -2100.7056 Acc: 0.3466\n",
      "val Loss: -2412.7816 Acc: 0.3425\n",
      "\n",
      "Epoch 4/299\n",
      "----------\n",
      "train Loss: -2706.1923 Acc: 0.3458\n",
      "val Loss: -3020.1276 Acc: 0.3445\n",
      "\n",
      "Epoch 5/299\n",
      "----------\n",
      "train Loss: -3310.2327 Acc: 0.3470\n",
      "val Loss: -3627.4950 Acc: 0.3437\n",
      "\n",
      "Epoch 6/299\n",
      "----------\n",
      "train Loss: -3914.9403 Acc: 0.3456\n",
      "val Loss: -4234.7916 Acc: 0.3445\n",
      "\n",
      "Epoch 7/299\n",
      "----------\n",
      "train Loss: -4520.1017 Acc: 0.3472\n",
      "val Loss: -4841.9878 Acc: 0.3454\n",
      "\n",
      "Epoch 8/299\n",
      "----------\n",
      "train Loss: -5127.2646 Acc: 0.3460\n",
      "val Loss: -5449.7987 Acc: 0.3441\n",
      "\n",
      "Epoch 9/299\n",
      "----------\n",
      "train Loss: -5729.7062 Acc: 0.3494\n",
      "val Loss: -6056.8573 Acc: 0.3450\n",
      "\n",
      "Epoch 10/299\n",
      "----------\n",
      "train Loss: -6334.4102 Acc: 0.3469\n",
      "val Loss: -6664.1698 Acc: 0.3450\n",
      "\n",
      "Epoch 11/299\n",
      "----------\n",
      "train Loss: -6943.1298 Acc: 0.3487\n",
      "val Loss: -7271.7730 Acc: 0.3450\n",
      "\n",
      "Epoch 12/299\n",
      "----------\n",
      "train Loss: -7547.4504 Acc: 0.3489\n",
      "val Loss: -7879.2505 Acc: 0.3450\n",
      "\n",
      "Epoch 13/299\n",
      "----------\n",
      "train Loss: -8149.4545 Acc: 0.3480\n",
      "val Loss: -8486.3732 Acc: 0.3450\n",
      "\n",
      "Epoch 14/299\n",
      "----------\n",
      "train Loss: -8755.6583 Acc: 0.3478\n",
      "val Loss: -9093.7052 Acc: 0.3450\n",
      "\n",
      "Epoch 15/299\n",
      "----------\n",
      "train Loss: -9358.6500 Acc: 0.3496\n",
      "val Loss: -9701.1322 Acc: 0.3450\n",
      "\n",
      "Epoch 16/299\n",
      "----------\n",
      "train Loss: -9968.6596 Acc: 0.3496\n",
      "val Loss: -10308.5689 Acc: 0.3450\n",
      "\n",
      "Epoch 17/299\n",
      "----------\n",
      "train Loss: -10567.5304 Acc: 0.3473\n",
      "val Loss: -10915.5847 Acc: 0.3450\n",
      "\n",
      "Epoch 18/299\n",
      "----------\n",
      "train Loss: -11174.2862 Acc: 0.3479\n",
      "val Loss: -11522.9220 Acc: 0.3454\n",
      "\n",
      "Epoch 19/299\n",
      "----------\n",
      "train Loss: -11780.1206 Acc: 0.3484\n",
      "val Loss: -12130.3029 Acc: 0.3450\n",
      "\n",
      "Epoch 20/299\n",
      "----------\n",
      "train Loss: -12388.4869 Acc: 0.3482\n",
      "val Loss: -12737.9213 Acc: 0.3454\n",
      "\n",
      "Epoch 21/299\n",
      "----------\n",
      "train Loss: -12993.0074 Acc: 0.3481\n",
      "val Loss: -13345.1695 Acc: 0.3454\n",
      "\n",
      "Epoch 22/299\n",
      "----------\n",
      "train Loss: -13595.1226 Acc: 0.3475\n",
      "val Loss: -13952.6019 Acc: 0.3450\n",
      "\n",
      "Epoch 23/299\n",
      "----------\n",
      "train Loss: -14198.7189 Acc: 0.3486\n",
      "val Loss: -14559.7643 Acc: 0.3450\n",
      "\n",
      "Epoch 24/299\n",
      "----------\n",
      "train Loss: -14802.9607 Acc: 0.3480\n",
      "val Loss: -15167.1567 Acc: 0.3450\n",
      "\n",
      "Epoch 25/299\n",
      "----------\n",
      "train Loss: -15405.0458 Acc: 0.3475\n",
      "val Loss: -15774.3582 Acc: 0.3450\n",
      "\n",
      "Epoch 26/299\n",
      "----------\n",
      "train Loss: -16019.4675 Acc: 0.3480\n",
      "val Loss: -16381.8026 Acc: 0.3450\n",
      "\n",
      "Epoch 27/299\n",
      "----------\n",
      "train Loss: -16624.6574 Acc: 0.3471\n",
      "val Loss: -16989.1823 Acc: 0.3454\n",
      "\n",
      "Epoch 28/299\n",
      "----------\n",
      "train Loss: -17223.8036 Acc: 0.3476\n",
      "val Loss: -17596.4433 Acc: 0.3454\n",
      "\n",
      "Epoch 29/299\n",
      "----------\n",
      "train Loss: -17836.5408 Acc: 0.3483\n",
      "val Loss: -18204.0055 Acc: 0.3458\n",
      "\n",
      "Epoch 30/299\n",
      "----------\n",
      "train Loss: -18439.0530 Acc: 0.3475\n",
      "val Loss: -18811.5330 Acc: 0.3454\n",
      "\n",
      "Epoch 31/299\n",
      "----------\n",
      "train Loss: -19038.4856 Acc: 0.3481\n",
      "val Loss: -19418.7614 Acc: 0.3454\n",
      "\n",
      "Epoch 32/299\n",
      "----------\n",
      "train Loss: -19648.1702 Acc: 0.3476\n",
      "val Loss: -20026.1974 Acc: 0.3454\n",
      "\n",
      "Epoch 33/299\n",
      "----------\n",
      "train Loss: -20251.4495 Acc: 0.3478\n",
      "val Loss: -20633.5021 Acc: 0.3454\n",
      "\n",
      "Epoch 34/299\n",
      "----------\n",
      "train Loss: -20861.9160 Acc: 0.3478\n",
      "val Loss: -21241.0382 Acc: 0.3454\n",
      "\n",
      "Epoch 35/299\n",
      "----------\n",
      "train Loss: -21461.0056 Acc: 0.3480\n",
      "val Loss: -21848.3162 Acc: 0.3454\n",
      "\n",
      "Epoch 36/299\n",
      "----------\n",
      "train Loss: -22066.3795 Acc: 0.3486\n",
      "val Loss: -22455.7098 Acc: 0.3454\n",
      "\n",
      "Epoch 37/299\n",
      "----------\n",
      "train Loss: -22675.3610 Acc: 0.3470\n",
      "val Loss: -23063.1078 Acc: 0.3454\n",
      "\n",
      "Epoch 38/299\n",
      "----------\n",
      "train Loss: -23289.9417 Acc: 0.3487\n",
      "val Loss: -23670.8294 Acc: 0.3454\n",
      "\n",
      "Epoch 39/299\n",
      "----------\n",
      "train Loss: -23887.7860 Acc: 0.3486\n",
      "val Loss: -24278.4079 Acc: 0.3454\n",
      "\n",
      "Epoch 40/299\n",
      "----------\n",
      "train Loss: -24489.0085 Acc: 0.3482\n",
      "val Loss: -24885.8440 Acc: 0.3454\n",
      "\n",
      "Epoch 41/299\n",
      "----------\n",
      "train Loss: -25104.1578 Acc: 0.3481\n",
      "val Loss: -25493.3518 Acc: 0.3454\n",
      "\n",
      "Epoch 42/299\n",
      "----------\n",
      "train Loss: -25698.5261 Acc: 0.3475\n",
      "val Loss: -26100.7471 Acc: 0.3454\n",
      "\n",
      "Epoch 43/299\n",
      "----------\n",
      "train Loss: -26299.1711 Acc: 0.3479\n",
      "val Loss: -26708.0276 Acc: 0.3454\n",
      "\n",
      "Epoch 44/299\n",
      "----------\n",
      "train Loss: -26902.7402 Acc: 0.3484\n",
      "val Loss: -27315.3129 Acc: 0.3454\n",
      "\n",
      "Epoch 45/299\n",
      "----------\n",
      "train Loss: -27516.6071 Acc: 0.3481\n",
      "val Loss: -27922.7526 Acc: 0.3454\n",
      "\n",
      "Epoch 46/299\n",
      "----------\n",
      "train Loss: -28111.8727 Acc: 0.3478\n",
      "val Loss: -28530.0346 Acc: 0.3454\n",
      "\n",
      "Epoch 47/299\n",
      "----------\n",
      "train Loss: -28716.8366 Acc: 0.3486\n",
      "val Loss: -29137.1899 Acc: 0.3454\n",
      "\n",
      "Epoch 48/299\n",
      "----------\n",
      "train Loss: -29316.1137 Acc: 0.3493\n",
      "val Loss: -29744.4091 Acc: 0.3454\n",
      "\n",
      "Epoch 49/299\n",
      "----------\n",
      "train Loss: -29946.2857 Acc: 0.3479\n",
      "val Loss: -30351.9468 Acc: 0.3454\n",
      "\n",
      "Epoch 50/299\n",
      "----------\n",
      "train Loss: -30527.2769 Acc: 0.3481\n",
      "val Loss: -30959.2041 Acc: 0.3454\n",
      "\n",
      "Epoch 51/299\n",
      "----------\n",
      "train Loss: -31147.1281 Acc: 0.3480\n",
      "val Loss: -31566.5564 Acc: 0.3454\n",
      "\n",
      "Epoch 52/299\n",
      "----------\n",
      "train Loss: -31749.7552 Acc: 0.3477\n",
      "val Loss: -32174.1253 Acc: 0.3454\n",
      "\n",
      "Epoch 53/299\n",
      "----------\n",
      "train Loss: -32355.2703 Acc: 0.3482\n",
      "val Loss: -32781.3725 Acc: 0.3454\n",
      "\n",
      "Epoch 54/299\n",
      "----------\n",
      "train Loss: -32975.7288 Acc: 0.3478\n",
      "val Loss: -33389.1139 Acc: 0.3454\n",
      "\n",
      "Epoch 55/299\n",
      "----------\n",
      "train Loss: -33565.2810 Acc: 0.3478\n",
      "val Loss: -33996.4440 Acc: 0.3454\n",
      "\n",
      "Epoch 56/299\n",
      "----------\n",
      "train Loss: -34169.2640 Acc: 0.3490\n",
      "val Loss: -34603.7686 Acc: 0.3454\n",
      "\n",
      "Epoch 57/299\n",
      "----------\n",
      "train Loss: -34782.3105 Acc: 0.3485\n",
      "val Loss: -35211.1569 Acc: 0.3454\n",
      "\n",
      "Epoch 58/299\n",
      "----------\n",
      "train Loss: -35379.0683 Acc: 0.3484\n",
      "val Loss: -35818.7158 Acc: 0.3454\n",
      "\n",
      "Epoch 59/299\n",
      "----------\n",
      "train Loss: -35983.9568 Acc: 0.3482\n",
      "val Loss: -36426.1229 Acc: 0.3454\n",
      "\n",
      "Epoch 60/299\n",
      "----------\n",
      "train Loss: -36580.8512 Acc: 0.3478\n",
      "val Loss: -37033.2754 Acc: 0.3454\n",
      "\n",
      "Epoch 61/299\n",
      "----------\n",
      "train Loss: -37195.1267 Acc: 0.3484\n",
      "val Loss: -37640.6484 Acc: 0.3454\n",
      "\n",
      "Epoch 62/299\n",
      "----------\n",
      "train Loss: -37788.7887 Acc: 0.3483\n",
      "val Loss: -38248.0677 Acc: 0.3454\n",
      "\n",
      "Epoch 63/299\n",
      "----------\n",
      "train Loss: -38401.2974 Acc: 0.3491\n",
      "val Loss: -38855.1780 Acc: 0.3454\n",
      "\n",
      "Epoch 64/299\n",
      "----------\n",
      "train Loss: -38985.9878 Acc: 0.3482\n",
      "val Loss: -39462.3535 Acc: 0.3454\n",
      "\n",
      "Epoch 65/299\n",
      "----------\n",
      "train Loss: -39621.4243 Acc: 0.3481\n",
      "val Loss: -40069.7084 Acc: 0.3454\n",
      "\n",
      "Epoch 66/299\n",
      "----------\n",
      "train Loss: -40212.5731 Acc: 0.3495\n",
      "val Loss: -40676.8947 Acc: 0.3454\n",
      "\n",
      "Epoch 67/299\n",
      "----------\n",
      "train Loss: -40823.8909 Acc: 0.3475\n",
      "val Loss: -41284.2070 Acc: 0.3454\n",
      "\n",
      "Epoch 68/299\n",
      "----------\n",
      "train Loss: -41428.8754 Acc: 0.3486\n",
      "val Loss: -41891.7846 Acc: 0.3454\n",
      "\n",
      "Epoch 69/299\n",
      "----------\n",
      "train Loss: -42041.3015 Acc: 0.3485\n",
      "val Loss: -42499.1681 Acc: 0.3454\n",
      "\n",
      "Epoch 70/299\n",
      "----------\n",
      "train Loss: -42643.7801 Acc: 0.3493\n",
      "val Loss: -43106.5688 Acc: 0.3454\n",
      "\n",
      "Epoch 71/299\n",
      "----------\n",
      "train Loss: -43232.1669 Acc: 0.3481\n",
      "val Loss: -43713.8489 Acc: 0.3454\n",
      "\n",
      "Epoch 72/299\n",
      "----------\n",
      "train Loss: -43856.7299 Acc: 0.3496\n",
      "val Loss: -44321.3438 Acc: 0.3454\n",
      "\n",
      "Epoch 73/299\n",
      "----------\n",
      "train Loss: -44442.1273 Acc: 0.3476\n",
      "val Loss: -44928.5043 Acc: 0.3454\n",
      "\n",
      "Epoch 74/299\n",
      "----------\n",
      "train Loss: -45073.6800 Acc: 0.3471\n",
      "val Loss: -45535.9821 Acc: 0.3454\n",
      "\n",
      "Epoch 75/299\n",
      "----------\n",
      "train Loss: -45667.3871 Acc: 0.3478\n",
      "val Loss: -46143.5251 Acc: 0.3454\n",
      "\n",
      "Epoch 76/299\n",
      "----------\n",
      "train Loss: -46265.3629 Acc: 0.3483\n",
      "val Loss: -46750.8475 Acc: 0.3454\n",
      "\n",
      "Epoch 77/299\n",
      "----------\n",
      "train Loss: -46863.9501 Acc: 0.3481\n",
      "val Loss: -47358.0674 Acc: 0.3454\n",
      "\n",
      "Epoch 78/299\n",
      "----------\n",
      "train Loss: -47467.6080 Acc: 0.3476\n",
      "val Loss: -47965.2312 Acc: 0.3454\n",
      "\n",
      "Epoch 79/299\n",
      "----------\n",
      "train Loss: -48086.3738 Acc: 0.3478\n",
      "val Loss: -48572.5977 Acc: 0.3454\n",
      "\n",
      "Epoch 80/299\n",
      "----------\n",
      "train Loss: -48680.2496 Acc: 0.3472\n",
      "val Loss: -49180.0047 Acc: 0.3454\n",
      "\n",
      "Epoch 81/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: -49303.1138 Acc: 0.3487\n",
      "val Loss: -49787.2691 Acc: 0.3454\n",
      "\n",
      "Epoch 82/299\n",
      "----------\n",
      "train Loss: -49911.9912 Acc: 0.3473\n",
      "val Loss: -50394.9193 Acc: 0.3454\n",
      "\n",
      "Epoch 83/299\n",
      "----------\n",
      "train Loss: -50491.2469 Acc: 0.3486\n",
      "val Loss: -51002.1105 Acc: 0.3454\n",
      "\n",
      "Epoch 84/299\n",
      "----------\n",
      "train Loss: -51110.0896 Acc: 0.3493\n",
      "val Loss: -51609.4218 Acc: 0.3454\n",
      "\n",
      "Epoch 85/299\n",
      "----------\n",
      "train Loss: -51726.6759 Acc: 0.3489\n",
      "val Loss: -52216.9312 Acc: 0.3454\n",
      "\n",
      "Epoch 86/299\n",
      "----------\n",
      "train Loss: -52337.6243 Acc: 0.3489\n",
      "val Loss: -52824.4902 Acc: 0.3454\n",
      "\n",
      "Epoch 87/299\n",
      "----------\n",
      "train Loss: -52927.8163 Acc: 0.3479\n",
      "val Loss: -53432.0363 Acc: 0.3454\n",
      "\n",
      "Epoch 88/299\n",
      "----------\n",
      "train Loss: -53544.7917 Acc: 0.3484\n",
      "val Loss: -54039.5255 Acc: 0.3454\n",
      "\n",
      "Epoch 89/299\n",
      "----------\n",
      "train Loss: -54133.0539 Acc: 0.3483\n",
      "val Loss: -54646.8829 Acc: 0.3454\n",
      "\n",
      "Epoch 90/299\n",
      "----------\n",
      "train Loss: -54743.9126 Acc: 0.3481\n",
      "val Loss: -55254.1337 Acc: 0.3454\n",
      "\n",
      "Epoch 91/299\n",
      "----------\n",
      "train Loss: -55352.1964 Acc: 0.3486\n",
      "val Loss: -55861.6601 Acc: 0.3454\n",
      "\n",
      "Epoch 92/299\n",
      "----------\n",
      "train Loss: -55963.1000 Acc: 0.3480\n",
      "val Loss: -56469.0651 Acc: 0.3458\n",
      "\n",
      "Epoch 93/299\n",
      "----------\n",
      "train Loss: -56565.4313 Acc: 0.3482\n",
      "val Loss: -57076.6871 Acc: 0.3454\n",
      "\n",
      "Epoch 94/299\n",
      "----------\n",
      "train Loss: -57135.7501 Acc: 0.3485\n",
      "val Loss: -57683.6888 Acc: 0.3454\n",
      "\n",
      "Epoch 95/299\n",
      "----------\n",
      "train Loss: -57769.9591 Acc: 0.3483\n",
      "val Loss: -58291.3431 Acc: 0.3454\n",
      "\n",
      "Epoch 96/299\n",
      "----------\n",
      "train Loss: -58390.9152 Acc: 0.3491\n",
      "val Loss: -58898.7262 Acc: 0.3454\n",
      "\n",
      "Epoch 97/299\n",
      "----------\n",
      "train Loss: -58973.3863 Acc: 0.3483\n",
      "val Loss: -59506.1113 Acc: 0.3454\n",
      "\n",
      "Epoch 98/299\n",
      "----------\n",
      "train Loss: -59601.4448 Acc: 0.3491\n",
      "val Loss: -60113.6639 Acc: 0.3454\n",
      "\n",
      "Epoch 99/299\n",
      "----------\n",
      "train Loss: -60174.6806 Acc: 0.3494\n",
      "val Loss: -60720.9301 Acc: 0.3454\n",
      "\n",
      "Epoch 100/299\n",
      "----------\n",
      "train Loss: -60768.2709 Acc: 0.3490\n",
      "val Loss: -61328.0477 Acc: 0.3454\n",
      "\n",
      "Epoch 101/299\n",
      "----------\n",
      "train Loss: -61397.9179 Acc: 0.3496\n",
      "val Loss: -61935.3050 Acc: 0.3454\n",
      "\n",
      "Epoch 102/299\n",
      "----------\n",
      "train Loss: -62009.6339 Acc: 0.3490\n",
      "val Loss: -62543.0273 Acc: 0.3454\n",
      "\n",
      "Epoch 103/299\n",
      "----------\n",
      "train Loss: -62585.4840 Acc: 0.3483\n",
      "val Loss: -63149.9841 Acc: 0.3454\n",
      "\n",
      "Epoch 104/299\n",
      "----------\n",
      "train Loss: -63199.3607 Acc: 0.3491\n",
      "val Loss: -63757.2487 Acc: 0.3454\n",
      "\n",
      "Epoch 105/299\n",
      "----------\n",
      "train Loss: -63801.5116 Acc: 0.3478\n",
      "val Loss: -64364.5044 Acc: 0.3458\n",
      "\n",
      "Epoch 106/299\n",
      "----------\n",
      "train Loss: -64434.3585 Acc: 0.3485\n",
      "val Loss: -64972.0573 Acc: 0.3454\n",
      "\n",
      "Epoch 107/299\n",
      "----------\n",
      "train Loss: -65018.0008 Acc: 0.3482\n",
      "val Loss: -65579.2688 Acc: 0.3454\n",
      "\n",
      "Epoch 108/299\n",
      "----------\n",
      "train Loss: -65620.1789 Acc: 0.3489\n",
      "val Loss: -66186.5334 Acc: 0.3454\n",
      "\n",
      "Epoch 109/299\n",
      "----------\n",
      "train Loss: -66221.6433 Acc: 0.3475\n",
      "val Loss: -66794.1214 Acc: 0.3454\n",
      "\n",
      "Epoch 110/299\n",
      "----------\n",
      "train Loss: -66855.4778 Acc: 0.3491\n",
      "val Loss: -67401.1872 Acc: 0.3458\n",
      "\n",
      "Epoch 111/299\n",
      "----------\n",
      "train Loss: -67445.6583 Acc: 0.3479\n",
      "val Loss: -68008.5671 Acc: 0.3458\n",
      "\n",
      "Epoch 112/299\n",
      "----------\n",
      "train Loss: -68063.3494 Acc: 0.3483\n",
      "val Loss: -68616.0878 Acc: 0.3454\n",
      "\n",
      "Epoch 113/299\n",
      "----------\n",
      "train Loss: -68671.8487 Acc: 0.3475\n",
      "val Loss: -69223.5411 Acc: 0.3458\n",
      "\n",
      "Epoch 114/299\n",
      "----------\n",
      "train Loss: -69278.2179 Acc: 0.3485\n",
      "val Loss: -69831.2140 Acc: 0.3454\n",
      "\n",
      "Epoch 115/299\n",
      "----------\n",
      "train Loss: -69895.4227 Acc: 0.3483\n",
      "val Loss: -70438.7165 Acc: 0.3458\n",
      "\n",
      "Epoch 116/299\n",
      "----------\n",
      "train Loss: -70491.6629 Acc: 0.3494\n",
      "val Loss: -71046.4024 Acc: 0.3454\n",
      "\n",
      "Epoch 117/299\n",
      "----------\n",
      "train Loss: -71080.5187 Acc: 0.3493\n",
      "val Loss: -71653.6260 Acc: 0.3454\n",
      "\n",
      "Epoch 118/299\n",
      "----------\n",
      "train Loss: -71667.6924 Acc: 0.3479\n",
      "val Loss: -72260.8473 Acc: 0.3458\n",
      "\n",
      "Epoch 119/299\n",
      "----------\n",
      "train Loss: -72275.1762 Acc: 0.3477\n",
      "val Loss: -72868.1079 Acc: 0.3454\n",
      "\n",
      "Epoch 120/299\n",
      "----------\n",
      "train Loss: -72889.4701 Acc: 0.3498\n",
      "val Loss: -73475.5825 Acc: 0.3454\n",
      "\n",
      "Epoch 121/299\n",
      "----------\n",
      "train Loss: -73476.2792 Acc: 0.3483\n",
      "val Loss: -74082.7905 Acc: 0.3454\n",
      "\n",
      "Epoch 122/299\n",
      "----------\n",
      "train Loss: -74089.4314 Acc: 0.3490\n",
      "val Loss: -74689.9000 Acc: 0.3454\n",
      "\n",
      "Epoch 123/299\n",
      "----------\n",
      "train Loss: -74715.9708 Acc: 0.3481\n",
      "val Loss: -75297.4186 Acc: 0.3454\n",
      "\n",
      "Epoch 124/299\n",
      "----------\n",
      "train Loss: -75283.2880 Acc: 0.3471\n",
      "val Loss: -75904.5262 Acc: 0.3454\n",
      "\n",
      "Epoch 125/299\n",
      "----------\n",
      "train Loss: -75925.9430 Acc: 0.3488\n",
      "val Loss: -76511.9706 Acc: 0.3458\n",
      "\n",
      "Epoch 126/299\n",
      "----------\n",
      "train Loss: -76537.7968 Acc: 0.3477\n",
      "val Loss: -77119.4815 Acc: 0.3454\n",
      "\n",
      "Epoch 127/299\n",
      "----------\n",
      "train Loss: -77147.0856 Acc: 0.3481\n",
      "val Loss: -77726.9750 Acc: 0.3458\n",
      "\n",
      "Epoch 128/299\n",
      "----------\n",
      "train Loss: -77750.3565 Acc: 0.3481\n",
      "val Loss: -78334.7228 Acc: 0.3454\n",
      "\n",
      "Epoch 129/299\n",
      "----------\n",
      "train Loss: -78339.7932 Acc: 0.3479\n",
      "val Loss: -78941.8385 Acc: 0.3454\n",
      "\n",
      "Epoch 130/299\n",
      "----------\n",
      "train Loss: -78953.3767 Acc: 0.3486\n",
      "val Loss: -79549.3768 Acc: 0.3454\n",
      "\n",
      "Epoch 131/299\n",
      "----------\n",
      "train Loss: -79552.9758 Acc: 0.3478\n",
      "val Loss: -80156.8148 Acc: 0.3454\n",
      "\n",
      "Epoch 132/299\n",
      "----------\n",
      "train Loss: -80126.9089 Acc: 0.3486\n",
      "val Loss: -80763.9574 Acc: 0.3458\n",
      "\n",
      "Epoch 133/299\n",
      "----------\n",
      "train Loss: -80767.5223 Acc: 0.3494\n",
      "val Loss: -81371.5992 Acc: 0.3454\n",
      "\n",
      "Epoch 134/299\n",
      "----------\n",
      "train Loss: -81325.6976 Acc: 0.3478\n",
      "val Loss: -81978.6117 Acc: 0.3458\n",
      "\n",
      "Epoch 135/299\n",
      "----------\n",
      "train Loss: -81994.1191 Acc: 0.3482\n",
      "val Loss: -82585.9845 Acc: 0.3454\n",
      "\n",
      "Epoch 136/299\n",
      "----------\n",
      "train Loss: -82572.1681 Acc: 0.3487\n",
      "val Loss: -83193.3131 Acc: 0.3458\n",
      "\n",
      "Epoch 137/299\n",
      "----------\n",
      "train Loss: -83168.0384 Acc: 0.3481\n",
      "val Loss: -83800.7337 Acc: 0.3454\n",
      "\n",
      "Epoch 138/299\n",
      "----------\n",
      "train Loss: -83749.4363 Acc: 0.3478\n",
      "val Loss: -84407.8516 Acc: 0.3458\n",
      "\n",
      "Epoch 139/299\n",
      "----------\n",
      "train Loss: -84375.9766 Acc: 0.3490\n",
      "val Loss: -85015.0481 Acc: 0.3458\n",
      "\n",
      "Epoch 140/299\n",
      "----------\n",
      "train Loss: -84996.3382 Acc: 0.3483\n",
      "val Loss: -85622.4654 Acc: 0.3454\n",
      "\n",
      "Epoch 141/299\n",
      "----------\n",
      "train Loss: -85591.3620 Acc: 0.3482\n",
      "val Loss: -86229.8570 Acc: 0.3454\n",
      "\n",
      "Epoch 142/299\n",
      "----------\n",
      "train Loss: -86189.5233 Acc: 0.3484\n",
      "val Loss: -86837.0621 Acc: 0.3458\n",
      "\n",
      "Epoch 143/299\n",
      "----------\n",
      "train Loss: -86804.2737 Acc: 0.3496\n",
      "val Loss: -87444.2615 Acc: 0.3454\n",
      "\n",
      "Epoch 144/299\n",
      "----------\n",
      "train Loss: -87459.9813 Acc: 0.3496\n",
      "val Loss: -88051.9987 Acc: 0.3454\n",
      "\n",
      "Epoch 145/299\n",
      "----------\n",
      "train Loss: -88009.0894 Acc: 0.3480\n",
      "val Loss: -88659.3247 Acc: 0.3454\n",
      "\n",
      "Epoch 146/299\n",
      "----------\n",
      "train Loss: -88614.7527 Acc: 0.3477\n",
      "val Loss: -89266.6896 Acc: 0.3454\n",
      "\n",
      "Epoch 147/299\n",
      "----------\n",
      "train Loss: -89257.9730 Acc: 0.3487\n",
      "val Loss: -89874.2878 Acc: 0.3454\n",
      "\n",
      "Epoch 148/299\n",
      "----------\n",
      "train Loss: -89850.4978 Acc: 0.3483\n",
      "val Loss: -90481.6883 Acc: 0.3454\n",
      "\n",
      "Epoch 149/299\n",
      "----------\n",
      "train Loss: -90443.6185 Acc: 0.3487\n",
      "val Loss: -91089.1688 Acc: 0.3454\n",
      "\n",
      "Epoch 150/299\n",
      "----------\n",
      "train Loss: -91059.9125 Acc: 0.3487\n",
      "val Loss: -91696.7138 Acc: 0.3454\n",
      "\n",
      "Epoch 151/299\n",
      "----------\n",
      "train Loss: -91655.3354 Acc: 0.3469\n",
      "val Loss: -92304.2058 Acc: 0.3454\n",
      "\n",
      "Epoch 152/299\n",
      "----------\n",
      "train Loss: -92235.5321 Acc: 0.3477\n",
      "val Loss: -92911.3473 Acc: 0.3454\n",
      "\n",
      "Epoch 153/299\n",
      "----------\n",
      "train Loss: -92863.5257 Acc: 0.3494\n",
      "val Loss: -93518.6061 Acc: 0.3458\n",
      "\n",
      "Epoch 154/299\n",
      "----------\n",
      "train Loss: -93427.9574 Acc: 0.3479\n",
      "val Loss: -94125.7939 Acc: 0.3458\n",
      "\n",
      "Epoch 155/299\n",
      "----------\n",
      "train Loss: -94096.4142 Acc: 0.3480\n",
      "val Loss: -94733.2355 Acc: 0.3454\n",
      "\n",
      "Epoch 156/299\n",
      "----------\n",
      "train Loss: -94707.6760 Acc: 0.3485\n",
      "val Loss: -95340.9325 Acc: 0.3454\n",
      "\n",
      "Epoch 157/299\n",
      "----------\n",
      "train Loss: -95260.8200 Acc: 0.3484\n",
      "val Loss: -95948.2010 Acc: 0.3454\n",
      "\n",
      "Epoch 158/299\n",
      "----------\n",
      "train Loss: -95915.4678 Acc: 0.3484\n",
      "val Loss: -96555.8842 Acc: 0.3454\n",
      "\n",
      "Epoch 159/299\n",
      "----------\n",
      "train Loss: -96511.3277 Acc: 0.3484\n",
      "val Loss: -97163.4076 Acc: 0.3454\n",
      "\n",
      "Epoch 160/299\n",
      "----------\n",
      "train Loss: -97124.1278 Acc: 0.3477\n",
      "val Loss: -97770.8647 Acc: 0.3454\n",
      "\n",
      "Epoch 161/299\n",
      "----------\n",
      "train Loss: -97724.6708 Acc: 0.3484\n",
      "val Loss: -98378.3049 Acc: 0.3454\n",
      "\n",
      "Epoch 162/299\n",
      "----------\n",
      "train Loss: -98301.9282 Acc: 0.3483\n",
      "val Loss: -98985.6791 Acc: 0.3454\n",
      "\n",
      "Epoch 163/299\n",
      "----------\n",
      "train Loss: -98882.3254 Acc: 0.3488\n",
      "val Loss: -99592.9119 Acc: 0.3454\n",
      "\n",
      "Epoch 164/299\n",
      "----------\n",
      "train Loss: -99568.2237 Acc: 0.3478\n",
      "val Loss: -100200.5267 Acc: 0.3458\n",
      "\n",
      "Epoch 165/299\n",
      "----------\n",
      "train Loss: -100130.4218 Acc: 0.3489\n",
      "val Loss: -100807.8837 Acc: 0.3454\n",
      "\n",
      "Epoch 166/299\n",
      "----------\n",
      "train Loss: -100729.5980 Acc: 0.3487\n",
      "val Loss: -101415.4909 Acc: 0.3454\n",
      "\n",
      "Epoch 167/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: -101351.6643 Acc: 0.3488\n",
      "val Loss: -102022.8661 Acc: 0.3458\n",
      "\n",
      "Epoch 168/299\n",
      "----------\n",
      "train Loss: -101930.7978 Acc: 0.3483\n",
      "val Loss: -102630.2473 Acc: 0.3454\n",
      "\n",
      "Epoch 169/299\n",
      "----------\n",
      "train Loss: -102566.1981 Acc: 0.3481\n",
      "val Loss: -103237.7677 Acc: 0.3454\n",
      "\n",
      "Epoch 170/299\n",
      "----------\n",
      "train Loss: -103204.5990 Acc: 0.3481\n",
      "val Loss: -103845.4874 Acc: 0.3454\n",
      "\n",
      "Epoch 171/299\n",
      "----------\n",
      "train Loss: -103731.9029 Acc: 0.3473\n",
      "val Loss: -104452.8297 Acc: 0.3454\n",
      "\n",
      "Epoch 172/299\n",
      "----------\n",
      "train Loss: -104353.8158 Acc: 0.3490\n",
      "val Loss: -105060.1536 Acc: 0.3454\n",
      "\n",
      "Epoch 173/299\n",
      "----------\n",
      "train Loss: -104991.0625 Acc: 0.3479\n",
      "val Loss: -105667.5914 Acc: 0.3454\n",
      "\n",
      "Epoch 174/299\n",
      "----------\n",
      "train Loss: -105558.8245 Acc: 0.3482\n",
      "val Loss: -106274.8445 Acc: 0.3454\n",
      "\n",
      "Epoch 175/299\n",
      "----------\n",
      "train Loss: -106179.7766 Acc: 0.3484\n",
      "val Loss: -106882.4656 Acc: 0.3454\n",
      "\n",
      "Epoch 176/299\n",
      "----------\n",
      "train Loss: -106803.7425 Acc: 0.3486\n",
      "val Loss: -107489.7966 Acc: 0.3454\n",
      "\n",
      "Epoch 177/299\n",
      "----------\n",
      "train Loss: -107341.2126 Acc: 0.3488\n",
      "val Loss: -108097.1131 Acc: 0.3454\n",
      "\n",
      "Epoch 178/299\n",
      "----------\n",
      "train Loss: -108005.5730 Acc: 0.3475\n",
      "val Loss: -108704.4662 Acc: 0.3454\n",
      "\n",
      "Epoch 179/299\n",
      "----------\n",
      "train Loss: -108568.1148 Acc: 0.3482\n",
      "val Loss: -109311.7568 Acc: 0.3454\n",
      "\n",
      "Epoch 180/299\n",
      "----------\n",
      "train Loss: -109255.3948 Acc: 0.3471\n",
      "val Loss: -109919.2541 Acc: 0.3454\n",
      "\n",
      "Epoch 181/299\n",
      "----------\n",
      "train Loss: -109772.1930 Acc: 0.3475\n",
      "val Loss: -110526.7590 Acc: 0.3454\n",
      "\n",
      "Epoch 182/299\n",
      "----------\n",
      "train Loss: -110422.7029 Acc: 0.3478\n",
      "val Loss: -111134.0988 Acc: 0.3454\n",
      "\n",
      "Epoch 183/299\n",
      "----------\n",
      "train Loss: -110984.1487 Acc: 0.3477\n",
      "val Loss: -111741.2321 Acc: 0.3454\n",
      "\n",
      "Epoch 184/299\n",
      "----------\n",
      "train Loss: -111615.6878 Acc: 0.3475\n",
      "val Loss: -112348.6358 Acc: 0.3454\n",
      "\n",
      "Epoch 185/299\n",
      "----------\n",
      "train Loss: -112173.4392 Acc: 0.3488\n",
      "val Loss: -112955.7507 Acc: 0.3454\n",
      "\n",
      "Epoch 186/299\n",
      "----------\n",
      "train Loss: -112880.9381 Acc: 0.3491\n",
      "val Loss: -113563.3071 Acc: 0.3454\n",
      "\n",
      "Epoch 187/299\n",
      "----------\n",
      "train Loss: -113434.6980 Acc: 0.3490\n",
      "val Loss: -114170.7688 Acc: 0.3454\n",
      "\n",
      "Epoch 188/299\n",
      "----------\n",
      "train Loss: -114034.7272 Acc: 0.3480\n",
      "val Loss: -114778.1501 Acc: 0.3454\n",
      "\n",
      "Epoch 189/299\n",
      "----------\n",
      "train Loss: -114647.6971 Acc: 0.3489\n",
      "val Loss: -115385.4933 Acc: 0.3454\n",
      "\n",
      "Epoch 190/299\n",
      "----------\n",
      "train Loss: -115237.9748 Acc: 0.3494\n",
      "val Loss: -115992.8105 Acc: 0.3454\n",
      "\n",
      "Epoch 191/299\n",
      "----------\n",
      "train Loss: -115874.5137 Acc: 0.3484\n",
      "val Loss: -116600.2730 Acc: 0.3454\n",
      "\n",
      "Epoch 192/299\n",
      "----------\n",
      "train Loss: -116434.6656 Acc: 0.3487\n",
      "val Loss: -117207.6846 Acc: 0.3454\n",
      "\n",
      "Epoch 193/299\n",
      "----------\n",
      "train Loss: -117106.0966 Acc: 0.3484\n",
      "val Loss: -117815.3165 Acc: 0.3454\n",
      "\n",
      "Epoch 194/299\n",
      "----------\n",
      "train Loss: -117655.1659 Acc: 0.3488\n",
      "val Loss: -118422.5239 Acc: 0.3454\n",
      "\n",
      "Epoch 195/299\n",
      "----------\n",
      "train Loss: -118266.8269 Acc: 0.3488\n",
      "val Loss: -119029.8027 Acc: 0.3454\n",
      "\n",
      "Epoch 196/299\n",
      "----------\n",
      "train Loss: -118849.4860 Acc: 0.3471\n",
      "val Loss: -119636.9522 Acc: 0.3454\n",
      "\n",
      "Epoch 197/299\n",
      "----------\n",
      "train Loss: -119465.5050 Acc: 0.3487\n",
      "val Loss: -120244.4528 Acc: 0.3454\n",
      "\n",
      "Epoch 198/299\n",
      "----------\n",
      "train Loss: -120104.6350 Acc: 0.3480\n",
      "val Loss: -120851.7235 Acc: 0.3454\n",
      "\n",
      "Epoch 199/299\n",
      "----------\n",
      "train Loss: -120688.6243 Acc: 0.3476\n",
      "val Loss: -121459.1389 Acc: 0.3454\n",
      "\n",
      "Epoch 200/299\n",
      "----------\n",
      "train Loss: -121323.3824 Acc: 0.3499\n",
      "val Loss: -122066.5393 Acc: 0.3454\n",
      "\n",
      "Epoch 201/299\n",
      "----------\n",
      "train Loss: -121941.7230 Acc: 0.3500\n",
      "val Loss: -122674.2713 Acc: 0.3454\n",
      "\n",
      "Epoch 202/299\n",
      "----------\n",
      "train Loss: -122459.3656 Acc: 0.3486\n",
      "val Loss: -123281.3380 Acc: 0.3454\n",
      "\n",
      "Epoch 203/299\n",
      "----------\n",
      "train Loss: -123146.3095 Acc: 0.3486\n",
      "val Loss: -123889.0196 Acc: 0.3454\n",
      "\n",
      "Epoch 204/299\n",
      "----------\n",
      "train Loss: -123711.2499 Acc: 0.3483\n",
      "val Loss: -124496.2245 Acc: 0.3454\n",
      "\n",
      "Epoch 205/299\n",
      "----------\n",
      "train Loss: -124319.2921 Acc: 0.3476\n",
      "val Loss: -125103.4881 Acc: 0.3454\n",
      "\n",
      "Epoch 206/299\n",
      "----------\n",
      "train Loss: -124940.0857 Acc: 0.3481\n",
      "val Loss: -125710.9758 Acc: 0.3454\n",
      "\n",
      "Epoch 207/299\n",
      "----------\n",
      "train Loss: -125576.9391 Acc: 0.3495\n",
      "val Loss: -126318.3479 Acc: 0.3454\n",
      "\n",
      "Epoch 208/299\n",
      "----------\n",
      "train Loss: -126166.4628 Acc: 0.3480\n",
      "val Loss: -126925.9409 Acc: 0.3454\n",
      "\n",
      "Epoch 209/299\n",
      "----------\n",
      "train Loss: -126734.2998 Acc: 0.3488\n",
      "val Loss: -127533.4444 Acc: 0.3454\n",
      "\n",
      "Epoch 210/299\n",
      "----------\n",
      "train Loss: -127331.4140 Acc: 0.3489\n",
      "val Loss: -128140.6392 Acc: 0.3454\n",
      "\n",
      "Epoch 211/299\n",
      "----------\n",
      "train Loss: -127933.4744 Acc: 0.3483\n",
      "val Loss: -128747.9522 Acc: 0.3454\n",
      "\n",
      "Epoch 212/299\n",
      "----------\n",
      "train Loss: -128638.4539 Acc: 0.3484\n",
      "val Loss: -129355.7801 Acc: 0.3454\n",
      "\n",
      "Epoch 213/299\n",
      "----------\n",
      "train Loss: -129180.1560 Acc: 0.3497\n",
      "val Loss: -129962.8877 Acc: 0.3454\n",
      "\n",
      "Epoch 214/299\n",
      "----------\n",
      "train Loss: -129769.6975 Acc: 0.3484\n",
      "val Loss: -130570.5426 Acc: 0.3454\n",
      "\n",
      "Epoch 215/299\n",
      "----------\n",
      "train Loss: -130354.9980 Acc: 0.3476\n",
      "val Loss: -131177.6298 Acc: 0.3454\n",
      "\n",
      "Epoch 216/299\n",
      "----------\n",
      "train Loss: -130940.9801 Acc: 0.3483\n",
      "val Loss: -131784.8337 Acc: 0.3454\n",
      "\n",
      "Epoch 217/299\n",
      "----------\n",
      "train Loss: -131611.0093 Acc: 0.3485\n",
      "val Loss: -132392.3926 Acc: 0.3454\n",
      "\n",
      "Epoch 218/299\n",
      "----------\n",
      "train Loss: -132190.0125 Acc: 0.3489\n",
      "val Loss: -132999.7867 Acc: 0.3454\n",
      "\n",
      "Epoch 219/299\n",
      "----------\n",
      "train Loss: -132848.2340 Acc: 0.3481\n",
      "val Loss: -133607.5043 Acc: 0.3454\n",
      "\n",
      "Epoch 220/299\n",
      "----------\n",
      "train Loss: -133345.4123 Acc: 0.3494\n",
      "val Loss: -134214.4487 Acc: 0.3454\n",
      "\n",
      "Epoch 221/299\n",
      "----------\n",
      "train Loss: -133985.1675 Acc: 0.3484\n",
      "val Loss: -134821.6910 Acc: 0.3454\n",
      "\n",
      "Epoch 222/299\n",
      "----------\n",
      "train Loss: -134605.0299 Acc: 0.3484\n",
      "val Loss: -135429.2035 Acc: 0.3454\n",
      "\n",
      "Epoch 223/299\n",
      "----------\n",
      "train Loss: -135238.9357 Acc: 0.3480\n",
      "val Loss: -136036.5421 Acc: 0.3454\n",
      "\n",
      "Epoch 224/299\n",
      "----------\n",
      "train Loss: -135792.0167 Acc: 0.3481\n",
      "val Loss: -136643.7715 Acc: 0.3454\n",
      "\n",
      "Epoch 225/299\n",
      "----------\n",
      "train Loss: -136378.1130 Acc: 0.3489\n",
      "val Loss: -137251.0695 Acc: 0.3454\n",
      "\n",
      "Epoch 226/299\n",
      "----------\n",
      "train Loss: -137030.9590 Acc: 0.3479\n",
      "val Loss: -137858.4110 Acc: 0.3454\n",
      "\n",
      "Epoch 227/299\n",
      "----------\n",
      "train Loss: -137663.5976 Acc: 0.3485\n",
      "val Loss: -138465.7377 Acc: 0.3454\n",
      "\n",
      "Epoch 228/299\n",
      "----------\n",
      "train Loss: -138261.6246 Acc: 0.3479\n",
      "val Loss: -139073.3937 Acc: 0.3454\n",
      "\n",
      "Epoch 229/299\n",
      "----------\n",
      "train Loss: -138837.9639 Acc: 0.3486\n",
      "val Loss: -139680.7179 Acc: 0.3454\n",
      "\n",
      "Epoch 230/299\n",
      "----------\n",
      "train Loss: -139436.1021 Acc: 0.3484\n",
      "val Loss: -140288.0221 Acc: 0.3454\n",
      "\n",
      "Epoch 231/299\n",
      "----------\n",
      "train Loss: -140069.9715 Acc: 0.3490\n",
      "val Loss: -140895.3039 Acc: 0.3454\n",
      "\n",
      "Epoch 232/299\n",
      "----------\n",
      "train Loss: -140654.6541 Acc: 0.3488\n",
      "val Loss: -141502.8323 Acc: 0.3454\n",
      "\n",
      "Epoch 233/299\n",
      "----------\n",
      "train Loss: -141230.5447 Acc: 0.3485\n",
      "val Loss: -142109.8974 Acc: 0.3454\n",
      "\n",
      "Epoch 234/299\n",
      "----------\n",
      "train Loss: -141877.5425 Acc: 0.3479\n",
      "val Loss: -142717.3042 Acc: 0.3454\n",
      "\n",
      "Epoch 235/299\n",
      "----------\n",
      "train Loss: -142494.5205 Acc: 0.3478\n",
      "val Loss: -143324.7524 Acc: 0.3454\n",
      "\n",
      "Epoch 236/299\n",
      "----------\n",
      "train Loss: -143016.2858 Acc: 0.3489\n",
      "val Loss: -143931.9069 Acc: 0.3454\n",
      "\n",
      "Epoch 237/299\n",
      "----------\n",
      "train Loss: -143690.3694 Acc: 0.3487\n",
      "val Loss: -144539.1370 Acc: 0.3454\n",
      "\n",
      "Epoch 238/299\n",
      "----------\n",
      "train Loss: -144266.9803 Acc: 0.3484\n",
      "val Loss: -145146.5605 Acc: 0.3454\n",
      "\n",
      "Epoch 239/299\n",
      "----------\n",
      "train Loss: -144950.2382 Acc: 0.3477\n",
      "val Loss: -145754.1574 Acc: 0.3454\n",
      "\n",
      "Epoch 240/299\n",
      "----------\n",
      "train Loss: -145538.6489 Acc: 0.3488\n",
      "val Loss: -146361.6513 Acc: 0.3454\n",
      "\n",
      "Epoch 241/299\n",
      "----------\n",
      "train Loss: -146066.4578 Acc: 0.3479\n",
      "val Loss: -146968.8827 Acc: 0.3454\n",
      "\n",
      "Epoch 242/299\n",
      "----------\n",
      "train Loss: -146697.1932 Acc: 0.3476\n",
      "val Loss: -147576.0478 Acc: 0.3454\n",
      "\n",
      "Epoch 243/299\n",
      "----------\n",
      "train Loss: -147313.5555 Acc: 0.3474\n",
      "val Loss: -148183.4762 Acc: 0.3454\n",
      "\n",
      "Epoch 244/299\n",
      "----------\n",
      "train Loss: -147977.1033 Acc: 0.3486\n",
      "val Loss: -148791.2846 Acc: 0.3454\n",
      "\n",
      "Epoch 245/299\n",
      "----------\n",
      "train Loss: -148534.8393 Acc: 0.3481\n",
      "val Loss: -149398.5762 Acc: 0.3454\n",
      "\n",
      "Epoch 246/299\n",
      "----------\n",
      "train Loss: -149196.9961 Acc: 0.3485\n",
      "val Loss: -150006.2352 Acc: 0.3454\n",
      "\n",
      "Epoch 247/299\n",
      "----------\n",
      "train Loss: -149665.7929 Acc: 0.3489\n",
      "val Loss: -150613.3265 Acc: 0.3454\n",
      "\n",
      "Epoch 248/299\n",
      "----------\n",
      "train Loss: -150423.8213 Acc: 0.3477\n",
      "val Loss: -151220.9508 Acc: 0.3454\n",
      "\n",
      "Epoch 249/299\n",
      "----------\n",
      "train Loss: -150956.7839 Acc: 0.3485\n",
      "val Loss: -151828.4339 Acc: 0.3454\n",
      "\n",
      "Epoch 250/299\n",
      "----------\n",
      "train Loss: -151597.3136 Acc: 0.3480\n",
      "val Loss: -152435.8955 Acc: 0.3454\n",
      "\n",
      "Epoch 251/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: -152176.9329 Acc: 0.3490\n",
      "val Loss: -153043.5440 Acc: 0.3454\n",
      "\n",
      "Epoch 252/299\n",
      "----------\n",
      "train Loss: -152755.8744 Acc: 0.3482\n",
      "val Loss: -153650.6849 Acc: 0.3454\n",
      "\n",
      "Epoch 253/299\n",
      "----------\n",
      "train Loss: -153365.2471 Acc: 0.3485\n",
      "val Loss: -154258.0633 Acc: 0.3454\n",
      "\n",
      "Epoch 254/299\n",
      "----------\n",
      "train Loss: -153983.6493 Acc: 0.3485\n",
      "val Loss: -154865.6511 Acc: 0.3454\n",
      "\n",
      "Epoch 255/299\n",
      "----------\n",
      "train Loss: -154597.8646 Acc: 0.3494\n",
      "val Loss: -155473.0241 Acc: 0.3454\n",
      "\n",
      "Epoch 256/299\n",
      "----------\n",
      "train Loss: -155173.6565 Acc: 0.3484\n",
      "val Loss: -156080.3608 Acc: 0.3454\n",
      "\n",
      "Epoch 257/299\n",
      "----------\n",
      "train Loss: -155798.7330 Acc: 0.3487\n",
      "val Loss: -156687.7994 Acc: 0.3454\n",
      "\n",
      "Epoch 258/299\n",
      "----------\n",
      "train Loss: -156357.3339 Acc: 0.3478\n",
      "val Loss: -157294.9161 Acc: 0.3458\n",
      "\n",
      "Epoch 259/299\n",
      "----------\n",
      "train Loss: -157015.9756 Acc: 0.3481\n",
      "val Loss: -157902.4971 Acc: 0.3454\n",
      "\n",
      "Epoch 260/299\n",
      "----------\n",
      "train Loss: -157668.0744 Acc: 0.3487\n",
      "val Loss: -158510.1198 Acc: 0.3454\n",
      "\n",
      "Epoch 261/299\n",
      "----------\n",
      "train Loss: -158213.6229 Acc: 0.3480\n",
      "val Loss: -159117.3782 Acc: 0.3454\n",
      "\n",
      "Epoch 262/299\n",
      "----------\n",
      "train Loss: -158862.4479 Acc: 0.3484\n",
      "val Loss: -159724.8472 Acc: 0.3454\n",
      "\n",
      "Epoch 263/299\n",
      "----------\n",
      "train Loss: -159393.9959 Acc: 0.3487\n",
      "val Loss: -160332.2865 Acc: 0.3454\n",
      "\n",
      "Epoch 264/299\n",
      "----------\n",
      "train Loss: -160051.1916 Acc: 0.3486\n",
      "val Loss: -160939.8236 Acc: 0.3454\n",
      "\n",
      "Epoch 265/299\n",
      "----------\n",
      "train Loss: -160630.8243 Acc: 0.3482\n",
      "val Loss: -161547.1227 Acc: 0.3454\n",
      "\n",
      "Epoch 266/299\n",
      "----------\n",
      "train Loss: -161233.8749 Acc: 0.3477\n",
      "val Loss: -162154.5852 Acc: 0.3454\n",
      "\n",
      "Epoch 267/299\n",
      "----------\n",
      "train Loss: -161795.0006 Acc: 0.3477\n",
      "val Loss: -162761.7314 Acc: 0.3454\n",
      "\n",
      "Epoch 268/299\n",
      "----------\n",
      "train Loss: -162470.4039 Acc: 0.3486\n",
      "val Loss: -163369.2208 Acc: 0.3454\n",
      "\n",
      "Epoch 269/299\n",
      "----------\n",
      "train Loss: -163102.4138 Acc: 0.3481\n",
      "val Loss: -163976.8847 Acc: 0.3454\n",
      "\n",
      "Epoch 270/299\n",
      "----------\n",
      "train Loss: -163702.5324 Acc: 0.3488\n",
      "val Loss: -164584.2898 Acc: 0.3454\n",
      "\n",
      "Epoch 271/299\n",
      "----------\n",
      "train Loss: -164220.9589 Acc: 0.3479\n",
      "val Loss: -165191.4485 Acc: 0.3454\n",
      "\n",
      "Epoch 272/299\n",
      "----------\n",
      "train Loss: -164857.4704 Acc: 0.3476\n",
      "val Loss: -165798.7831 Acc: 0.3454\n",
      "\n",
      "Epoch 273/299\n",
      "----------\n",
      "train Loss: -165470.7306 Acc: 0.3491\n",
      "val Loss: -166406.1306 Acc: 0.3454\n",
      "\n",
      "Epoch 274/299\n",
      "----------\n",
      "train Loss: -166045.0091 Acc: 0.3485\n",
      "val Loss: -167013.6583 Acc: 0.3454\n",
      "\n",
      "Epoch 275/299\n",
      "----------\n",
      "train Loss: -166732.4780 Acc: 0.3489\n",
      "val Loss: -167621.0735 Acc: 0.3454\n",
      "\n",
      "Epoch 276/299\n",
      "----------\n",
      "train Loss: -167332.0001 Acc: 0.3474\n",
      "val Loss: -168228.5480 Acc: 0.3454\n",
      "\n",
      "Epoch 277/299\n",
      "----------\n",
      "train Loss: -167883.5837 Acc: 0.3484\n",
      "val Loss: -168835.9891 Acc: 0.3454\n",
      "\n",
      "Epoch 278/299\n",
      "----------\n",
      "train Loss: -168525.5494 Acc: 0.3487\n",
      "val Loss: -169443.3378 Acc: 0.3454\n",
      "\n",
      "Epoch 279/299\n",
      "----------\n",
      "train Loss: -169148.0300 Acc: 0.3484\n",
      "val Loss: -170050.8290 Acc: 0.3454\n",
      "\n",
      "Epoch 280/299\n",
      "----------\n",
      "train Loss: -169710.3841 Acc: 0.3478\n",
      "val Loss: -170658.4241 Acc: 0.3454\n",
      "\n",
      "Epoch 281/299\n",
      "----------\n",
      "train Loss: -170300.6556 Acc: 0.3483\n",
      "val Loss: -171265.8110 Acc: 0.3454\n",
      "\n",
      "Epoch 282/299\n",
      "----------\n",
      "train Loss: -170934.7804 Acc: 0.3484\n",
      "val Loss: -171873.1315 Acc: 0.3458\n",
      "\n",
      "Epoch 283/299\n",
      "----------\n",
      "train Loss: -171540.0580 Acc: 0.3481\n",
      "val Loss: -172480.5881 Acc: 0.3454\n",
      "\n",
      "Epoch 284/299\n",
      "----------\n",
      "train Loss: -172091.9944 Acc: 0.3491\n",
      "val Loss: -173087.8820 Acc: 0.3454\n",
      "\n",
      "Epoch 285/299\n",
      "----------\n",
      "train Loss: -172720.2371 Acc: 0.3485\n",
      "val Loss: -173695.1222 Acc: 0.3454\n",
      "\n",
      "Epoch 286/299\n",
      "----------\n",
      "train Loss: -173318.5535 Acc: 0.3489\n",
      "val Loss: -174302.4250 Acc: 0.3454\n",
      "\n",
      "Epoch 287/299\n",
      "----------\n",
      "train Loss: -174001.5117 Acc: 0.3477\n",
      "val Loss: -174910.1439 Acc: 0.3454\n",
      "\n",
      "Epoch 288/299\n",
      "----------\n",
      "train Loss: -174544.4303 Acc: 0.3483\n",
      "val Loss: -175517.4441 Acc: 0.3454\n",
      "\n",
      "Epoch 289/299\n",
      "----------\n",
      "train Loss: -175141.2984 Acc: 0.3476\n",
      "val Loss: -176124.7660 Acc: 0.3454\n",
      "\n",
      "Epoch 290/299\n",
      "----------\n",
      "train Loss: -175811.0886 Acc: 0.3483\n",
      "val Loss: -176732.3401 Acc: 0.3454\n",
      "\n",
      "Epoch 291/299\n",
      "----------\n",
      "train Loss: -176313.2087 Acc: 0.3503\n",
      "val Loss: -177339.6407 Acc: 0.3454\n",
      "\n",
      "Epoch 292/299\n",
      "----------\n",
      "train Loss: -176964.9360 Acc: 0.3488\n",
      "val Loss: -177947.0080 Acc: 0.3454\n",
      "\n",
      "Epoch 293/299\n",
      "----------\n",
      "train Loss: -177523.9174 Acc: 0.3481\n",
      "val Loss: -178554.1615 Acc: 0.3454\n",
      "\n",
      "Epoch 294/299\n",
      "----------\n",
      "train Loss: -178135.8124 Acc: 0.3476\n",
      "val Loss: -179161.3916 Acc: 0.3454\n",
      "\n",
      "Epoch 295/299\n",
      "----------\n",
      "train Loss: -178847.5286 Acc: 0.3481\n",
      "val Loss: -179768.8035 Acc: 0.3454\n",
      "\n",
      "Epoch 296/299\n",
      "----------\n",
      "train Loss: -179425.4682 Acc: 0.3481\n",
      "val Loss: -180376.3244 Acc: 0.3454\n",
      "\n",
      "Epoch 297/299\n",
      "----------\n",
      "train Loss: -179987.2513 Acc: 0.3479\n",
      "val Loss: -180983.8316 Acc: 0.3454\n",
      "\n",
      "Epoch 298/299\n",
      "----------\n",
      "train Loss: -180607.3543 Acc: 0.3486\n",
      "val Loss: -181591.1540 Acc: 0.3454\n",
      "\n",
      "Epoch 299/299\n",
      "----------\n",
      "train Loss: -181274.4451 Acc: 0.3486\n",
      "val Loss: -182198.8130 Acc: 0.3454\n",
      "\n",
      "\n",
      "##############################\n",
      "------ Summary ------\n",
      "model -> squeezenet\n",
      "epochs -> 300\n",
      "lr -> 0.0001\n",
      "batch size -> 8\n",
      "optimizer -> SGD\n",
      "criteriun -> NLLLoss\n",
      "\n",
      "Training complete in 152m 42s\n",
      "Best val Acc: 0.345787\n",
      "##############################\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== END ====\n",
      "[[  3 665 123]\n",
      " [  0 799   0]\n",
      " [  0 788  31]]\n",
      "\n",
      "Sensitivity or recall total\n",
      "0.3457866334578663\n",
      "\n",
      "Sensitivity or recall per classes\n",
      "[0.   1.   0.04]\n",
      "\n",
      "Precision\n",
      "[1.   0.35 0.2 ]\n",
      "\n",
      "F1 Score\n",
      "[0.01 0.52 0.06]\n",
      "Confusion matrix, without normalization\n",
      "\n",
      "============================================================\n",
      "==== INITIALIZING WITH PARAMETERS: ====\n",
      "model -> squeezenet\n",
      "epochs -> 300\n",
      "lr -> 0.0001\n",
      "batch size -> 8\n",
      "optimizer -> 2\n",
      "criteriun -> 1\n",
      "\n",
      "--------------------\n",
      "Params to learn:\n",
      "\t classifier.1.weight\n",
      "\t classifier.1.bias\n",
      "\n",
      "--------------------\n",
      "\n",
      "== Epochs ==\n",
      "Epoch 0/299\n",
      "----------\n",
      "train Loss: 0.7661 Acc: 0.7051\n",
      "val Loss: 0.4548 Acc: 0.8896\n",
      "\n",
      "Epoch 1/299\n",
      "----------\n",
      "train Loss: 0.3851 Acc: 0.8945\n",
      "val Loss: 0.2996 Acc: 0.9207\n",
      "\n",
      "Epoch 2/299\n",
      "----------\n",
      "train Loss: 0.2895 Acc: 0.9233\n",
      "val Loss: 0.2445 Acc: 0.9365\n",
      "\n",
      "Epoch 3/299\n",
      "----------\n",
      "train Loss: 0.2454 Acc: 0.9320\n",
      "val Loss: 0.2108 Acc: 0.9444\n",
      "\n",
      "Epoch 4/299\n",
      "----------\n",
      "train Loss: 0.2209 Acc: 0.9377\n",
      "val Loss: 0.1895 Acc: 0.9502\n",
      "\n",
      "Epoch 5/299\n",
      "----------\n",
      "train Loss: 0.2038 Acc: 0.9395\n",
      "val Loss: 0.1761 Acc: 0.9560\n",
      "\n",
      "Epoch 6/299\n",
      "----------\n",
      "train Loss: 0.1888 Acc: 0.9445\n",
      "val Loss: 0.1635 Acc: 0.9560\n",
      "\n",
      "Epoch 7/299\n",
      "----------\n",
      "train Loss: 0.1769 Acc: 0.9465\n",
      "val Loss: 0.1579 Acc: 0.9560\n",
      "\n",
      "Epoch 8/299\n",
      "----------\n",
      "train Loss: 0.1704 Acc: 0.9483\n",
      "val Loss: 0.1477 Acc: 0.9572\n",
      "\n",
      "Epoch 9/299\n",
      "----------\n",
      "train Loss: 0.1657 Acc: 0.9480\n",
      "val Loss: 0.1424 Acc: 0.9606\n",
      "\n",
      "Epoch 10/299\n",
      "----------\n",
      "train Loss: 0.1564 Acc: 0.9513\n",
      "val Loss: 0.1366 Acc: 0.9606\n",
      "\n",
      "Epoch 11/299\n",
      "----------\n",
      "train Loss: 0.1522 Acc: 0.9534\n",
      "val Loss: 0.1327 Acc: 0.9601\n",
      "\n",
      "Epoch 12/299\n",
      "----------\n",
      "train Loss: 0.1478 Acc: 0.9539\n",
      "val Loss: 0.1275 Acc: 0.9618\n",
      "\n",
      "Epoch 13/299\n",
      "----------\n",
      "train Loss: 0.1449 Acc: 0.9545\n",
      "val Loss: 0.1239 Acc: 0.9635\n",
      "\n",
      "Epoch 14/299\n",
      "----------\n",
      "train Loss: 0.1399 Acc: 0.9588\n",
      "val Loss: 0.1265 Acc: 0.9622\n",
      "\n",
      "Epoch 15/299\n",
      "----------\n",
      "train Loss: 0.1374 Acc: 0.9578\n",
      "val Loss: 0.1265 Acc: 0.9589\n",
      "\n",
      "Epoch 16/299\n",
      "----------\n",
      "train Loss: 0.1362 Acc: 0.9558\n",
      "val Loss: 0.1155 Acc: 0.9647\n",
      "\n",
      "Epoch 17/299\n",
      "----------\n",
      "train Loss: 0.1331 Acc: 0.9571\n",
      "val Loss: 0.1157 Acc: 0.9647\n",
      "\n",
      "Epoch 18/299\n",
      "----------\n",
      "train Loss: 0.1286 Acc: 0.9590\n",
      "val Loss: 0.1113 Acc: 0.9643\n",
      "\n",
      "Epoch 19/299\n",
      "----------\n",
      "train Loss: 0.1272 Acc: 0.9598\n",
      "val Loss: 0.1122 Acc: 0.9647\n",
      "\n",
      "Epoch 20/299\n",
      "----------\n",
      "train Loss: 0.1249 Acc: 0.9604\n",
      "val Loss: 0.1099 Acc: 0.9639\n",
      "\n",
      "Epoch 21/299\n",
      "----------\n",
      "train Loss: 0.1243 Acc: 0.9611\n",
      "val Loss: 0.1115 Acc: 0.9651\n",
      "\n",
      "Epoch 22/299\n",
      "----------\n",
      "train Loss: 0.1203 Acc: 0.9629\n",
      "val Loss: 0.1042 Acc: 0.9668\n",
      "\n",
      "Epoch 23/299\n",
      "----------\n",
      "train Loss: 0.1199 Acc: 0.9623\n",
      "val Loss: 0.1036 Acc: 0.9651\n",
      "\n",
      "Epoch 24/299\n",
      "----------\n",
      "train Loss: 0.1178 Acc: 0.9623\n",
      "val Loss: 0.1012 Acc: 0.9680\n",
      "\n",
      "Epoch 25/299\n",
      "----------\n",
      "train Loss: 0.1170 Acc: 0.9630\n",
      "val Loss: 0.1004 Acc: 0.9676\n",
      "\n",
      "Epoch 26/299\n",
      "----------\n",
      "train Loss: 0.1160 Acc: 0.9618\n",
      "val Loss: 0.0990 Acc: 0.9680\n",
      "\n",
      "Epoch 27/299\n",
      "----------\n",
      "train Loss: 0.1167 Acc: 0.9631\n",
      "val Loss: 0.1031 Acc: 0.9676\n",
      "\n",
      "Epoch 28/299\n",
      "----------\n",
      "train Loss: 0.1148 Acc: 0.9632\n",
      "val Loss: 0.1004 Acc: 0.9709\n",
      "\n",
      "Epoch 29/299\n",
      "----------\n",
      "train Loss: 0.1134 Acc: 0.9634\n",
      "val Loss: 0.0955 Acc: 0.9705\n",
      "\n",
      "Epoch 30/299\n",
      "----------\n",
      "train Loss: 0.1109 Acc: 0.9638\n",
      "val Loss: 0.0960 Acc: 0.9730\n",
      "\n",
      "Epoch 31/299\n",
      "----------\n",
      "train Loss: 0.1107 Acc: 0.9643\n",
      "val Loss: 0.0938 Acc: 0.9697\n",
      "\n",
      "Epoch 32/299\n",
      "----------\n",
      "train Loss: 0.1093 Acc: 0.9635\n",
      "val Loss: 0.0951 Acc: 0.9718\n",
      "\n",
      "Epoch 33/299\n",
      "----------\n",
      "train Loss: 0.1091 Acc: 0.9653\n",
      "val Loss: 0.0940 Acc: 0.9676\n",
      "\n",
      "Epoch 34/299\n",
      "----------\n",
      "train Loss: 0.1080 Acc: 0.9645\n",
      "val Loss: 0.0914 Acc: 0.9714\n",
      "\n",
      "Epoch 35/299\n",
      "----------\n",
      "train Loss: 0.1058 Acc: 0.9659\n",
      "val Loss: 0.0914 Acc: 0.9701\n",
      "\n",
      "Epoch 36/299\n",
      "----------\n",
      "train Loss: 0.1066 Acc: 0.9655\n",
      "val Loss: 0.0897 Acc: 0.9718\n",
      "\n",
      "Epoch 37/299\n",
      "----------\n",
      "train Loss: 0.1052 Acc: 0.9652\n",
      "val Loss: 0.0893 Acc: 0.9718\n",
      "\n",
      "Epoch 38/299\n",
      "----------\n",
      "train Loss: 0.1051 Acc: 0.9663\n",
      "val Loss: 0.0886 Acc: 0.9722\n",
      "\n",
      "Epoch 39/299\n",
      "----------\n",
      "train Loss: 0.1029 Acc: 0.9668\n",
      "val Loss: 0.0919 Acc: 0.9672\n",
      "\n",
      "Epoch 40/299\n",
      "----------\n",
      "train Loss: 0.1051 Acc: 0.9657\n",
      "val Loss: 0.0887 Acc: 0.9693\n",
      "\n",
      "Epoch 41/299\n",
      "----------\n",
      "train Loss: 0.1018 Acc: 0.9672\n",
      "val Loss: 0.0872 Acc: 0.9705\n",
      "\n",
      "Epoch 42/299\n",
      "----------\n",
      "train Loss: 0.1021 Acc: 0.9679\n",
      "val Loss: 0.0893 Acc: 0.9755\n",
      "\n",
      "Epoch 43/299\n",
      "----------\n",
      "train Loss: 0.1017 Acc: 0.9675\n",
      "val Loss: 0.0871 Acc: 0.9743\n",
      "\n",
      "Epoch 44/299\n",
      "----------\n",
      "train Loss: 0.1002 Acc: 0.9665\n",
      "val Loss: 0.0869 Acc: 0.9701\n",
      "\n",
      "Epoch 45/299\n",
      "----------\n",
      "train Loss: 0.0985 Acc: 0.9673\n",
      "val Loss: 0.0867 Acc: 0.9697\n",
      "\n",
      "Epoch 46/299\n",
      "----------\n",
      "train Loss: 0.1002 Acc: 0.9674\n",
      "val Loss: 0.0842 Acc: 0.9738\n",
      "\n",
      "Epoch 47/299\n",
      "----------\n",
      "train Loss: 0.0995 Acc: 0.9665\n",
      "val Loss: 0.0850 Acc: 0.9738\n",
      "\n",
      "Epoch 48/299\n",
      "----------\n",
      "train Loss: 0.0981 Acc: 0.9659\n",
      "val Loss: 0.0837 Acc: 0.9743\n",
      "\n",
      "Epoch 49/299\n",
      "----------\n",
      "train Loss: 0.0965 Acc: 0.9672\n",
      "val Loss: 0.0848 Acc: 0.9768\n",
      "\n",
      "Epoch 50/299\n",
      "----------\n",
      "train Loss: 0.0964 Acc: 0.9686\n",
      "val Loss: 0.0827 Acc: 0.9759\n",
      "\n",
      "Epoch 51/299\n",
      "----------\n",
      "train Loss: 0.0979 Acc: 0.9678\n",
      "val Loss: 0.0834 Acc: 0.9768\n",
      "\n",
      "Epoch 52/299\n",
      "----------\n",
      "train Loss: 0.0956 Acc: 0.9686\n",
      "val Loss: 0.0824 Acc: 0.9755\n",
      "\n",
      "Epoch 53/299\n",
      "----------\n",
      "train Loss: 0.0978 Acc: 0.9676\n",
      "val Loss: 0.0834 Acc: 0.9772\n",
      "\n",
      "Epoch 54/299\n",
      "----------\n",
      "train Loss: 0.0965 Acc: 0.9661\n",
      "val Loss: 0.0818 Acc: 0.9734\n",
      "\n",
      "Epoch 55/299\n",
      "----------\n",
      "train Loss: 0.0963 Acc: 0.9682\n",
      "val Loss: 0.0809 Acc: 0.9730\n",
      "\n",
      "Epoch 56/299\n",
      "----------\n",
      "train Loss: 0.0958 Acc: 0.9695\n",
      "val Loss: 0.0804 Acc: 0.9755\n",
      "\n",
      "Epoch 57/299\n",
      "----------\n",
      "train Loss: 0.0960 Acc: 0.9668\n",
      "val Loss: 0.0803 Acc: 0.9747\n",
      "\n",
      "Epoch 58/299\n",
      "----------\n",
      "train Loss: 0.0953 Acc: 0.9695\n",
      "val Loss: 0.0801 Acc: 0.9763\n",
      "\n",
      "Epoch 59/299\n",
      "----------\n",
      "train Loss: 0.0944 Acc: 0.9695\n",
      "val Loss: 0.0820 Acc: 0.9718\n",
      "\n",
      "Epoch 60/299\n",
      "----------\n",
      "train Loss: 0.0952 Acc: 0.9675\n",
      "val Loss: 0.0792 Acc: 0.9768\n",
      "\n",
      "Epoch 61/299\n",
      "----------\n",
      "train Loss: 0.0927 Acc: 0.9705\n",
      "val Loss: 0.0789 Acc: 0.9768\n",
      "\n",
      "Epoch 62/299\n",
      "----------\n",
      "train Loss: 0.0957 Acc: 0.9686\n",
      "val Loss: 0.0787 Acc: 0.9759\n",
      "\n",
      "Epoch 63/299\n",
      "----------\n",
      "train Loss: 0.0935 Acc: 0.9670\n",
      "val Loss: 0.0783 Acc: 0.9759\n",
      "\n",
      "Epoch 64/299\n",
      "----------\n",
      "train Loss: 0.0941 Acc: 0.9676\n",
      "val Loss: 0.0792 Acc: 0.9763\n",
      "\n",
      "Epoch 65/299\n",
      "----------\n",
      "train Loss: 0.0962 Acc: 0.9670\n",
      "val Loss: 0.0781 Acc: 0.9776\n",
      "\n",
      "Epoch 66/299\n",
      "----------\n",
      "train Loss: 0.0935 Acc: 0.9667\n",
      "val Loss: 0.0777 Acc: 0.9772\n",
      "\n",
      "Epoch 67/299\n",
      "----------\n",
      "train Loss: 0.0923 Acc: 0.9682\n",
      "val Loss: 0.0784 Acc: 0.9738\n",
      "\n",
      "Epoch 68/299\n",
      "----------\n",
      "train Loss: 0.0904 Acc: 0.9692\n",
      "val Loss: 0.0782 Acc: 0.9738\n",
      "\n",
      "Epoch 69/299\n",
      "----------\n",
      "train Loss: 0.0907 Acc: 0.9696\n",
      "val Loss: 0.0772 Acc: 0.9772\n",
      "\n",
      "Epoch 70/299\n",
      "----------\n",
      "train Loss: 0.0900 Acc: 0.9688\n",
      "val Loss: 0.0768 Acc: 0.9776\n",
      "\n",
      "Epoch 71/299\n",
      "----------\n",
      "train Loss: 0.0894 Acc: 0.9691\n",
      "val Loss: 0.0764 Acc: 0.9776\n",
      "\n",
      "Epoch 72/299\n",
      "----------\n",
      "train Loss: 0.0931 Acc: 0.9695\n",
      "val Loss: 0.0776 Acc: 0.9763\n",
      "\n",
      "Epoch 73/299\n",
      "----------\n",
      "train Loss: 0.0895 Acc: 0.9698\n",
      "val Loss: 0.0759 Acc: 0.9763\n",
      "\n",
      "Epoch 74/299\n",
      "----------\n",
      "train Loss: 0.0896 Acc: 0.9696\n",
      "val Loss: 0.0767 Acc: 0.9747\n",
      "\n",
      "Epoch 75/299\n",
      "----------\n",
      "train Loss: 0.0895 Acc: 0.9691\n",
      "val Loss: 0.0754 Acc: 0.9768\n",
      "\n",
      "Epoch 76/299\n",
      "----------\n",
      "train Loss: 0.0892 Acc: 0.9673\n",
      "val Loss: 0.0756 Acc: 0.9768\n",
      "\n",
      "Epoch 77/299\n",
      "----------\n",
      "train Loss: 0.0926 Acc: 0.9681\n",
      "val Loss: 0.0786 Acc: 0.9784\n",
      "\n",
      "Epoch 78/299\n",
      "----------\n",
      "train Loss: 0.0880 Acc: 0.9691\n",
      "val Loss: 0.0749 Acc: 0.9768\n",
      "\n",
      "Epoch 79/299\n",
      "----------\n",
      "train Loss: 0.0887 Acc: 0.9703\n",
      "val Loss: 0.0756 Acc: 0.9768\n",
      "\n",
      "Epoch 80/299\n",
      "----------\n",
      "train Loss: 0.0877 Acc: 0.9698\n",
      "val Loss: 0.0755 Acc: 0.9755\n",
      "\n",
      "Epoch 81/299\n",
      "----------\n",
      "train Loss: 0.0866 Acc: 0.9692\n",
      "val Loss: 0.0743 Acc: 0.9768\n",
      "\n",
      "Epoch 82/299\n",
      "----------\n",
      "train Loss: 0.0892 Acc: 0.9703\n",
      "val Loss: 0.0747 Acc: 0.9772\n",
      "\n",
      "Epoch 83/299\n",
      "----------\n",
      "train Loss: 0.0877 Acc: 0.9711\n",
      "val Loss: 0.0749 Acc: 0.9747\n",
      "\n",
      "Epoch 84/299\n",
      "----------\n",
      "train Loss: 0.0879 Acc: 0.9705\n",
      "val Loss: 0.0766 Acc: 0.9743\n",
      "\n",
      "Epoch 85/299\n",
      "----------\n",
      "train Loss: 0.0884 Acc: 0.9717\n",
      "val Loss: 0.0747 Acc: 0.9763\n",
      "\n",
      "Epoch 86/299\n",
      "----------\n",
      "train Loss: 0.0879 Acc: 0.9710\n",
      "val Loss: 0.0735 Acc: 0.9784\n",
      "\n",
      "Epoch 87/299\n",
      "----------\n",
      "train Loss: 0.0867 Acc: 0.9711\n",
      "val Loss: 0.0733 Acc: 0.9772\n",
      "\n",
      "Epoch 88/299\n",
      "----------\n",
      "train Loss: 0.0851 Acc: 0.9704\n",
      "val Loss: 0.0770 Acc: 0.9734\n",
      "\n",
      "Epoch 89/299\n",
      "----------\n",
      "train Loss: 0.0882 Acc: 0.9696\n",
      "val Loss: 0.0739 Acc: 0.9788\n",
      "\n",
      "Epoch 90/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0854 Acc: 0.9708\n",
      "val Loss: 0.0725 Acc: 0.9772\n",
      "\n",
      "Epoch 91/299\n",
      "----------\n",
      "train Loss: 0.0868 Acc: 0.9697\n",
      "val Loss: 0.0726 Acc: 0.9768\n",
      "\n",
      "Epoch 92/299\n",
      "----------\n",
      "train Loss: 0.0880 Acc: 0.9711\n",
      "val Loss: 0.0739 Acc: 0.9797\n",
      "\n",
      "Epoch 93/299\n",
      "----------\n",
      "train Loss: 0.0879 Acc: 0.9700\n",
      "val Loss: 0.0722 Acc: 0.9772\n",
      "\n",
      "Epoch 94/299\n",
      "----------\n",
      "train Loss: 0.0874 Acc: 0.9692\n",
      "val Loss: 0.0717 Acc: 0.9772\n",
      "\n",
      "Epoch 95/299\n",
      "----------\n",
      "train Loss: 0.0861 Acc: 0.9700\n",
      "val Loss: 0.0717 Acc: 0.9780\n",
      "\n",
      "Epoch 96/299\n",
      "----------\n",
      "train Loss: 0.0885 Acc: 0.9709\n",
      "val Loss: 0.0724 Acc: 0.9768\n",
      "\n",
      "Epoch 97/299\n",
      "----------\n",
      "train Loss: 0.0854 Acc: 0.9724\n",
      "val Loss: 0.0730 Acc: 0.9772\n",
      "\n",
      "Epoch 98/299\n",
      "----------\n",
      "train Loss: 0.0839 Acc: 0.9724\n",
      "val Loss: 0.0714 Acc: 0.9776\n",
      "\n",
      "Epoch 99/299\n",
      "----------\n",
      "train Loss: 0.0842 Acc: 0.9724\n",
      "val Loss: 0.0712 Acc: 0.9772\n",
      "\n",
      "Epoch 100/299\n",
      "----------\n",
      "train Loss: 0.0859 Acc: 0.9699\n",
      "val Loss: 0.0724 Acc: 0.9788\n",
      "\n",
      "Epoch 101/299\n",
      "----------\n",
      "train Loss: 0.0824 Acc: 0.9720\n",
      "val Loss: 0.0709 Acc: 0.9776\n",
      "\n",
      "Epoch 102/299\n",
      "----------\n",
      "train Loss: 0.0882 Acc: 0.9691\n",
      "val Loss: 0.0711 Acc: 0.9784\n",
      "\n",
      "Epoch 103/299\n",
      "----------\n",
      "train Loss: 0.0843 Acc: 0.9705\n",
      "val Loss: 0.0718 Acc: 0.9788\n",
      "\n",
      "Epoch 104/299\n",
      "----------\n",
      "train Loss: 0.0883 Acc: 0.9702\n",
      "val Loss: 0.0707 Acc: 0.9788\n",
      "\n",
      "Epoch 105/299\n",
      "----------\n",
      "train Loss: 0.0835 Acc: 0.9714\n",
      "val Loss: 0.0725 Acc: 0.9759\n",
      "\n",
      "Epoch 106/299\n",
      "----------\n",
      "train Loss: 0.0840 Acc: 0.9713\n",
      "val Loss: 0.0713 Acc: 0.9788\n",
      "\n",
      "Epoch 107/299\n",
      "----------\n",
      "train Loss: 0.0847 Acc: 0.9701\n",
      "val Loss: 0.0706 Acc: 0.9772\n",
      "\n",
      "Epoch 108/299\n",
      "----------\n",
      "train Loss: 0.0828 Acc: 0.9718\n",
      "val Loss: 0.0707 Acc: 0.9768\n",
      "\n",
      "Epoch 109/299\n",
      "----------\n",
      "train Loss: 0.0865 Acc: 0.9707\n",
      "val Loss: 0.0703 Acc: 0.9776\n",
      "\n",
      "Epoch 110/299\n",
      "----------\n",
      "train Loss: 0.0838 Acc: 0.9721\n",
      "val Loss: 0.0700 Acc: 0.9776\n",
      "\n",
      "Epoch 111/299\n",
      "----------\n",
      "train Loss: 0.0853 Acc: 0.9681\n",
      "val Loss: 0.0713 Acc: 0.9768\n",
      "\n",
      "Epoch 112/299\n",
      "----------\n",
      "train Loss: 0.0864 Acc: 0.9714\n",
      "val Loss: 0.0732 Acc: 0.9747\n",
      "\n",
      "Epoch 113/299\n",
      "----------\n",
      "train Loss: 0.0844 Acc: 0.9705\n",
      "val Loss: 0.0699 Acc: 0.9776\n",
      "\n",
      "Epoch 114/299\n",
      "----------\n",
      "train Loss: 0.0848 Acc: 0.9697\n",
      "val Loss: 0.0706 Acc: 0.9797\n",
      "\n",
      "Epoch 115/299\n",
      "----------\n",
      "train Loss: 0.0813 Acc: 0.9745\n",
      "val Loss: 0.0720 Acc: 0.9768\n",
      "\n",
      "Epoch 116/299\n",
      "----------\n",
      "train Loss: 0.0832 Acc: 0.9714\n",
      "val Loss: 0.0698 Acc: 0.9776\n",
      "\n",
      "Epoch 117/299\n",
      "----------\n",
      "train Loss: 0.0822 Acc: 0.9713\n",
      "val Loss: 0.0706 Acc: 0.9763\n",
      "\n",
      "Epoch 118/299\n",
      "----------\n",
      "train Loss: 0.0822 Acc: 0.9725\n",
      "val Loss: 0.0693 Acc: 0.9776\n",
      "\n",
      "Epoch 119/299\n",
      "----------\n",
      "train Loss: 0.0802 Acc: 0.9735\n",
      "val Loss: 0.0695 Acc: 0.9784\n",
      "\n",
      "Epoch 120/299\n",
      "----------\n",
      "train Loss: 0.0812 Acc: 0.9723\n",
      "val Loss: 0.0692 Acc: 0.9780\n",
      "\n",
      "Epoch 121/299\n",
      "----------\n",
      "train Loss: 0.0818 Acc: 0.9726\n",
      "val Loss: 0.0691 Acc: 0.9784\n",
      "\n",
      "Epoch 122/299\n",
      "----------\n",
      "train Loss: 0.0840 Acc: 0.9705\n",
      "val Loss: 0.0700 Acc: 0.9763\n",
      "\n",
      "Epoch 123/299\n",
      "----------\n",
      "train Loss: 0.0825 Acc: 0.9701\n",
      "val Loss: 0.0709 Acc: 0.9797\n",
      "\n",
      "Epoch 124/299\n",
      "----------\n",
      "train Loss: 0.0806 Acc: 0.9719\n",
      "val Loss: 0.0689 Acc: 0.9772\n",
      "\n",
      "Epoch 125/299\n",
      "----------\n",
      "train Loss: 0.0801 Acc: 0.9729\n",
      "val Loss: 0.0727 Acc: 0.9747\n",
      "\n",
      "Epoch 126/299\n",
      "----------\n",
      "train Loss: 0.0822 Acc: 0.9709\n",
      "val Loss: 0.0689 Acc: 0.9780\n",
      "\n",
      "Epoch 127/299\n",
      "----------\n",
      "train Loss: 0.0828 Acc: 0.9716\n",
      "val Loss: 0.0685 Acc: 0.9772\n",
      "\n",
      "Epoch 128/299\n",
      "----------\n",
      "train Loss: 0.0815 Acc: 0.9713\n",
      "val Loss: 0.0684 Acc: 0.9776\n",
      "\n",
      "Epoch 129/299\n",
      "----------\n",
      "train Loss: 0.0808 Acc: 0.9728\n",
      "val Loss: 0.0684 Acc: 0.9797\n",
      "\n",
      "Epoch 130/299\n",
      "----------\n",
      "train Loss: 0.0802 Acc: 0.9720\n",
      "val Loss: 0.0681 Acc: 0.9788\n",
      "\n",
      "Epoch 131/299\n",
      "----------\n",
      "train Loss: 0.0828 Acc: 0.9701\n",
      "val Loss: 0.0685 Acc: 0.9801\n",
      "\n",
      "Epoch 132/299\n",
      "----------\n",
      "train Loss: 0.0838 Acc: 0.9721\n",
      "val Loss: 0.0696 Acc: 0.9797\n",
      "\n",
      "Epoch 133/299\n",
      "----------\n",
      "train Loss: 0.0829 Acc: 0.9703\n",
      "val Loss: 0.0678 Acc: 0.9797\n",
      "\n",
      "Epoch 134/299\n",
      "----------\n",
      "train Loss: 0.0802 Acc: 0.9730\n",
      "val Loss: 0.0679 Acc: 0.9784\n",
      "\n",
      "Epoch 135/299\n",
      "----------\n",
      "train Loss: 0.0813 Acc: 0.9716\n",
      "val Loss: 0.0688 Acc: 0.9772\n",
      "\n",
      "Epoch 136/299\n",
      "----------\n",
      "train Loss: 0.0801 Acc: 0.9729\n",
      "val Loss: 0.0685 Acc: 0.9776\n",
      "\n",
      "Epoch 137/299\n",
      "----------\n",
      "train Loss: 0.0821 Acc: 0.9710\n",
      "val Loss: 0.0685 Acc: 0.9797\n",
      "\n",
      "Epoch 138/299\n",
      "----------\n",
      "train Loss: 0.0791 Acc: 0.9732\n",
      "val Loss: 0.0679 Acc: 0.9784\n",
      "\n",
      "Epoch 139/299\n",
      "----------\n",
      "train Loss: 0.0823 Acc: 0.9710\n",
      "val Loss: 0.0675 Acc: 0.9784\n",
      "\n",
      "Epoch 140/299\n",
      "----------\n",
      "train Loss: 0.0788 Acc: 0.9747\n",
      "val Loss: 0.0685 Acc: 0.9809\n",
      "\n",
      "Epoch 141/299\n",
      "----------\n",
      "train Loss: 0.0819 Acc: 0.9707\n",
      "val Loss: 0.0678 Acc: 0.9784\n",
      "\n",
      "Epoch 142/299\n",
      "----------\n",
      "train Loss: 0.0810 Acc: 0.9711\n",
      "val Loss: 0.0675 Acc: 0.9780\n",
      "\n",
      "Epoch 143/299\n",
      "----------\n",
      "train Loss: 0.0816 Acc: 0.9725\n",
      "val Loss: 0.0673 Acc: 0.9792\n",
      "\n",
      "Epoch 144/299\n",
      "----------\n",
      "train Loss: 0.0801 Acc: 0.9713\n",
      "val Loss: 0.0703 Acc: 0.9768\n",
      "\n",
      "Epoch 145/299\n",
      "----------\n",
      "train Loss: 0.0807 Acc: 0.9725\n",
      "val Loss: 0.0672 Acc: 0.9784\n",
      "\n",
      "Epoch 146/299\n",
      "----------\n",
      "train Loss: 0.0783 Acc: 0.9724\n",
      "val Loss: 0.0683 Acc: 0.9768\n",
      "\n",
      "Epoch 147/299\n",
      "----------\n",
      "train Loss: 0.0833 Acc: 0.9704\n",
      "val Loss: 0.0679 Acc: 0.9801\n",
      "\n",
      "Epoch 148/299\n",
      "----------\n",
      "train Loss: 0.0807 Acc: 0.9721\n",
      "val Loss: 0.0700 Acc: 0.9772\n",
      "\n",
      "Epoch 149/299\n",
      "----------\n",
      "train Loss: 0.0793 Acc: 0.9713\n",
      "val Loss: 0.0667 Acc: 0.9784\n",
      "\n",
      "Epoch 150/299\n",
      "----------\n",
      "train Loss: 0.0779 Acc: 0.9734\n",
      "val Loss: 0.0689 Acc: 0.9768\n",
      "\n",
      "Epoch 151/299\n",
      "----------\n",
      "train Loss: 0.0789 Acc: 0.9723\n",
      "val Loss: 0.0685 Acc: 0.9768\n",
      "\n",
      "Epoch 152/299\n",
      "----------\n",
      "train Loss: 0.0771 Acc: 0.9755\n",
      "val Loss: 0.0678 Acc: 0.9776\n",
      "\n",
      "Epoch 153/299\n",
      "----------\n",
      "train Loss: 0.0786 Acc: 0.9709\n",
      "val Loss: 0.0668 Acc: 0.9801\n",
      "\n",
      "Epoch 154/299\n",
      "----------\n",
      "train Loss: 0.0797 Acc: 0.9727\n",
      "val Loss: 0.0688 Acc: 0.9768\n",
      "\n",
      "Epoch 155/299\n",
      "----------\n",
      "train Loss: 0.0793 Acc: 0.9723\n",
      "val Loss: 0.0663 Acc: 0.9780\n",
      "\n",
      "Epoch 156/299\n",
      "----------\n",
      "train Loss: 0.0794 Acc: 0.9721\n",
      "val Loss: 0.0666 Acc: 0.9792\n",
      "\n",
      "Epoch 157/299\n",
      "----------\n",
      "train Loss: 0.0800 Acc: 0.9707\n",
      "val Loss: 0.0665 Acc: 0.9792\n",
      "\n",
      "Epoch 158/299\n",
      "----------\n",
      "train Loss: 0.0777 Acc: 0.9733\n",
      "val Loss: 0.0666 Acc: 0.9780\n",
      "\n",
      "Epoch 159/299\n",
      "----------\n",
      "train Loss: 0.0782 Acc: 0.9728\n",
      "val Loss: 0.0670 Acc: 0.9801\n",
      "\n",
      "Epoch 160/299\n",
      "----------\n",
      "train Loss: 0.0799 Acc: 0.9720\n",
      "val Loss: 0.0678 Acc: 0.9780\n",
      "\n",
      "Epoch 161/299\n",
      "----------\n",
      "train Loss: 0.0785 Acc: 0.9709\n",
      "val Loss: 0.0667 Acc: 0.9801\n",
      "\n",
      "Epoch 162/299\n",
      "----------\n",
      "train Loss: 0.0797 Acc: 0.9722\n",
      "val Loss: 0.0662 Acc: 0.9788\n",
      "\n",
      "Epoch 163/299\n",
      "----------\n",
      "train Loss: 0.0771 Acc: 0.9725\n",
      "val Loss: 0.0667 Acc: 0.9809\n",
      "\n",
      "Epoch 164/299\n",
      "----------\n",
      "train Loss: 0.0798 Acc: 0.9712\n",
      "val Loss: 0.0661 Acc: 0.9784\n",
      "\n",
      "Epoch 165/299\n",
      "----------\n",
      "train Loss: 0.0773 Acc: 0.9723\n",
      "val Loss: 0.0658 Acc: 0.9792\n",
      "\n",
      "Epoch 166/299\n",
      "----------\n",
      "train Loss: 0.0822 Acc: 0.9702\n",
      "val Loss: 0.0670 Acc: 0.9784\n",
      "\n",
      "Epoch 167/299\n",
      "----------\n",
      "train Loss: 0.0774 Acc: 0.9735\n",
      "val Loss: 0.0697 Acc: 0.9763\n",
      "\n",
      "Epoch 168/299\n",
      "----------\n",
      "train Loss: 0.0783 Acc: 0.9726\n",
      "val Loss: 0.0670 Acc: 0.9784\n",
      "\n",
      "Epoch 169/299\n",
      "----------\n",
      "train Loss: 0.0783 Acc: 0.9731\n",
      "val Loss: 0.0662 Acc: 0.9788\n",
      "\n",
      "Epoch 170/299\n",
      "----------\n",
      "train Loss: 0.0789 Acc: 0.9717\n",
      "val Loss: 0.0655 Acc: 0.9797\n",
      "\n",
      "Epoch 171/299\n",
      "----------\n",
      "train Loss: 0.0786 Acc: 0.9726\n",
      "val Loss: 0.0671 Acc: 0.9768\n",
      "\n",
      "Epoch 172/299\n",
      "----------\n",
      "train Loss: 0.0771 Acc: 0.9732\n",
      "val Loss: 0.0660 Acc: 0.9780\n",
      "\n",
      "Epoch 173/299\n",
      "----------\n",
      "train Loss: 0.0779 Acc: 0.9732\n",
      "val Loss: 0.0657 Acc: 0.9801\n",
      "\n",
      "Epoch 174/299\n",
      "----------\n",
      "train Loss: 0.0790 Acc: 0.9723\n",
      "val Loss: 0.0655 Acc: 0.9805\n",
      "\n",
      "Epoch 175/299\n",
      "----------\n",
      "train Loss: 0.0792 Acc: 0.9708\n",
      "val Loss: 0.0650 Acc: 0.9788\n",
      "\n",
      "Epoch 176/299\n",
      "----------\n",
      "train Loss: 0.0778 Acc: 0.9723\n",
      "val Loss: 0.0660 Acc: 0.9805\n",
      "\n",
      "Epoch 177/299\n",
      "----------\n",
      "train Loss: 0.0770 Acc: 0.9725\n",
      "val Loss: 0.0650 Acc: 0.9792\n",
      "\n",
      "Epoch 178/299\n",
      "----------\n",
      "train Loss: 0.0780 Acc: 0.9739\n",
      "val Loss: 0.0650 Acc: 0.9788\n",
      "\n",
      "Epoch 179/299\n",
      "----------\n",
      "train Loss: 0.0768 Acc: 0.9727\n",
      "val Loss: 0.0653 Acc: 0.9801\n",
      "\n",
      "Epoch 180/299\n",
      "----------\n",
      "train Loss: 0.0769 Acc: 0.9718\n",
      "val Loss: 0.0654 Acc: 0.9784\n",
      "\n",
      "Epoch 181/299\n",
      "----------\n",
      "train Loss: 0.0777 Acc: 0.9726\n",
      "val Loss: 0.0654 Acc: 0.9797\n",
      "\n",
      "Epoch 182/299\n",
      "----------\n",
      "train Loss: 0.0778 Acc: 0.9749\n",
      "val Loss: 0.0648 Acc: 0.9792\n",
      "\n",
      "Epoch 183/299\n",
      "----------\n",
      "train Loss: 0.0767 Acc: 0.9746\n",
      "val Loss: 0.0659 Acc: 0.9788\n",
      "\n",
      "Epoch 184/299\n",
      "----------\n",
      "train Loss: 0.0795 Acc: 0.9724\n",
      "val Loss: 0.0654 Acc: 0.9801\n",
      "\n",
      "Epoch 185/299\n",
      "----------\n",
      "train Loss: 0.0790 Acc: 0.9714\n",
      "val Loss: 0.0645 Acc: 0.9780\n",
      "\n",
      "Epoch 186/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0792 Acc: 0.9711\n",
      "val Loss: 0.0648 Acc: 0.9788\n",
      "\n",
      "Epoch 187/299\n",
      "----------\n",
      "train Loss: 0.0767 Acc: 0.9741\n",
      "val Loss: 0.0650 Acc: 0.9788\n",
      "\n",
      "Epoch 188/299\n",
      "----------\n",
      "train Loss: 0.0770 Acc: 0.9717\n",
      "val Loss: 0.0647 Acc: 0.9797\n",
      "\n",
      "Epoch 189/299\n",
      "----------\n",
      "train Loss: 0.0785 Acc: 0.9726\n",
      "val Loss: 0.0652 Acc: 0.9780\n",
      "\n",
      "Epoch 190/299\n",
      "----------\n",
      "train Loss: 0.0769 Acc: 0.9726\n",
      "val Loss: 0.0662 Acc: 0.9784\n",
      "\n",
      "Epoch 191/299\n",
      "----------\n",
      "train Loss: 0.0753 Acc: 0.9716\n",
      "val Loss: 0.0646 Acc: 0.9797\n",
      "\n",
      "Epoch 192/299\n",
      "----------\n",
      "train Loss: 0.0755 Acc: 0.9753\n",
      "val Loss: 0.0654 Acc: 0.9780\n",
      "\n",
      "Epoch 193/299\n",
      "----------\n",
      "train Loss: 0.0785 Acc: 0.9718\n",
      "val Loss: 0.0652 Acc: 0.9805\n",
      "\n",
      "Epoch 194/299\n",
      "----------\n",
      "train Loss: 0.0758 Acc: 0.9735\n",
      "val Loss: 0.0647 Acc: 0.9805\n",
      "\n",
      "Epoch 195/299\n",
      "----------\n",
      "train Loss: 0.0783 Acc: 0.9721\n",
      "val Loss: 0.0647 Acc: 0.9780\n",
      "\n",
      "Epoch 196/299\n",
      "----------\n",
      "train Loss: 0.0766 Acc: 0.9756\n",
      "val Loss: 0.0652 Acc: 0.9784\n",
      "\n",
      "Epoch 197/299\n",
      "----------\n",
      "train Loss: 0.0745 Acc: 0.9741\n",
      "val Loss: 0.0642 Acc: 0.9797\n",
      "\n",
      "Epoch 198/299\n",
      "----------\n",
      "train Loss: 0.0772 Acc: 0.9720\n",
      "val Loss: 0.0640 Acc: 0.9801\n",
      "\n",
      "Epoch 199/299\n",
      "----------\n",
      "train Loss: 0.0778 Acc: 0.9723\n",
      "val Loss: 0.0650 Acc: 0.9780\n",
      "\n",
      "Epoch 200/299\n",
      "----------\n",
      "train Loss: 0.0786 Acc: 0.9721\n",
      "val Loss: 0.0649 Acc: 0.9788\n",
      "\n",
      "Epoch 201/299\n",
      "----------\n",
      "train Loss: 0.0768 Acc: 0.9724\n",
      "val Loss: 0.0646 Acc: 0.9780\n",
      "\n",
      "Epoch 202/299\n",
      "----------\n",
      "train Loss: 0.0796 Acc: 0.9703\n",
      "val Loss: 0.0641 Acc: 0.9788\n",
      "\n",
      "Epoch 203/299\n",
      "----------\n",
      "train Loss: 0.0747 Acc: 0.9734\n",
      "val Loss: 0.0650 Acc: 0.9805\n",
      "\n",
      "Epoch 204/299\n",
      "----------\n",
      "train Loss: 0.0770 Acc: 0.9735\n",
      "val Loss: 0.0640 Acc: 0.9797\n",
      "\n",
      "Epoch 205/299\n",
      "----------\n",
      "train Loss: 0.0762 Acc: 0.9726\n",
      "val Loss: 0.0638 Acc: 0.9801\n",
      "\n",
      "Epoch 206/299\n",
      "----------\n",
      "train Loss: 0.0769 Acc: 0.9725\n",
      "val Loss: 0.0642 Acc: 0.9788\n",
      "\n",
      "Epoch 207/299\n",
      "----------\n",
      "train Loss: 0.0803 Acc: 0.9721\n",
      "val Loss: 0.0644 Acc: 0.9788\n",
      "\n",
      "Epoch 208/299\n",
      "----------\n",
      "train Loss: 0.0719 Acc: 0.9735\n",
      "val Loss: 0.0652 Acc: 0.9776\n",
      "\n",
      "Epoch 209/299\n",
      "----------\n",
      "train Loss: 0.0751 Acc: 0.9744\n",
      "val Loss: 0.0638 Acc: 0.9801\n",
      "\n",
      "Epoch 210/299\n",
      "----------\n",
      "train Loss: 0.0769 Acc: 0.9739\n",
      "val Loss: 0.0641 Acc: 0.9797\n",
      "\n",
      "Epoch 211/299\n",
      "----------\n",
      "train Loss: 0.0769 Acc: 0.9743\n",
      "val Loss: 0.0637 Acc: 0.9792\n",
      "\n",
      "Epoch 212/299\n",
      "----------\n",
      "train Loss: 0.0728 Acc: 0.9736\n",
      "val Loss: 0.0637 Acc: 0.9792\n",
      "\n",
      "Epoch 213/299\n",
      "----------\n",
      "train Loss: 0.0757 Acc: 0.9740\n",
      "val Loss: 0.0649 Acc: 0.9801\n",
      "\n",
      "Epoch 214/299\n",
      "----------\n",
      "train Loss: 0.0786 Acc: 0.9724\n",
      "val Loss: 0.0634 Acc: 0.9797\n",
      "\n",
      "Epoch 215/299\n",
      "----------\n",
      "train Loss: 0.0747 Acc: 0.9727\n",
      "val Loss: 0.0664 Acc: 0.9780\n",
      "\n",
      "Epoch 216/299\n",
      "----------\n",
      "train Loss: 0.0763 Acc: 0.9727\n",
      "val Loss: 0.0649 Acc: 0.9797\n",
      "\n",
      "Epoch 217/299\n",
      "----------\n",
      "train Loss: 0.0748 Acc: 0.9734\n",
      "val Loss: 0.0644 Acc: 0.9780\n",
      "\n",
      "Epoch 218/299\n",
      "----------\n",
      "train Loss: 0.0757 Acc: 0.9736\n",
      "val Loss: 0.0641 Acc: 0.9772\n",
      "\n",
      "Epoch 219/299\n",
      "----------\n",
      "train Loss: 0.0760 Acc: 0.9717\n",
      "val Loss: 0.0676 Acc: 0.9772\n",
      "\n",
      "Epoch 220/299\n",
      "----------\n",
      "train Loss: 0.0785 Acc: 0.9704\n",
      "val Loss: 0.0654 Acc: 0.9780\n",
      "\n",
      "Epoch 221/299\n",
      "----------\n",
      "train Loss: 0.0802 Acc: 0.9720\n",
      "val Loss: 0.0638 Acc: 0.9784\n",
      "\n",
      "Epoch 222/299\n",
      "----------\n",
      "train Loss: 0.0740 Acc: 0.9734\n",
      "val Loss: 0.0632 Acc: 0.9792\n",
      "\n",
      "Epoch 223/299\n",
      "----------\n",
      "train Loss: 0.0747 Acc: 0.9734\n",
      "val Loss: 0.0646 Acc: 0.9780\n",
      "\n",
      "Epoch 224/299\n",
      "----------\n",
      "train Loss: 0.0746 Acc: 0.9740\n",
      "val Loss: 0.0639 Acc: 0.9788\n",
      "\n",
      "Epoch 225/299\n",
      "----------\n",
      "train Loss: 0.0773 Acc: 0.9730\n",
      "val Loss: 0.0632 Acc: 0.9805\n",
      "\n",
      "Epoch 226/299\n",
      "----------\n",
      "train Loss: 0.0750 Acc: 0.9738\n",
      "val Loss: 0.0635 Acc: 0.9809\n",
      "\n",
      "Epoch 227/299\n",
      "----------\n",
      "train Loss: 0.0754 Acc: 0.9731\n",
      "val Loss: 0.0647 Acc: 0.9776\n",
      "\n",
      "Epoch 228/299\n",
      "----------\n",
      "train Loss: 0.0741 Acc: 0.9755\n",
      "val Loss: 0.0633 Acc: 0.9788\n",
      "\n",
      "Epoch 229/299\n",
      "----------\n",
      "train Loss: 0.0737 Acc: 0.9738\n",
      "val Loss: 0.0644 Acc: 0.9784\n",
      "\n",
      "Epoch 230/299\n",
      "----------\n",
      "train Loss: 0.0778 Acc: 0.9726\n",
      "val Loss: 0.0646 Acc: 0.9772\n",
      "\n",
      "Epoch 231/299\n",
      "----------\n",
      "train Loss: 0.0781 Acc: 0.9720\n",
      "val Loss: 0.0630 Acc: 0.9784\n",
      "\n",
      "Epoch 232/299\n",
      "----------\n",
      "train Loss: 0.0749 Acc: 0.9726\n",
      "val Loss: 0.0638 Acc: 0.9788\n",
      "\n",
      "Epoch 233/299\n",
      "----------\n",
      "train Loss: 0.0722 Acc: 0.9750\n",
      "val Loss: 0.0642 Acc: 0.9801\n",
      "\n",
      "Epoch 234/299\n",
      "----------\n",
      "train Loss: 0.0772 Acc: 0.9733\n",
      "val Loss: 0.0638 Acc: 0.9813\n",
      "\n",
      "Epoch 235/299\n",
      "----------\n",
      "train Loss: 0.0742 Acc: 0.9725\n",
      "val Loss: 0.0632 Acc: 0.9805\n",
      "\n",
      "Epoch 236/299\n",
      "----------\n",
      "train Loss: 0.0733 Acc: 0.9741\n",
      "val Loss: 0.0634 Acc: 0.9805\n",
      "\n",
      "Epoch 237/299\n",
      "----------\n",
      "train Loss: 0.0781 Acc: 0.9727\n",
      "val Loss: 0.0669 Acc: 0.9801\n",
      "\n",
      "Epoch 238/299\n",
      "----------\n",
      "train Loss: 0.0749 Acc: 0.9732\n",
      "val Loss: 0.0629 Acc: 0.9784\n",
      "\n",
      "Epoch 239/299\n",
      "----------\n",
      "train Loss: 0.0725 Acc: 0.9738\n",
      "val Loss: 0.0630 Acc: 0.9792\n",
      "\n",
      "Epoch 240/299\n",
      "----------\n",
      "train Loss: 0.0759 Acc: 0.9716\n",
      "val Loss: 0.0641 Acc: 0.9801\n",
      "\n",
      "Epoch 241/299\n",
      "----------\n",
      "train Loss: 0.0747 Acc: 0.9739\n",
      "val Loss: 0.0633 Acc: 0.9805\n",
      "\n",
      "Epoch 242/299\n",
      "----------\n",
      "train Loss: 0.0749 Acc: 0.9729\n",
      "val Loss: 0.0629 Acc: 0.9784\n",
      "\n",
      "Epoch 243/299\n",
      "----------\n",
      "train Loss: 0.0767 Acc: 0.9719\n",
      "val Loss: 0.0636 Acc: 0.9784\n",
      "\n",
      "Epoch 244/299\n",
      "----------\n",
      "train Loss: 0.0753 Acc: 0.9729\n",
      "val Loss: 0.0646 Acc: 0.9805\n",
      "\n",
      "Epoch 245/299\n",
      "----------\n",
      "train Loss: 0.0757 Acc: 0.9733\n",
      "val Loss: 0.0630 Acc: 0.9809\n",
      "\n",
      "Epoch 246/299\n",
      "----------\n",
      "train Loss: 0.0743 Acc: 0.9746\n",
      "val Loss: 0.0633 Acc: 0.9797\n",
      "\n",
      "Epoch 247/299\n",
      "----------\n",
      "train Loss: 0.0744 Acc: 0.9740\n",
      "val Loss: 0.0629 Acc: 0.9792\n",
      "\n",
      "Epoch 248/299\n",
      "----------\n",
      "train Loss: 0.0755 Acc: 0.9722\n",
      "val Loss: 0.0633 Acc: 0.9809\n",
      "\n",
      "Epoch 249/299\n",
      "----------\n",
      "train Loss: 0.0751 Acc: 0.9746\n",
      "val Loss: 0.0626 Acc: 0.9797\n",
      "\n",
      "Epoch 250/299\n",
      "----------\n",
      "train Loss: 0.0750 Acc: 0.9741\n",
      "val Loss: 0.0628 Acc: 0.9805\n",
      "\n",
      "Epoch 251/299\n",
      "----------\n",
      "train Loss: 0.0731 Acc: 0.9748\n",
      "val Loss: 0.0628 Acc: 0.9805\n",
      "\n",
      "Epoch 252/299\n",
      "----------\n",
      "train Loss: 0.0740 Acc: 0.9750\n",
      "val Loss: 0.0636 Acc: 0.9788\n",
      "\n",
      "Epoch 253/299\n",
      "----------\n",
      "train Loss: 0.0739 Acc: 0.9735\n",
      "val Loss: 0.0633 Acc: 0.9780\n",
      "\n",
      "Epoch 254/299\n",
      "----------\n",
      "train Loss: 0.0697 Acc: 0.9754\n",
      "val Loss: 0.0624 Acc: 0.9797\n",
      "\n",
      "Epoch 255/299\n",
      "----------\n",
      "train Loss: 0.0736 Acc: 0.9753\n",
      "val Loss: 0.0628 Acc: 0.9792\n",
      "\n",
      "Epoch 256/299\n",
      "----------\n",
      "train Loss: 0.0749 Acc: 0.9731\n",
      "val Loss: 0.0624 Acc: 0.9797\n",
      "\n",
      "Epoch 257/299\n",
      "----------\n",
      "train Loss: 0.0771 Acc: 0.9729\n",
      "val Loss: 0.0642 Acc: 0.9768\n",
      "\n",
      "Epoch 258/299\n",
      "----------\n",
      "train Loss: 0.0743 Acc: 0.9740\n",
      "val Loss: 0.0631 Acc: 0.9784\n",
      "\n",
      "Epoch 259/299\n",
      "----------\n",
      "train Loss: 0.0744 Acc: 0.9733\n",
      "val Loss: 0.0635 Acc: 0.9780\n",
      "\n",
      "Epoch 260/299\n",
      "----------\n",
      "train Loss: 0.0749 Acc: 0.9726\n",
      "val Loss: 0.0628 Acc: 0.9784\n",
      "\n",
      "Epoch 261/299\n",
      "----------\n",
      "train Loss: 0.0746 Acc: 0.9740\n",
      "val Loss: 0.0635 Acc: 0.9780\n",
      "\n",
      "Epoch 262/299\n",
      "----------\n",
      "train Loss: 0.0725 Acc: 0.9737\n",
      "val Loss: 0.0632 Acc: 0.9784\n",
      "\n",
      "Epoch 263/299\n",
      "----------\n",
      "train Loss: 0.0760 Acc: 0.9723\n",
      "val Loss: 0.0648 Acc: 0.9776\n",
      "\n",
      "Epoch 264/299\n",
      "----------\n",
      "train Loss: 0.0751 Acc: 0.9736\n",
      "val Loss: 0.0642 Acc: 0.9801\n",
      "\n",
      "Epoch 265/299\n",
      "----------\n",
      "train Loss: 0.0722 Acc: 0.9749\n",
      "val Loss: 0.0623 Acc: 0.9797\n",
      "\n",
      "Epoch 266/299\n",
      "----------\n",
      "train Loss: 0.0726 Acc: 0.9734\n",
      "val Loss: 0.0625 Acc: 0.9792\n",
      "\n",
      "Epoch 267/299\n",
      "----------\n",
      "train Loss: 0.0761 Acc: 0.9728\n",
      "val Loss: 0.0641 Acc: 0.9805\n",
      "\n",
      "Epoch 268/299\n",
      "----------\n",
      "train Loss: 0.0692 Acc: 0.9749\n",
      "val Loss: 0.0624 Acc: 0.9817\n",
      "\n",
      "Epoch 269/299\n",
      "----------\n",
      "train Loss: 0.0751 Acc: 0.9747\n",
      "val Loss: 0.0622 Acc: 0.9813\n",
      "\n",
      "Epoch 270/299\n",
      "----------\n",
      "train Loss: 0.0725 Acc: 0.9740\n",
      "val Loss: 0.0624 Acc: 0.9809\n",
      "\n",
      "Epoch 271/299\n",
      "----------\n",
      "train Loss: 0.0734 Acc: 0.9725\n",
      "val Loss: 0.0627 Acc: 0.9805\n",
      "\n",
      "Epoch 272/299\n",
      "----------\n",
      "train Loss: 0.0714 Acc: 0.9750\n",
      "val Loss: 0.0640 Acc: 0.9780\n",
      "\n",
      "Epoch 273/299\n",
      "----------\n",
      "train Loss: 0.0775 Acc: 0.9723\n",
      "val Loss: 0.0638 Acc: 0.9801\n",
      "\n",
      "Epoch 274/299\n",
      "----------\n",
      "train Loss: 0.0751 Acc: 0.9734\n",
      "val Loss: 0.0621 Acc: 0.9801\n",
      "\n",
      "Epoch 275/299\n",
      "----------\n",
      "train Loss: 0.0767 Acc: 0.9714\n",
      "val Loss: 0.0626 Acc: 0.9780\n",
      "\n",
      "Epoch 276/299\n",
      "----------\n",
      "train Loss: 0.0742 Acc: 0.9741\n",
      "val Loss: 0.0622 Acc: 0.9809\n",
      "\n",
      "Epoch 277/299\n",
      "----------\n",
      "train Loss: 0.0735 Acc: 0.9737\n",
      "val Loss: 0.0621 Acc: 0.9801\n",
      "\n",
      "Epoch 278/299\n",
      "----------\n",
      "train Loss: 0.0763 Acc: 0.9727\n",
      "val Loss: 0.0621 Acc: 0.9801\n",
      "\n",
      "Epoch 279/299\n",
      "----------\n",
      "train Loss: 0.0750 Acc: 0.9738\n",
      "val Loss: 0.0672 Acc: 0.9763\n",
      "\n",
      "Epoch 280/299\n",
      "----------\n",
      "train Loss: 0.0744 Acc: 0.9719\n",
      "val Loss: 0.0619 Acc: 0.9797\n",
      "\n",
      "Epoch 281/299\n",
      "----------\n",
      "train Loss: 0.0715 Acc: 0.9747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0628 Acc: 0.9801\n",
      "\n",
      "Epoch 282/299\n",
      "----------\n",
      "train Loss: 0.0748 Acc: 0.9744\n",
      "val Loss: 0.0622 Acc: 0.9792\n",
      "\n",
      "Epoch 283/299\n",
      "----------\n",
      "train Loss: 0.0745 Acc: 0.9725\n",
      "val Loss: 0.0632 Acc: 0.9813\n",
      "\n",
      "Epoch 284/299\n",
      "----------\n",
      "train Loss: 0.0718 Acc: 0.9741\n",
      "val Loss: 0.0626 Acc: 0.9792\n",
      "\n",
      "Epoch 285/299\n",
      "----------\n",
      "train Loss: 0.0755 Acc: 0.9724\n",
      "val Loss: 0.0634 Acc: 0.9792\n",
      "\n",
      "Epoch 286/299\n",
      "----------\n",
      "train Loss: 0.0755 Acc: 0.9724\n",
      "val Loss: 0.0619 Acc: 0.9809\n",
      "\n",
      "Epoch 287/299\n",
      "----------\n",
      "train Loss: 0.0728 Acc: 0.9747\n",
      "val Loss: 0.0626 Acc: 0.9788\n",
      "\n",
      "Epoch 288/299\n",
      "----------\n",
      "train Loss: 0.0748 Acc: 0.9737\n",
      "val Loss: 0.0635 Acc: 0.9784\n",
      "\n",
      "Epoch 289/299\n",
      "----------\n",
      "train Loss: 0.0733 Acc: 0.9738\n",
      "val Loss: 0.0627 Acc: 0.9809\n",
      "\n",
      "Epoch 290/299\n",
      "----------\n",
      "train Loss: 0.0724 Acc: 0.9744\n",
      "val Loss: 0.0628 Acc: 0.9797\n",
      "\n",
      "Epoch 291/299\n",
      "----------\n",
      "train Loss: 0.0720 Acc: 0.9729\n",
      "val Loss: 0.0621 Acc: 0.9801\n",
      "\n",
      "Epoch 292/299\n",
      "----------\n",
      "train Loss: 0.0732 Acc: 0.9737\n",
      "val Loss: 0.0616 Acc: 0.9805\n",
      "\n",
      "Epoch 293/299\n",
      "----------\n",
      "train Loss: 0.0722 Acc: 0.9753\n",
      "val Loss: 0.0629 Acc: 0.9772\n",
      "\n",
      "Epoch 294/299\n",
      "----------\n",
      "train Loss: 0.0758 Acc: 0.9753\n",
      "val Loss: 0.0619 Acc: 0.9792\n",
      "\n",
      "Epoch 295/299\n",
      "----------\n",
      "train Loss: 0.0731 Acc: 0.9748\n",
      "val Loss: 0.0615 Acc: 0.9797\n",
      "\n",
      "Epoch 296/299\n",
      "----------\n",
      "train Loss: 0.0741 Acc: 0.9722\n",
      "val Loss: 0.0614 Acc: 0.9809\n",
      "\n",
      "Epoch 297/299\n",
      "----------\n",
      "train Loss: 0.0798 Acc: 0.9714\n",
      "val Loss: 0.0613 Acc: 0.9801\n",
      "\n",
      "Epoch 298/299\n",
      "----------\n",
      "train Loss: 0.0735 Acc: 0.9735\n",
      "val Loss: 0.0656 Acc: 0.9755\n",
      "\n",
      "Epoch 299/299\n",
      "----------\n",
      "train Loss: 0.0708 Acc: 0.9748\n",
      "val Loss: 0.0619 Acc: 0.9780\n",
      "\n",
      "\n",
      "##############################\n",
      "------ Summary ------\n",
      "model -> squeezenet\n",
      "epochs -> 300\n",
      "lr -> 0.0001\n",
      "batch size -> 8\n",
      "optimizer -> ADAM\n",
      "criteriun -> CrossEntropyLoss\n",
      "\n",
      "Training complete in 152m 43s\n",
      "Best val Acc: 0.981735\n",
      "##############################\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== END ====\n",
      "[[791   0   0]\n",
      " [  0 779  20]\n",
      " [  4  20 795]]\n",
      "\n",
      "Sensitivity or recall total\n",
      "0.9817351598173516\n",
      "\n",
      "Sensitivity or recall per classes\n",
      "[1.   0.97 0.97]\n",
      "\n",
      "Precision\n",
      "[0.99 0.97 0.98]\n",
      "\n",
      "F1 Score\n",
      "[1.   0.97 0.97]\n",
      "Confusion matrix, without normalization\n",
      "\n",
      "============================================================\n",
      "==== INITIALIZING WITH PARAMETERS: ====\n",
      "model -> squeezenet\n",
      "epochs -> 300\n",
      "lr -> 0.0001\n",
      "batch size -> 8\n",
      "optimizer -> 2\n",
      "criteriun -> 2\n",
      "\n",
      "--------------------\n",
      "Params to learn:\n",
      "\t classifier.1.weight\n",
      "\t classifier.1.bias\n",
      "\n",
      "--------------------\n",
      "\n",
      "== Epochs ==\n",
      "Epoch 0/299\n",
      "----------\n",
      "train Loss: -17.8177 Acc: 0.3258\n",
      "val Loss: -35.0713 Acc: 0.3520\n",
      "\n",
      "Epoch 1/299\n",
      "----------\n",
      "train Loss: -51.4627 Acc: 0.3474\n",
      "val Loss: -68.1625 Acc: 0.3570\n",
      "\n",
      "Epoch 2/299\n",
      "----------\n",
      "train Loss: -84.2700 Acc: 0.3555\n",
      "val Loss: -100.9338 Acc: 0.3715\n",
      "\n",
      "Epoch 3/299\n",
      "----------\n",
      "train Loss: -116.8398 Acc: 0.3784\n",
      "val Loss: -133.5810 Acc: 0.3944\n",
      "\n",
      "Epoch 4/299\n",
      "----------\n",
      "train Loss: -149.4152 Acc: 0.3889\n",
      "val Loss: -166.2083 Acc: 0.4363\n",
      "\n",
      "Epoch 5/299\n",
      "----------\n",
      "train Loss: -181.9471 Acc: 0.4244\n",
      "val Loss: -198.8018 Acc: 0.4720\n",
      "\n",
      "Epoch 6/299\n",
      "----------\n",
      "train Loss: -214.3459 Acc: 0.4509\n",
      "val Loss: -231.3487 Acc: 0.4848\n",
      "\n",
      "Epoch 7/299\n",
      "----------\n",
      "train Loss: -246.8364 Acc: 0.4562\n",
      "val Loss: -263.9191 Acc: 0.4969\n",
      "\n",
      "Epoch 8/299\n",
      "----------\n",
      "train Loss: -279.2493 Acc: 0.4676\n",
      "val Loss: -296.4605 Acc: 0.4932\n",
      "\n",
      "Epoch 9/299\n",
      "----------\n",
      "train Loss: -311.7346 Acc: 0.4817\n",
      "val Loss: -329.0088 Acc: 0.4720\n",
      "\n",
      "Epoch 10/299\n",
      "----------\n",
      "train Loss: -344.2604 Acc: 0.4489\n",
      "val Loss: -361.5594 Acc: 0.4819\n",
      "\n",
      "Epoch 11/299\n",
      "----------\n",
      "train Loss: -376.5933 Acc: 0.4521\n",
      "val Loss: -394.0773 Acc: 0.4716\n",
      "\n",
      "Epoch 12/299\n",
      "----------\n",
      "train Loss: -408.9887 Acc: 0.4707\n",
      "val Loss: -426.6284 Acc: 0.4645\n",
      "\n",
      "Epoch 13/299\n",
      "----------\n",
      "train Loss: -441.6258 Acc: 0.4797\n",
      "val Loss: -459.2035 Acc: 0.4305\n",
      "\n",
      "Epoch 14/299\n",
      "----------\n",
      "train Loss: -474.0684 Acc: 0.4568\n",
      "val Loss: -491.6857 Acc: 0.4384\n",
      "\n",
      "Epoch 15/299\n",
      "----------\n",
      "train Loss: -506.4527 Acc: 0.4595\n",
      "val Loss: -524.1748 Acc: 0.4321\n",
      "\n",
      "Epoch 16/299\n",
      "----------\n",
      "train Loss: -538.8718 Acc: 0.4177\n",
      "val Loss: -556.6817 Acc: 0.4429\n",
      "\n",
      "Epoch 17/299\n",
      "----------\n",
      "train Loss: -571.1100 Acc: 0.4074\n",
      "val Loss: -589.2025 Acc: 0.4483\n",
      "\n",
      "Epoch 18/299\n",
      "----------\n",
      "train Loss: -603.7168 Acc: 0.4335\n",
      "val Loss: -621.7486 Acc: 0.4276\n",
      "\n",
      "Epoch 19/299\n",
      "----------\n",
      "train Loss: -635.9164 Acc: 0.4264\n",
      "val Loss: -654.2441 Acc: 0.3989\n",
      "\n",
      "Epoch 20/299\n",
      "----------\n",
      "train Loss: -668.6997 Acc: 0.3989\n",
      "val Loss: -686.7617 Acc: 0.4093\n",
      "\n",
      "Epoch 21/299\n",
      "----------\n",
      "train Loss: -700.8124 Acc: 0.4034\n",
      "val Loss: -719.2647 Acc: 0.4027\n",
      "\n",
      "Epoch 22/299\n",
      "----------\n",
      "train Loss: -733.2673 Acc: 0.3819\n",
      "val Loss: -751.8096 Acc: 0.3989\n",
      "\n",
      "Epoch 23/299\n",
      "----------\n",
      "train Loss: -765.9004 Acc: 0.4007\n",
      "val Loss: -784.3284 Acc: 0.3811\n",
      "\n",
      "Epoch 24/299\n",
      "----------\n",
      "train Loss: -798.1430 Acc: 0.3834\n",
      "val Loss: -816.8134 Acc: 0.3778\n",
      "\n",
      "Epoch 25/299\n",
      "----------\n",
      "train Loss: -830.2578 Acc: 0.3766\n",
      "val Loss: -849.3304 Acc: 0.3786\n",
      "\n",
      "Epoch 26/299\n",
      "----------\n",
      "train Loss: -862.8201 Acc: 0.3975\n",
      "val Loss: -881.8652 Acc: 0.3694\n",
      "\n",
      "Epoch 27/299\n",
      "----------\n",
      "train Loss: -895.5976 Acc: 0.3809\n",
      "val Loss: -914.4017 Acc: 0.3678\n",
      "\n",
      "Epoch 28/299\n",
      "----------\n",
      "train Loss: -927.7862 Acc: 0.3659\n",
      "val Loss: -946.9194 Acc: 0.3686\n",
      "\n",
      "Epoch 29/299\n",
      "----------\n",
      "train Loss: -960.2318 Acc: 0.3677\n",
      "val Loss: -979.4386 Acc: 0.3765\n",
      "\n",
      "Epoch 30/299\n",
      "----------\n",
      "train Loss: -992.7009 Acc: 0.3952\n",
      "val Loss: -1011.9608 Acc: 0.3773\n",
      "\n",
      "Epoch 31/299\n",
      "----------\n",
      "train Loss: -1025.3025 Acc: 0.3839\n",
      "val Loss: -1044.4925 Acc: 0.3719\n",
      "\n",
      "Epoch 32/299\n",
      "----------\n",
      "train Loss: -1057.6048 Acc: 0.3712\n",
      "val Loss: -1077.0408 Acc: 0.3678\n",
      "\n",
      "Epoch 33/299\n",
      "----------\n",
      "train Loss: -1090.0011 Acc: 0.3733\n",
      "val Loss: -1109.5425 Acc: 0.3657\n",
      "\n",
      "Epoch 34/299\n",
      "----------\n",
      "train Loss: -1122.5211 Acc: 0.3735\n",
      "val Loss: -1142.0559 Acc: 0.3624\n",
      "\n",
      "Epoch 35/299\n",
      "----------\n",
      "train Loss: -1155.0442 Acc: 0.3778\n",
      "val Loss: -1174.5915 Acc: 0.3611\n",
      "\n",
      "Epoch 36/299\n",
      "----------\n",
      "train Loss: -1187.1223 Acc: 0.3706\n",
      "val Loss: -1207.1280 Acc: 0.3616\n",
      "\n",
      "Epoch 37/299\n",
      "----------\n",
      "train Loss: -1219.8108 Acc: 0.3600\n",
      "val Loss: -1239.6732 Acc: 0.3616\n",
      "\n",
      "Epoch 38/299\n",
      "----------\n",
      "train Loss: -1251.9194 Acc: 0.3581\n",
      "val Loss: -1272.1927 Acc: 0.3653\n",
      "\n",
      "Epoch 39/299\n",
      "----------\n",
      "train Loss: -1284.2230 Acc: 0.3619\n",
      "val Loss: -1304.7274 Acc: 0.3682\n",
      "\n",
      "Epoch 40/299\n",
      "----------\n",
      "train Loss: -1316.9209 Acc: 0.3794\n",
      "val Loss: -1337.2021 Acc: 0.3628\n",
      "\n",
      "Epoch 41/299\n",
      "----------\n",
      "train Loss: -1349.1456 Acc: 0.3606\n",
      "val Loss: -1369.6773 Acc: 0.3624\n",
      "\n",
      "Epoch 42/299\n",
      "----------\n",
      "train Loss: -1381.5452 Acc: 0.3690\n",
      "val Loss: -1402.1538 Acc: 0.3582\n",
      "\n",
      "Epoch 43/299\n",
      "----------\n",
      "train Loss: -1414.1397 Acc: 0.3567\n",
      "val Loss: -1434.6066 Acc: 0.3603\n",
      "\n",
      "Epoch 44/299\n",
      "----------\n",
      "train Loss: -1446.5190 Acc: 0.3563\n",
      "val Loss: -1467.0982 Acc: 0.3591\n",
      "\n",
      "Epoch 45/299\n",
      "----------\n",
      "train Loss: -1478.9617 Acc: 0.3617\n",
      "val Loss: -1499.6381 Acc: 0.3545\n",
      "\n",
      "Epoch 46/299\n",
      "----------\n",
      "train Loss: -1511.0594 Acc: 0.3526\n",
      "val Loss: -1532.1807 Acc: 0.3603\n",
      "\n",
      "Epoch 47/299\n",
      "----------\n",
      "train Loss: -1543.9754 Acc: 0.3588\n",
      "val Loss: -1564.7294 Acc: 0.3578\n",
      "\n",
      "Epoch 48/299\n",
      "----------\n",
      "train Loss: -1576.1479 Acc: 0.3600\n",
      "val Loss: -1597.2350 Acc: 0.3553\n",
      "\n",
      "Epoch 49/299\n",
      "----------\n",
      "train Loss: -1608.6789 Acc: 0.3558\n",
      "val Loss: -1629.7664 Acc: 0.3599\n",
      "\n",
      "Epoch 50/299\n",
      "----------\n",
      "train Loss: -1641.0832 Acc: 0.3558\n",
      "val Loss: -1662.2611 Acc: 0.3611\n",
      "\n",
      "Epoch 51/299\n",
      "----------\n",
      "train Loss: -1673.5804 Acc: 0.3607\n",
      "val Loss: -1694.7919 Acc: 0.3578\n",
      "\n",
      "Epoch 52/299\n",
      "----------\n",
      "train Loss: -1705.7050 Acc: 0.3622\n",
      "val Loss: -1727.3695 Acc: 0.3557\n",
      "\n",
      "Epoch 53/299\n",
      "----------\n",
      "train Loss: -1738.5374 Acc: 0.3569\n",
      "val Loss: -1759.9697 Acc: 0.3541\n",
      "\n",
      "Epoch 54/299\n",
      "----------\n",
      "train Loss: -1771.0418 Acc: 0.3533\n",
      "val Loss: -1792.5826 Acc: 0.3545\n",
      "\n",
      "Epoch 55/299\n",
      "----------\n",
      "train Loss: -1803.6733 Acc: 0.3547\n",
      "val Loss: -1825.1849 Acc: 0.3537\n",
      "\n",
      "Epoch 56/299\n",
      "----------\n",
      "train Loss: -1835.6465 Acc: 0.3505\n",
      "val Loss: -1857.7080 Acc: 0.3566\n",
      "\n",
      "Epoch 57/299\n",
      "----------\n",
      "train Loss: -1868.1650 Acc: 0.3569\n",
      "val Loss: -1890.2360 Acc: 0.3553\n",
      "\n",
      "Epoch 58/299\n",
      "----------\n",
      "train Loss: -1900.9521 Acc: 0.3581\n",
      "val Loss: -1922.7630 Acc: 0.3549\n",
      "\n",
      "Epoch 59/299\n",
      "----------\n",
      "train Loss: -1932.8480 Acc: 0.3570\n",
      "val Loss: -1955.3061 Acc: 0.3566\n",
      "\n",
      "Epoch 60/299\n",
      "----------\n",
      "train Loss: -1965.3445 Acc: 0.3557\n",
      "val Loss: -1987.8721 Acc: 0.3557\n",
      "\n",
      "Epoch 61/299\n",
      "----------\n",
      "train Loss: -1998.1697 Acc: 0.3564\n",
      "val Loss: -2020.4635 Acc: 0.3562\n",
      "\n",
      "Epoch 62/299\n",
      "----------\n",
      "train Loss: -2030.4121 Acc: 0.3559\n",
      "val Loss: -2053.0260 Acc: 0.3533\n",
      "\n",
      "Epoch 63/299\n",
      "----------\n",
      "train Loss: -2062.5073 Acc: 0.3551\n",
      "val Loss: -2085.5481 Acc: 0.3524\n",
      "\n",
      "Epoch 64/299\n",
      "----------\n",
      "train Loss: -2095.4393 Acc: 0.3516\n",
      "val Loss: -2118.0772 Acc: 0.3533\n",
      "\n",
      "Epoch 65/299\n",
      "----------\n",
      "train Loss: -2128.4463 Acc: 0.3514\n",
      "val Loss: -2150.6209 Acc: 0.3533\n",
      "\n",
      "Epoch 66/299\n",
      "----------\n",
      "train Loss: -2160.8157 Acc: 0.3577\n",
      "val Loss: -2183.1843 Acc: 0.3533\n",
      "\n",
      "Epoch 67/299\n",
      "----------\n",
      "train Loss: -2192.5646 Acc: 0.3508\n",
      "val Loss: -2215.7012 Acc: 0.3516\n",
      "\n",
      "Epoch 68/299\n",
      "----------\n",
      "train Loss: -2224.6712 Acc: 0.3500\n",
      "val Loss: -2248.2020 Acc: 0.3528\n",
      "\n",
      "Epoch 69/299\n",
      "----------\n",
      "train Loss: -2256.9784 Acc: 0.3517\n",
      "val Loss: -2280.7150 Acc: 0.3528\n",
      "\n",
      "Epoch 70/299\n",
      "----------\n",
      "train Loss: -2290.1041 Acc: 0.3525\n",
      "val Loss: -2313.2530 Acc: 0.3512\n",
      "\n",
      "Epoch 71/299\n",
      "----------\n",
      "train Loss: -2322.8334 Acc: 0.3520\n",
      "val Loss: -2345.7854 Acc: 0.3499\n",
      "\n",
      "Epoch 72/299\n",
      "----------\n",
      "train Loss: -2354.7165 Acc: 0.3508\n",
      "val Loss: -2378.2615 Acc: 0.3504\n",
      "\n",
      "Epoch 73/299\n",
      "----------\n",
      "train Loss: -2387.1396 Acc: 0.3512\n",
      "val Loss: -2410.7645 Acc: 0.3499\n",
      "\n",
      "Epoch 74/299\n",
      "----------\n",
      "train Loss: -2419.7175 Acc: 0.3529\n",
      "val Loss: -2443.3062 Acc: 0.3495\n",
      "\n",
      "Epoch 75/299\n",
      "----------\n",
      "train Loss: -2452.1851 Acc: 0.3542\n",
      "val Loss: -2475.8105 Acc: 0.3508\n",
      "\n",
      "Epoch 76/299\n",
      "----------\n",
      "train Loss: -2485.0710 Acc: 0.3522\n",
      "val Loss: -2508.3348 Acc: 0.3508\n",
      "\n",
      "Epoch 77/299\n",
      "----------\n",
      "train Loss: -2517.4854 Acc: 0.3537\n",
      "val Loss: -2540.8702 Acc: 0.3504\n",
      "\n",
      "Epoch 78/299\n",
      "----------\n",
      "train Loss: -2549.4054 Acc: 0.3510\n",
      "val Loss: -2573.4232 Acc: 0.3495\n",
      "\n",
      "Epoch 79/299\n",
      "----------\n",
      "train Loss: -2582.6786 Acc: 0.3550\n",
      "val Loss: -2605.9798 Acc: 0.3495\n",
      "\n",
      "Epoch 80/299\n",
      "----------\n",
      "train Loss: -2614.6913 Acc: 0.3509\n",
      "val Loss: -2638.5236 Acc: 0.3495\n",
      "\n",
      "Epoch 81/299\n",
      "----------\n",
      "train Loss: -2646.6059 Acc: 0.3501\n",
      "val Loss: -2671.0747 Acc: 0.3504\n",
      "\n",
      "Epoch 82/299\n",
      "----------\n",
      "train Loss: -2679.0863 Acc: 0.3502\n",
      "val Loss: -2703.6151 Acc: 0.3499\n",
      "\n",
      "Epoch 83/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: -2711.5150 Acc: 0.3516\n",
      "val Loss: -2736.1843 Acc: 0.3499\n",
      "\n",
      "Epoch 84/299\n",
      "----------\n",
      "train Loss: -2744.7060 Acc: 0.3530\n",
      "val Loss: -2768.7091 Acc: 0.3495\n",
      "\n",
      "Epoch 85/299\n",
      "----------\n",
      "train Loss: -2776.6680 Acc: 0.3527\n",
      "val Loss: -2801.1878 Acc: 0.3495\n",
      "\n",
      "Epoch 86/299\n",
      "----------\n",
      "train Loss: -2808.7930 Acc: 0.3505\n",
      "val Loss: -2833.6347 Acc: 0.3487\n",
      "\n",
      "Epoch 87/299\n",
      "----------\n",
      "train Loss: -2841.6621 Acc: 0.3505\n",
      "val Loss: -2866.1572 Acc: 0.3487\n",
      "\n",
      "Epoch 88/299\n",
      "----------\n",
      "train Loss: -2873.6348 Acc: 0.3514\n",
      "val Loss: -2898.6800 Acc: 0.3495\n",
      "\n",
      "Epoch 89/299\n",
      "----------\n",
      "train Loss: -2905.9592 Acc: 0.3503\n",
      "val Loss: -2931.1855 Acc: 0.3491\n",
      "\n",
      "Epoch 90/299\n",
      "----------\n",
      "train Loss: -2938.8332 Acc: 0.3500\n",
      "val Loss: -2963.7121 Acc: 0.3495\n",
      "\n",
      "Epoch 91/299\n",
      "----------\n",
      "train Loss: -2971.4509 Acc: 0.3528\n",
      "val Loss: -2996.2315 Acc: 0.3495\n",
      "\n",
      "Epoch 92/299\n",
      "----------\n",
      "train Loss: -3003.0830 Acc: 0.3514\n",
      "val Loss: -3028.7598 Acc: 0.3524\n",
      "\n",
      "Epoch 93/299\n",
      "----------\n",
      "train Loss: -3035.8381 Acc: 0.3526\n",
      "val Loss: -3061.3416 Acc: 0.3524\n",
      "\n",
      "Epoch 94/299\n",
      "----------\n",
      "train Loss: -3068.5042 Acc: 0.3535\n",
      "val Loss: -3093.9010 Acc: 0.3528\n",
      "\n",
      "Epoch 95/299\n",
      "----------\n",
      "train Loss: -3100.6617 Acc: 0.3524\n",
      "val Loss: -3126.4454 Acc: 0.3508\n",
      "\n",
      "Epoch 96/299\n",
      "----------\n",
      "train Loss: -3133.1150 Acc: 0.3531\n",
      "val Loss: -3158.9543 Acc: 0.3504\n",
      "\n",
      "Epoch 97/299\n",
      "----------\n",
      "train Loss: -3165.6555 Acc: 0.3512\n",
      "val Loss: -3191.4615 Acc: 0.3508\n",
      "\n",
      "Epoch 98/299\n",
      "----------\n",
      "train Loss: -3197.5664 Acc: 0.3531\n",
      "val Loss: -3223.9475 Acc: 0.3504\n",
      "\n",
      "Epoch 99/299\n",
      "----------\n",
      "train Loss: -3230.6280 Acc: 0.3523\n",
      "val Loss: -3256.4339 Acc: 0.3541\n",
      "\n",
      "Epoch 100/299\n",
      "----------\n",
      "train Loss: -3262.8039 Acc: 0.3544\n",
      "val Loss: -3288.9333 Acc: 0.3508\n",
      "\n",
      "Epoch 101/299\n",
      "----------\n",
      "train Loss: -3295.5595 Acc: 0.3523\n",
      "val Loss: -3321.4211 Acc: 0.3528\n",
      "\n",
      "Epoch 102/299\n",
      "----------\n",
      "train Loss: -3326.8412 Acc: 0.3516\n",
      "val Loss: -3353.9407 Acc: 0.3545\n",
      "\n",
      "Epoch 103/299\n",
      "----------\n",
      "train Loss: -3360.3650 Acc: 0.3532\n",
      "val Loss: -3386.4649 Acc: 0.3528\n",
      "\n",
      "Epoch 104/299\n",
      "----------\n",
      "train Loss: -3393.4900 Acc: 0.3530\n",
      "val Loss: -3418.9561 Acc: 0.3541\n",
      "\n",
      "Epoch 105/299\n",
      "----------\n",
      "train Loss: -3425.3915 Acc: 0.3531\n",
      "val Loss: -3451.4674 Acc: 0.3533\n",
      "\n",
      "Epoch 106/299\n",
      "----------\n",
      "train Loss: -3457.0369 Acc: 0.3518\n",
      "val Loss: -3484.0242 Acc: 0.3524\n",
      "\n",
      "Epoch 107/299\n",
      "----------\n",
      "train Loss: -3489.5490 Acc: 0.3523\n",
      "val Loss: -3516.5702 Acc: 0.3524\n",
      "\n",
      "Epoch 108/299\n",
      "----------\n",
      "train Loss: -3521.7320 Acc: 0.3518\n",
      "val Loss: -3549.1311 Acc: 0.3508\n",
      "\n",
      "Epoch 109/299\n",
      "----------\n",
      "train Loss: -3554.3795 Acc: 0.3518\n",
      "val Loss: -3581.6838 Acc: 0.3508\n",
      "\n",
      "Epoch 110/299\n",
      "----------\n",
      "train Loss: -3587.1451 Acc: 0.3518\n",
      "val Loss: -3614.2392 Acc: 0.3487\n",
      "\n",
      "Epoch 111/299\n",
      "----------\n",
      "train Loss: -3619.6882 Acc: 0.3520\n",
      "val Loss: -3646.7566 Acc: 0.3491\n",
      "\n",
      "Epoch 112/299\n",
      "----------\n",
      "train Loss: -3651.6486 Acc: 0.3486\n",
      "val Loss: -3679.2320 Acc: 0.3495\n",
      "\n",
      "Epoch 113/299\n",
      "----------\n",
      "train Loss: -3684.0295 Acc: 0.3515\n",
      "val Loss: -3711.6970 Acc: 0.3504\n",
      "\n",
      "Epoch 114/299\n",
      "----------\n",
      "train Loss: -3716.5988 Acc: 0.3532\n",
      "val Loss: -3744.1917 Acc: 0.3508\n",
      "\n",
      "Epoch 115/299\n",
      "----------\n",
      "train Loss: -3749.9624 Acc: 0.3511\n",
      "val Loss: -3776.6970 Acc: 0.3512\n",
      "\n",
      "Epoch 116/299\n",
      "----------\n",
      "train Loss: -3781.4732 Acc: 0.3526\n",
      "val Loss: -3809.2007 Acc: 0.3504\n",
      "\n",
      "Epoch 117/299\n",
      "----------\n",
      "train Loss: -3813.9991 Acc: 0.3503\n",
      "val Loss: -3841.6946 Acc: 0.3504\n",
      "\n",
      "Epoch 118/299\n",
      "----------\n",
      "train Loss: -3846.8951 Acc: 0.3514\n",
      "val Loss: -3874.2398 Acc: 0.3495\n",
      "\n",
      "Epoch 119/299\n",
      "----------\n",
      "train Loss: -3878.4098 Acc: 0.3509\n",
      "val Loss: -3906.8169 Acc: 0.3495\n",
      "\n",
      "Epoch 120/299\n",
      "----------\n",
      "train Loss: -3911.4953 Acc: 0.3515\n",
      "val Loss: -3939.3681 Acc: 0.3504\n",
      "\n",
      "Epoch 121/299\n",
      "----------\n",
      "train Loss: -3943.3265 Acc: 0.3520\n",
      "val Loss: -3971.9020 Acc: 0.3499\n",
      "\n",
      "Epoch 122/299\n",
      "----------\n",
      "train Loss: -3976.5965 Acc: 0.3511\n",
      "val Loss: -4004.4334 Acc: 0.3491\n",
      "\n",
      "Epoch 123/299\n",
      "----------\n",
      "train Loss: -4009.6095 Acc: 0.3516\n",
      "val Loss: -4036.9281 Acc: 0.3495\n",
      "\n",
      "Epoch 124/299\n",
      "----------\n",
      "train Loss: -4042.0610 Acc: 0.3512\n",
      "val Loss: -4069.5020 Acc: 0.3495\n",
      "\n",
      "Epoch 125/299\n",
      "----------\n",
      "train Loss: -4074.9700 Acc: 0.3507\n",
      "val Loss: -4102.0634 Acc: 0.3504\n",
      "\n",
      "Epoch 126/299\n",
      "----------\n",
      "train Loss: -4106.6029 Acc: 0.3528\n",
      "val Loss: -4134.5686 Acc: 0.3499\n",
      "\n",
      "Epoch 127/299\n",
      "----------\n",
      "train Loss: -4139.1409 Acc: 0.3507\n",
      "val Loss: -4167.0698 Acc: 0.3499\n",
      "\n",
      "Epoch 128/299\n",
      "----------\n",
      "train Loss: -4171.2933 Acc: 0.3516\n",
      "val Loss: -4199.5628 Acc: 0.3516\n",
      "\n",
      "Epoch 129/299\n",
      "----------\n",
      "train Loss: -4203.4252 Acc: 0.3500\n",
      "val Loss: -4232.0600 Acc: 0.3499\n",
      "\n",
      "Epoch 130/299\n",
      "----------\n",
      "train Loss: -4235.7552 Acc: 0.3507\n",
      "val Loss: -4264.5715 Acc: 0.3499\n",
      "\n",
      "Epoch 131/299\n",
      "----------\n",
      "train Loss: -4268.9705 Acc: 0.3524\n",
      "val Loss: -4297.1112 Acc: 0.3499\n",
      "\n",
      "Epoch 132/299\n",
      "----------\n",
      "train Loss: -4300.6414 Acc: 0.3505\n",
      "val Loss: -4329.6379 Acc: 0.3499\n",
      "\n",
      "Epoch 133/299\n",
      "----------\n",
      "train Loss: -4332.1269 Acc: 0.3536\n",
      "val Loss: -4362.1534 Acc: 0.3512\n",
      "\n",
      "Epoch 134/299\n",
      "----------\n",
      "train Loss: -4365.1267 Acc: 0.3504\n",
      "val Loss: -4394.6227 Acc: 0.3512\n",
      "\n",
      "Epoch 135/299\n",
      "----------\n",
      "train Loss: -4397.6872 Acc: 0.3496\n",
      "val Loss: -4427.0845 Acc: 0.3495\n",
      "\n",
      "Epoch 136/299\n",
      "----------\n",
      "train Loss: -4428.8168 Acc: 0.3510\n",
      "val Loss: -4459.5868 Acc: 0.3495\n",
      "\n",
      "Epoch 137/299\n",
      "----------\n",
      "train Loss: -4462.9137 Acc: 0.3510\n",
      "val Loss: -4492.0900 Acc: 0.3516\n",
      "\n",
      "Epoch 138/299\n",
      "----------\n",
      "train Loss: -4494.8173 Acc: 0.3502\n",
      "val Loss: -4524.5998 Acc: 0.3495\n",
      "\n",
      "Epoch 139/299\n",
      "----------\n",
      "train Loss: -4527.4249 Acc: 0.3510\n",
      "val Loss: -4557.1417 Acc: 0.3508\n",
      "\n",
      "Epoch 140/299\n",
      "----------\n",
      "train Loss: -4561.4421 Acc: 0.3507\n",
      "val Loss: -4589.6941 Acc: 0.3520\n",
      "\n",
      "Epoch 141/299\n",
      "----------\n",
      "train Loss: -4591.8607 Acc: 0.3502\n",
      "val Loss: -4622.2089 Acc: 0.3520\n",
      "\n",
      "Epoch 142/299\n",
      "----------\n",
      "train Loss: -4623.9683 Acc: 0.3509\n",
      "val Loss: -4654.7118 Acc: 0.3520\n",
      "\n",
      "Epoch 143/299\n",
      "----------\n",
      "train Loss: -4657.4413 Acc: 0.3523\n",
      "val Loss: -4687.2406 Acc: 0.3520\n",
      "\n",
      "Epoch 144/299\n",
      "----------\n",
      "train Loss: -4688.5680 Acc: 0.3515\n",
      "val Loss: -4719.7505 Acc: 0.3520\n",
      "\n",
      "Epoch 145/299\n",
      "----------\n",
      "train Loss: -4721.6725 Acc: 0.3527\n",
      "val Loss: -4752.2571 Acc: 0.3508\n",
      "\n",
      "Epoch 146/299\n",
      "----------\n",
      "train Loss: -4754.7601 Acc: 0.3518\n",
      "val Loss: -4784.7492 Acc: 0.3512\n",
      "\n",
      "Epoch 147/299\n",
      "----------\n",
      "train Loss: -4786.7635 Acc: 0.3507\n",
      "val Loss: -4817.2646 Acc: 0.3499\n",
      "\n",
      "Epoch 148/299\n",
      "----------\n",
      "train Loss: -4819.3823 Acc: 0.3510\n",
      "val Loss: -4849.7931 Acc: 0.3499\n",
      "\n",
      "Epoch 149/299\n",
      "----------\n",
      "train Loss: -4852.6225 Acc: 0.3513\n",
      "val Loss: -4882.3001 Acc: 0.3520\n",
      "\n",
      "Epoch 150/299\n",
      "----------\n",
      "train Loss: -4884.5031 Acc: 0.3508\n",
      "val Loss: -4914.8011 Acc: 0.3520\n",
      "\n",
      "Epoch 151/299\n",
      "----------\n",
      "train Loss: -4915.3941 Acc: 0.3517\n",
      "val Loss: -4947.3164 Acc: 0.3508\n",
      "\n",
      "Epoch 152/299\n",
      "----------\n",
      "train Loss: -4949.5949 Acc: 0.3527\n",
      "val Loss: -4979.8380 Acc: 0.3508\n",
      "\n",
      "Epoch 153/299\n",
      "----------\n",
      "train Loss: -4981.4831 Acc: 0.3522\n",
      "val Loss: -5012.3433 Acc: 0.3516\n",
      "\n",
      "Epoch 154/299\n",
      "----------\n",
      "train Loss: -5013.3997 Acc: 0.3497\n",
      "val Loss: -5044.8252 Acc: 0.3533\n",
      "\n",
      "Epoch 155/299\n",
      "----------\n",
      "train Loss: -5046.5186 Acc: 0.3536\n",
      "val Loss: -5077.3503 Acc: 0.3537\n",
      "\n",
      "Epoch 156/299\n",
      "----------\n",
      "train Loss: -5078.4817 Acc: 0.3542\n",
      "val Loss: -5109.8966 Acc: 0.3537\n",
      "\n",
      "Epoch 157/299\n",
      "----------\n",
      "train Loss: -5110.4570 Acc: 0.3537\n",
      "val Loss: -5142.4203 Acc: 0.3533\n",
      "\n",
      "Epoch 158/299\n",
      "----------\n",
      "train Loss: -5142.9501 Acc: 0.3531\n",
      "val Loss: -5174.9537 Acc: 0.3545\n",
      "\n",
      "Epoch 159/299\n",
      "----------\n",
      "train Loss: -5174.6534 Acc: 0.3533\n",
      "val Loss: -5207.4713 Acc: 0.3545\n",
      "\n",
      "Epoch 160/299\n",
      "----------\n",
      "train Loss: -5208.1385 Acc: 0.3532\n",
      "val Loss: -5240.0387 Acc: 0.3545\n",
      "\n",
      "Epoch 161/299\n",
      "----------\n",
      "train Loss: -5241.1662 Acc: 0.3536\n",
      "val Loss: -5272.5636 Acc: 0.3545\n",
      "\n",
      "Epoch 162/299\n",
      "----------\n",
      "train Loss: -5272.8178 Acc: 0.3559\n",
      "val Loss: -5305.0465 Acc: 0.3549\n",
      "\n",
      "Epoch 163/299\n",
      "----------\n",
      "train Loss: -5306.3951 Acc: 0.3537\n",
      "val Loss: -5337.5518 Acc: 0.3541\n",
      "\n",
      "Epoch 164/299\n",
      "----------\n",
      "train Loss: -5336.8204 Acc: 0.3543\n",
      "val Loss: -5370.0180 Acc: 0.3553\n",
      "\n",
      "Epoch 165/299\n",
      "----------\n",
      "train Loss: -5370.4056 Acc: 0.3563\n",
      "val Loss: -5402.5588 Acc: 0.3545\n",
      "\n",
      "Epoch 166/299\n",
      "----------\n",
      "train Loss: -5403.1361 Acc: 0.3530\n",
      "val Loss: -5435.1369 Acc: 0.3553\n",
      "\n",
      "Epoch 167/299\n",
      "----------\n",
      "train Loss: -5435.1260 Acc: 0.3542\n",
      "val Loss: -5467.7124 Acc: 0.3545\n",
      "\n",
      "Epoch 168/299\n",
      "----------\n",
      "train Loss: -5467.1737 Acc: 0.3521\n",
      "val Loss: -5500.2818 Acc: 0.3557\n",
      "\n",
      "Epoch 169/299\n",
      "----------\n",
      "train Loss: -5501.7294 Acc: 0.3542\n",
      "val Loss: -5532.8488 Acc: 0.3566\n",
      "\n",
      "Epoch 170/299\n",
      "----------\n",
      "train Loss: -5533.8025 Acc: 0.3536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: -5565.4315 Acc: 0.3553\n",
      "\n",
      "Epoch 171/299\n",
      "----------\n",
      "train Loss: -5565.4746 Acc: 0.3545\n",
      "val Loss: -5597.9612 Acc: 0.3566\n",
      "\n",
      "Epoch 172/299\n",
      "----------\n",
      "train Loss: -5598.3068 Acc: 0.3545\n",
      "val Loss: -5630.4534 Acc: 0.3562\n",
      "\n",
      "Epoch 173/299\n",
      "----------\n",
      "train Loss: -5629.1885 Acc: 0.3553\n",
      "val Loss: -5662.9564 Acc: 0.3557\n",
      "\n",
      "Epoch 174/299\n",
      "----------\n",
      "train Loss: -5662.9058 Acc: 0.3547\n",
      "val Loss: -5695.5046 Acc: 0.3553\n",
      "\n",
      "Epoch 175/299\n",
      "----------\n",
      "train Loss: -5694.7483 Acc: 0.3550\n",
      "val Loss: -5728.0136 Acc: 0.3553\n",
      "\n",
      "Epoch 176/299\n",
      "----------\n",
      "train Loss: -5727.0389 Acc: 0.3550\n",
      "val Loss: -5760.5137 Acc: 0.3553\n",
      "\n",
      "Epoch 177/299\n",
      "----------\n",
      "train Loss: -5758.4841 Acc: 0.3549\n",
      "val Loss: -5793.0341 Acc: 0.3553\n",
      "\n",
      "Epoch 178/299\n",
      "----------\n",
      "train Loss: -5792.0631 Acc: 0.3542\n",
      "val Loss: -5825.5660 Acc: 0.3545\n",
      "\n",
      "Epoch 179/299\n",
      "----------\n",
      "train Loss: -5825.0117 Acc: 0.3542\n",
      "val Loss: -5858.0373 Acc: 0.3541\n",
      "\n",
      "Epoch 180/299\n",
      "----------\n",
      "train Loss: -5857.7291 Acc: 0.3533\n",
      "val Loss: -5890.4943 Acc: 0.3537\n",
      "\n",
      "Epoch 181/299\n",
      "----------\n",
      "train Loss: -5888.8999 Acc: 0.3543\n",
      "val Loss: -5922.9938 Acc: 0.3537\n",
      "\n",
      "Epoch 182/299\n",
      "----------\n",
      "train Loss: -5922.1671 Acc: 0.3535\n",
      "val Loss: -5955.5264 Acc: 0.3541\n",
      "\n",
      "Epoch 183/299\n",
      "----------\n",
      "train Loss: -5952.6297 Acc: 0.3542\n",
      "val Loss: -5988.0371 Acc: 0.3537\n",
      "\n",
      "Epoch 184/299\n",
      "----------\n",
      "train Loss: -5988.1189 Acc: 0.3534\n",
      "val Loss: -6020.5625 Acc: 0.3533\n",
      "\n",
      "Epoch 185/299\n",
      "----------\n",
      "train Loss: -6020.1233 Acc: 0.3534\n",
      "val Loss: -6053.0901 Acc: 0.3537\n",
      "\n",
      "Epoch 186/299\n",
      "----------\n",
      "train Loss: -6050.6856 Acc: 0.3527\n",
      "val Loss: -6085.5739 Acc: 0.3545\n",
      "\n",
      "Epoch 187/299\n",
      "----------\n",
      "train Loss: -6083.9727 Acc: 0.3548\n",
      "val Loss: -6118.0154 Acc: 0.3533\n",
      "\n",
      "Epoch 188/299\n",
      "----------\n",
      "train Loss: -6118.0572 Acc: 0.3529\n",
      "val Loss: -6150.4519 Acc: 0.3537\n",
      "\n",
      "Epoch 189/299\n",
      "----------\n",
      "train Loss: -6149.1818 Acc: 0.3527\n",
      "val Loss: -6182.9099 Acc: 0.3537\n",
      "\n",
      "Epoch 190/299\n",
      "----------\n",
      "train Loss: -6180.1481 Acc: 0.3535\n",
      "val Loss: -6215.3605 Acc: 0.3549\n",
      "\n",
      "Epoch 191/299\n",
      "----------\n",
      "train Loss: -6212.6179 Acc: 0.3536\n",
      "val Loss: -6247.8951 Acc: 0.3541\n",
      "\n",
      "Epoch 192/299\n",
      "----------\n",
      "train Loss: -6243.5057 Acc: 0.3545\n",
      "val Loss: -6280.4552 Acc: 0.3541\n",
      "\n",
      "Epoch 193/299\n",
      "----------\n",
      "train Loss: -6277.0126 Acc: 0.3535\n",
      "val Loss: -6312.9694 Acc: 0.3541\n",
      "\n",
      "Epoch 194/299\n",
      "----------\n",
      "train Loss: -6311.8152 Acc: 0.3541\n",
      "val Loss: -6345.5098 Acc: 0.3541\n",
      "\n",
      "Epoch 195/299\n",
      "----------\n",
      "train Loss: -6343.9280 Acc: 0.3542\n",
      "val Loss: -6378.0584 Acc: 0.3533\n",
      "\n",
      "Epoch 196/299\n",
      "----------\n",
      "train Loss: -6374.7092 Acc: 0.3552\n",
      "val Loss: -6410.6294 Acc: 0.3541\n",
      "\n",
      "Epoch 197/299\n",
      "----------\n",
      "train Loss: -6407.0901 Acc: 0.3521\n",
      "val Loss: -6443.2352 Acc: 0.3533\n",
      "\n",
      "Epoch 198/299\n",
      "----------\n",
      "train Loss: -6439.6713 Acc: 0.3554\n",
      "val Loss: -6475.8397 Acc: 0.3541\n",
      "\n",
      "Epoch 199/299\n",
      "----------\n",
      "train Loss: -6471.9922 Acc: 0.3525\n",
      "val Loss: -6508.4185 Acc: 0.3541\n",
      "\n",
      "Epoch 200/299\n",
      "----------\n",
      "train Loss: -6505.6203 Acc: 0.3539\n",
      "val Loss: -6540.9817 Acc: 0.3549\n",
      "\n",
      "Epoch 201/299\n",
      "----------\n",
      "train Loss: -6536.9453 Acc: 0.3552\n",
      "val Loss: -6573.5392 Acc: 0.3545\n",
      "\n",
      "Epoch 202/299\n",
      "----------\n",
      "train Loss: -6570.8493 Acc: 0.3553\n",
      "val Loss: -6606.0494 Acc: 0.3545\n",
      "\n",
      "Epoch 203/299\n",
      "----------\n",
      "train Loss: -6602.6875 Acc: 0.3538\n",
      "val Loss: -6638.5979 Acc: 0.3541\n",
      "\n",
      "Epoch 204/299\n",
      "----------\n",
      "train Loss: -6634.8881 Acc: 0.3545\n",
      "val Loss: -6671.1377 Acc: 0.3545\n",
      "\n",
      "Epoch 205/299\n",
      "----------\n",
      "train Loss: -6666.3475 Acc: 0.3534\n",
      "val Loss: -6703.7342 Acc: 0.3545\n",
      "\n",
      "Epoch 206/299\n",
      "----------\n",
      "train Loss: -6698.0944 Acc: 0.3538\n",
      "val Loss: -6736.3014 Acc: 0.3545\n",
      "\n",
      "Epoch 207/299\n",
      "----------\n",
      "train Loss: -6732.1388 Acc: 0.3557\n",
      "val Loss: -6768.8464 Acc: 0.3533\n",
      "\n",
      "Epoch 208/299\n",
      "----------\n",
      "train Loss: -6764.7111 Acc: 0.3534\n",
      "val Loss: -6801.3687 Acc: 0.3537\n",
      "\n",
      "Epoch 209/299\n",
      "----------\n",
      "train Loss: -6796.0640 Acc: 0.3539\n",
      "val Loss: -6833.8992 Acc: 0.3545\n",
      "\n",
      "Epoch 210/299\n",
      "----------\n",
      "train Loss: -6829.0431 Acc: 0.3539\n",
      "val Loss: -6866.4499 Acc: 0.3541\n",
      "\n",
      "Epoch 211/299\n",
      "----------\n",
      "train Loss: -6861.3735 Acc: 0.3541\n",
      "val Loss: -6899.0062 Acc: 0.3533\n",
      "\n",
      "Epoch 212/299\n",
      "----------\n",
      "train Loss: -6895.3488 Acc: 0.3531\n",
      "val Loss: -6931.5050 Acc: 0.3528\n",
      "\n",
      "Epoch 213/299\n",
      "----------\n",
      "train Loss: -6924.7931 Acc: 0.3533\n",
      "val Loss: -6963.9618 Acc: 0.3533\n",
      "\n",
      "Epoch 214/299\n",
      "----------\n",
      "train Loss: -6960.4238 Acc: 0.3536\n",
      "val Loss: -6996.4682 Acc: 0.3537\n",
      "\n",
      "Epoch 215/299\n",
      "----------\n",
      "train Loss: -6991.4983 Acc: 0.3538\n",
      "val Loss: -7028.9830 Acc: 0.3528\n",
      "\n",
      "Epoch 216/299\n",
      "----------\n",
      "train Loss: -7024.3086 Acc: 0.3548\n",
      "val Loss: -7061.5118 Acc: 0.3524\n",
      "\n",
      "Epoch 217/299\n",
      "----------\n",
      "train Loss: -7057.4258 Acc: 0.3521\n",
      "val Loss: -7093.9756 Acc: 0.3524\n",
      "\n",
      "Epoch 218/299\n",
      "----------\n",
      "train Loss: -7088.3882 Acc: 0.3532\n",
      "val Loss: -7126.4514 Acc: 0.3524\n",
      "\n",
      "Epoch 219/299\n",
      "----------\n",
      "train Loss: -7123.6486 Acc: 0.3527\n",
      "val Loss: -7158.9306 Acc: 0.3524\n",
      "\n",
      "Epoch 220/299\n",
      "----------\n",
      "train Loss: -7153.5609 Acc: 0.3540\n",
      "val Loss: -7191.4680 Acc: 0.3524\n",
      "\n",
      "Epoch 221/299\n",
      "----------\n",
      "train Loss: -7184.3650 Acc: 0.3531\n",
      "val Loss: -7224.0129 Acc: 0.3524\n",
      "\n",
      "Epoch 222/299\n",
      "----------\n",
      "train Loss: -7218.2197 Acc: 0.3522\n",
      "val Loss: -7256.5396 Acc: 0.3524\n",
      "\n",
      "Epoch 223/299\n",
      "----------\n",
      "train Loss: -7253.6425 Acc: 0.3530\n",
      "val Loss: -7289.1049 Acc: 0.3524\n",
      "\n",
      "Epoch 224/299\n",
      "----------\n",
      "train Loss: -7284.8706 Acc: 0.3529\n",
      "val Loss: -7321.6854 Acc: 0.3528\n",
      "\n",
      "Epoch 225/299\n",
      "----------\n",
      "train Loss: -7315.5556 Acc: 0.3525\n",
      "val Loss: -7354.2385 Acc: 0.3537\n",
      "\n",
      "Epoch 226/299\n",
      "----------\n",
      "train Loss: -7349.0514 Acc: 0.3534\n",
      "val Loss: -7386.7780 Acc: 0.3537\n",
      "\n",
      "Epoch 227/299\n",
      "----------\n",
      "train Loss: -7379.9771 Acc: 0.3534\n",
      "val Loss: -7419.2880 Acc: 0.3528\n",
      "\n",
      "Epoch 228/299\n",
      "----------\n",
      "train Loss: -7413.1525 Acc: 0.3556\n",
      "val Loss: -7451.7756 Acc: 0.3533\n",
      "\n",
      "Epoch 229/299\n",
      "----------\n",
      "train Loss: -7445.5811 Acc: 0.3526\n",
      "val Loss: -7484.2500 Acc: 0.3533\n",
      "\n",
      "Epoch 230/299\n",
      "----------\n",
      "train Loss: -7477.7975 Acc: 0.3534\n",
      "val Loss: -7516.7518 Acc: 0.3537\n",
      "\n",
      "Epoch 231/299\n",
      "----------\n",
      "train Loss: -7510.0302 Acc: 0.3542\n",
      "val Loss: -7549.2853 Acc: 0.3528\n",
      "\n",
      "Epoch 232/299\n",
      "----------\n",
      "train Loss: -7542.0875 Acc: 0.3526\n",
      "val Loss: -7581.8668 Acc: 0.3528\n",
      "\n",
      "Epoch 233/299\n",
      "----------\n",
      "train Loss: -7574.9978 Acc: 0.3557\n",
      "val Loss: -7614.4215 Acc: 0.3524\n",
      "\n",
      "Epoch 234/299\n",
      "----------\n",
      "train Loss: -7608.4026 Acc: 0.3524\n",
      "val Loss: -7646.9618 Acc: 0.3533\n",
      "\n",
      "Epoch 235/299\n",
      "----------\n",
      "train Loss: -7641.3290 Acc: 0.3530\n",
      "val Loss: -7679.4797 Acc: 0.3520\n",
      "\n",
      "Epoch 236/299\n",
      "----------\n",
      "train Loss: -7673.7992 Acc: 0.3534\n",
      "val Loss: -7711.9948 Acc: 0.3520\n",
      "\n",
      "Epoch 237/299\n",
      "----------\n",
      "train Loss: -7705.6428 Acc: 0.3532\n",
      "val Loss: -7744.4805 Acc: 0.3520\n",
      "\n",
      "Epoch 238/299\n",
      "----------\n",
      "train Loss: -7739.4065 Acc: 0.3529\n",
      "val Loss: -7776.9549 Acc: 0.3516\n",
      "\n",
      "Epoch 239/299\n",
      "----------\n",
      "train Loss: -7769.7135 Acc: 0.3530\n",
      "val Loss: -7809.4204 Acc: 0.3520\n",
      "\n",
      "Epoch 240/299\n",
      "----------\n",
      "train Loss: -7801.8790 Acc: 0.3525\n",
      "val Loss: -7841.9200 Acc: 0.3520\n",
      "\n",
      "Epoch 241/299\n",
      "----------\n",
      "train Loss: -7835.4381 Acc: 0.3530\n",
      "val Loss: -7874.4447 Acc: 0.3524\n",
      "\n",
      "Epoch 242/299\n",
      "----------\n",
      "train Loss: -7868.8987 Acc: 0.3522\n",
      "val Loss: -7906.9765 Acc: 0.3524\n",
      "\n",
      "Epoch 243/299\n",
      "----------\n",
      "train Loss: -7897.3539 Acc: 0.3536\n",
      "val Loss: -7939.5524 Acc: 0.3520\n",
      "\n",
      "Epoch 244/299\n",
      "----------\n",
      "train Loss: -7931.6147 Acc: 0.3545\n",
      "val Loss: -7972.1467 Acc: 0.3520\n",
      "\n",
      "Epoch 245/299\n",
      "----------\n",
      "train Loss: -7963.6542 Acc: 0.3531\n",
      "val Loss: -8004.7040 Acc: 0.3524\n",
      "\n",
      "Epoch 246/299\n",
      "----------\n",
      "train Loss: -7997.2918 Acc: 0.3521\n",
      "val Loss: -8037.2774 Acc: 0.3537\n",
      "\n",
      "Epoch 247/299\n",
      "----------\n",
      "train Loss: -8029.1994 Acc: 0.3544\n",
      "val Loss: -8069.8560 Acc: 0.3537\n",
      "\n",
      "Epoch 248/299\n",
      "----------\n",
      "train Loss: -8062.9136 Acc: 0.3541\n",
      "val Loss: -8102.4288 Acc: 0.3537\n",
      "\n",
      "Epoch 249/299\n",
      "----------\n",
      "train Loss: -8093.3484 Acc: 0.3541\n",
      "val Loss: -8134.9454 Acc: 0.3537\n",
      "\n",
      "Epoch 250/299\n",
      "----------\n",
      "train Loss: -8128.3415 Acc: 0.3539\n",
      "val Loss: -8167.5093 Acc: 0.3537\n",
      "\n",
      "Epoch 251/299\n",
      "----------\n",
      "train Loss: -8161.0826 Acc: 0.3545\n",
      "val Loss: -8200.0635 Acc: 0.3537\n",
      "\n",
      "Epoch 252/299\n",
      "----------\n",
      "train Loss: -8190.3529 Acc: 0.3548\n",
      "val Loss: -8232.6375 Acc: 0.3537\n",
      "\n",
      "Epoch 253/299\n",
      "----------\n",
      "train Loss: -8222.8394 Acc: 0.3548\n",
      "val Loss: -8265.1882 Acc: 0.3545\n",
      "\n",
      "Epoch 254/299\n",
      "----------\n",
      "train Loss: -8256.4030 Acc: 0.3540\n",
      "val Loss: -8297.7357 Acc: 0.3549\n",
      "\n",
      "Epoch 255/299\n",
      "----------\n",
      "train Loss: -8287.6289 Acc: 0.3549\n",
      "val Loss: -8330.2899 Acc: 0.3549\n",
      "\n",
      "Epoch 256/299\n",
      "----------\n",
      "train Loss: -8321.3634 Acc: 0.3549\n",
      "val Loss: -8362.8388 Acc: 0.3545\n",
      "\n",
      "Epoch 257/299\n",
      "----------\n",
      "train Loss: -8354.8745 Acc: 0.3541\n",
      "val Loss: -8395.3499 Acc: 0.3545\n",
      "\n",
      "Epoch 258/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: -8386.2788 Acc: 0.3548\n",
      "val Loss: -8427.8291 Acc: 0.3545\n",
      "\n",
      "Epoch 259/299\n",
      "----------\n",
      "train Loss: -8418.0670 Acc: 0.3537\n",
      "val Loss: -8460.3291 Acc: 0.3545\n",
      "\n",
      "Epoch 260/299\n",
      "----------\n",
      "train Loss: -8450.1666 Acc: 0.3544\n",
      "val Loss: -8492.8545 Acc: 0.3545\n",
      "\n",
      "Epoch 261/299\n",
      "----------\n",
      "train Loss: -8485.0819 Acc: 0.3538\n",
      "val Loss: -8525.3720 Acc: 0.3545\n",
      "\n",
      "Epoch 262/299\n",
      "----------\n",
      "train Loss: -8517.1916 Acc: 0.3531\n",
      "val Loss: -8557.8737 Acc: 0.3545\n",
      "\n",
      "Epoch 263/299\n",
      "----------\n",
      "train Loss: -8549.4590 Acc: 0.3542\n",
      "val Loss: -8590.3924 Acc: 0.3541\n",
      "\n",
      "Epoch 264/299\n",
      "----------\n",
      "train Loss: -8582.0229 Acc: 0.3534\n",
      "val Loss: -8622.9058 Acc: 0.3545\n",
      "\n",
      "Epoch 265/299\n",
      "----------\n",
      "train Loss: -8613.5608 Acc: 0.3542\n",
      "val Loss: -8655.4343 Acc: 0.3545\n",
      "\n",
      "Epoch 266/299\n",
      "----------\n",
      "train Loss: -8645.5920 Acc: 0.3547\n",
      "val Loss: -8687.9449 Acc: 0.3545\n",
      "\n",
      "Epoch 267/299\n",
      "----------\n",
      "train Loss: -8679.1320 Acc: 0.3539\n",
      "val Loss: -8720.5191 Acc: 0.3545\n",
      "\n",
      "Epoch 268/299\n",
      "----------\n",
      "train Loss: -8709.8133 Acc: 0.3536\n",
      "val Loss: -8753.0925 Acc: 0.3545\n",
      "\n",
      "Epoch 269/299\n",
      "----------\n",
      "train Loss: -8743.6928 Acc: 0.3547\n",
      "val Loss: -8785.6854 Acc: 0.3537\n",
      "\n",
      "Epoch 270/299\n",
      "----------\n",
      "train Loss: -8776.1216 Acc: 0.3532\n",
      "val Loss: -8818.2064 Acc: 0.3545\n",
      "\n",
      "Epoch 271/299\n",
      "----------\n",
      "train Loss: -8805.6103 Acc: 0.3544\n",
      "val Loss: -8850.7010 Acc: 0.3545\n",
      "\n",
      "Epoch 272/299\n",
      "----------\n",
      "train Loss: -8842.6147 Acc: 0.3543\n",
      "val Loss: -8883.1837 Acc: 0.3541\n",
      "\n",
      "Epoch 273/299\n",
      "----------\n",
      "train Loss: -8873.5608 Acc: 0.3561\n",
      "val Loss: -8915.6922 Acc: 0.3541\n",
      "\n",
      "Epoch 274/299\n",
      "----------\n",
      "train Loss: -8904.7894 Acc: 0.3547\n",
      "val Loss: -8948.1758 Acc: 0.3541\n",
      "\n",
      "Epoch 275/299\n",
      "----------\n",
      "train Loss: -8939.0452 Acc: 0.3544\n",
      "val Loss: -8980.6827 Acc: 0.3541\n",
      "\n",
      "Epoch 276/299\n",
      "----------\n",
      "train Loss: -8972.6528 Acc: 0.3527\n",
      "val Loss: -9013.1932 Acc: 0.3545\n",
      "\n",
      "Epoch 277/299\n",
      "----------\n",
      "train Loss: -9003.7023 Acc: 0.3542\n",
      "val Loss: -9045.7033 Acc: 0.3545\n",
      "\n",
      "Epoch 278/299\n",
      "----------\n",
      "train Loss: -9036.1066 Acc: 0.3541\n",
      "val Loss: -9078.2265 Acc: 0.3545\n",
      "\n",
      "Epoch 279/299\n",
      "----------\n",
      "train Loss: -9066.1389 Acc: 0.3551\n",
      "val Loss: -9110.7362 Acc: 0.3549\n",
      "\n",
      "Epoch 280/299\n",
      "----------\n",
      "train Loss: -9099.9077 Acc: 0.3545\n",
      "val Loss: -9143.2149 Acc: 0.3545\n",
      "\n",
      "Epoch 281/299\n",
      "----------\n",
      "train Loss: -9132.3882 Acc: 0.3553\n",
      "val Loss: -9175.7192 Acc: 0.3545\n",
      "\n",
      "Epoch 282/299\n",
      "----------\n",
      "train Loss: -9164.0078 Acc: 0.3555\n",
      "val Loss: -9208.2230 Acc: 0.3545\n",
      "\n",
      "Epoch 283/299\n",
      "----------\n",
      "train Loss: -9197.4684 Acc: 0.3556\n",
      "val Loss: -9240.7095 Acc: 0.3545\n",
      "\n",
      "Epoch 284/299\n",
      "----------\n",
      "train Loss: -9229.3136 Acc: 0.3551\n",
      "val Loss: -9273.1776 Acc: 0.3549\n",
      "\n",
      "Epoch 285/299\n",
      "----------\n",
      "train Loss: -9260.8963 Acc: 0.3558\n",
      "val Loss: -9305.6720 Acc: 0.3545\n",
      "\n",
      "Epoch 286/299\n",
      "----------\n",
      "train Loss: -9297.2614 Acc: 0.3537\n",
      "val Loss: -9338.1660 Acc: 0.3549\n",
      "\n",
      "Epoch 287/299\n",
      "----------\n",
      "train Loss: -9325.3405 Acc: 0.3554\n",
      "val Loss: -9370.6300 Acc: 0.3553\n",
      "\n",
      "Epoch 288/299\n",
      "----------\n",
      "train Loss: -9359.0359 Acc: 0.3558\n",
      "val Loss: -9403.1253 Acc: 0.3549\n",
      "\n",
      "Epoch 289/299\n",
      "----------\n",
      "train Loss: -9392.0102 Acc: 0.3536\n",
      "val Loss: -9435.6122 Acc: 0.3549\n",
      "\n",
      "Epoch 290/299\n",
      "----------\n",
      "train Loss: -9423.8213 Acc: 0.3542\n",
      "val Loss: -9468.1567 Acc: 0.3549\n",
      "\n",
      "Epoch 291/299\n",
      "----------\n",
      "train Loss: -9457.8418 Acc: 0.3552\n",
      "val Loss: -9500.7024 Acc: 0.3549\n",
      "\n",
      "Epoch 292/299\n",
      "----------\n",
      "train Loss: -9487.7959 Acc: 0.3552\n",
      "val Loss: -9533.1777 Acc: 0.3549\n",
      "\n",
      "Epoch 293/299\n",
      "----------\n",
      "train Loss: -9519.2483 Acc: 0.3539\n",
      "val Loss: -9565.6616 Acc: 0.3549\n",
      "\n",
      "Epoch 294/299\n",
      "----------\n",
      "train Loss: -9554.3231 Acc: 0.3548\n",
      "val Loss: -9598.1612 Acc: 0.3549\n",
      "\n",
      "Epoch 295/299\n",
      "----------\n",
      "train Loss: -9588.3871 Acc: 0.3545\n",
      "val Loss: -9630.6705 Acc: 0.3553\n",
      "\n",
      "Epoch 296/299\n",
      "----------\n",
      "train Loss: -9618.4661 Acc: 0.3549\n",
      "val Loss: -9663.1685 Acc: 0.3553\n",
      "\n",
      "Epoch 297/299\n",
      "----------\n",
      "train Loss: -9651.4499 Acc: 0.3543\n",
      "val Loss: -9695.7218 Acc: 0.3549\n",
      "\n",
      "Epoch 298/299\n",
      "----------\n",
      "train Loss: -9684.8100 Acc: 0.3543\n",
      "val Loss: -9728.2656 Acc: 0.3549\n",
      "\n",
      "Epoch 299/299\n",
      "----------\n",
      "train Loss: -9714.4521 Acc: 0.3545\n",
      "val Loss: -9760.8051 Acc: 0.3545\n",
      "\n",
      "\n",
      "##############################\n",
      "------ Summary ------\n",
      "model -> squeezenet\n",
      "epochs -> 300\n",
      "lr -> 0.0001\n",
      "batch size -> 8\n",
      "optimizer -> ADAM\n",
      "criteriun -> NLLLoss\n",
      "\n",
      "Training complete in 152m 39s\n",
      "Best val Acc: 0.496887\n",
      "##############################\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== END ====\n",
      "[[  2 322 467]\n",
      " [  0 642 157]\n",
      " [  1 265 553]]\n",
      "\n",
      "Sensitivity or recall total\n",
      "0.49688667496886674\n",
      "\n",
      "Sensitivity or recall per classes\n",
      "[0.   0.8  0.68]\n",
      "\n",
      "Precision\n",
      "[0.67 0.52 0.47]\n",
      "\n",
      "F1 Score\n",
      "[0.01 0.63 0.55]\n",
      "Confusion matrix, without normalization\n",
      "\n",
      "============================================================\n",
      "==== INITIALIZING WITH PARAMETERS: ====\n",
      "model -> squeezenet\n",
      "epochs -> 300\n",
      "lr -> 0.0001\n",
      "batch size -> 8\n",
      "optimizer -> 3\n",
      "criteriun -> 1\n",
      "\n",
      "--------------------\n",
      "Params to learn:\n",
      "\t classifier.1.weight\n",
      "\t classifier.1.bias\n",
      "\n",
      "--------------------\n",
      "\n",
      "== Epochs ==\n",
      "Epoch 0/299\n",
      "----------\n",
      "train Loss: 0.5666 Acc: 0.8316\n",
      "val Loss: 0.3645 Acc: 0.9070\n",
      "\n",
      "Epoch 1/299\n",
      "----------\n",
      "train Loss: 0.3305 Acc: 0.9101\n",
      "val Loss: 0.2604 Acc: 0.9328\n",
      "\n",
      "Epoch 2/299\n",
      "----------\n",
      "train Loss: 0.2640 Acc: 0.9248\n",
      "val Loss: 0.2165 Acc: 0.9431\n",
      "\n",
      "Epoch 3/299\n",
      "----------\n",
      "train Loss: 0.2263 Acc: 0.9357\n",
      "val Loss: 0.1920 Acc: 0.9494\n",
      "\n",
      "Epoch 4/299\n",
      "----------\n",
      "train Loss: 0.2050 Acc: 0.9376\n",
      "val Loss: 0.1764 Acc: 0.9527\n",
      "\n",
      "Epoch 5/299\n",
      "----------\n",
      "train Loss: 0.1906 Acc: 0.9423\n",
      "val Loss: 0.1619 Acc: 0.9564\n",
      "\n",
      "Epoch 6/299\n",
      "----------\n",
      "train Loss: 0.1795 Acc: 0.9454\n",
      "val Loss: 0.1545 Acc: 0.9572\n",
      "\n",
      "Epoch 7/299\n",
      "----------\n",
      "train Loss: 0.1719 Acc: 0.9473\n",
      "val Loss: 0.1468 Acc: 0.9606\n",
      "\n",
      "Epoch 8/299\n",
      "----------\n",
      "train Loss: 0.1642 Acc: 0.9500\n",
      "val Loss: 0.1436 Acc: 0.9581\n",
      "\n",
      "Epoch 9/299\n",
      "----------\n",
      "train Loss: 0.1579 Acc: 0.9516\n",
      "val Loss: 0.1348 Acc: 0.9606\n",
      "\n",
      "Epoch 10/299\n",
      "----------\n",
      "train Loss: 0.1517 Acc: 0.9529\n",
      "val Loss: 0.1320 Acc: 0.9610\n",
      "\n",
      "Epoch 11/299\n",
      "----------\n",
      "train Loss: 0.1471 Acc: 0.9525\n",
      "val Loss: 0.1261 Acc: 0.9622\n",
      "\n",
      "Epoch 12/299\n",
      "----------\n",
      "train Loss: 0.1446 Acc: 0.9535\n",
      "val Loss: 0.1233 Acc: 0.9622\n",
      "\n",
      "Epoch 13/299\n",
      "----------\n",
      "train Loss: 0.1401 Acc: 0.9572\n",
      "val Loss: 0.1200 Acc: 0.9626\n",
      "\n",
      "Epoch 14/299\n",
      "----------\n",
      "train Loss: 0.1389 Acc: 0.9565\n",
      "val Loss: 0.1178 Acc: 0.9635\n",
      "\n",
      "Epoch 15/299\n",
      "----------\n",
      "train Loss: 0.1349 Acc: 0.9580\n",
      "val Loss: 0.1167 Acc: 0.9639\n",
      "\n",
      "Epoch 16/299\n",
      "----------\n",
      "train Loss: 0.1326 Acc: 0.9585\n",
      "val Loss: 0.1155 Acc: 0.9635\n",
      "\n",
      "Epoch 17/299\n",
      "----------\n",
      "train Loss: 0.1302 Acc: 0.9601\n",
      "val Loss: 0.1106 Acc: 0.9664\n",
      "\n",
      "Epoch 18/299\n",
      "----------\n",
      "train Loss: 0.1283 Acc: 0.9580\n",
      "val Loss: 0.1095 Acc: 0.9626\n",
      "\n",
      "Epoch 19/299\n",
      "----------\n",
      "train Loss: 0.1260 Acc: 0.9601\n",
      "val Loss: 0.1080 Acc: 0.9655\n",
      "\n",
      "Epoch 20/299\n",
      "----------\n",
      "train Loss: 0.1226 Acc: 0.9607\n",
      "val Loss: 0.1101 Acc: 0.9647\n",
      "\n",
      "Epoch 21/299\n",
      "----------\n",
      "train Loss: 0.1223 Acc: 0.9592\n",
      "val Loss: 0.1048 Acc: 0.9685\n",
      "\n",
      "Epoch 22/299\n",
      "----------\n",
      "train Loss: 0.1220 Acc: 0.9595\n",
      "val Loss: 0.1031 Acc: 0.9651\n",
      "\n",
      "Epoch 23/299\n",
      "----------\n",
      "train Loss: 0.1194 Acc: 0.9620\n",
      "val Loss: 0.1024 Acc: 0.9676\n",
      "\n",
      "Epoch 24/299\n",
      "----------\n",
      "train Loss: 0.1179 Acc: 0.9622\n",
      "val Loss: 0.1007 Acc: 0.9660\n",
      "\n",
      "Epoch 25/299\n",
      "----------\n",
      "train Loss: 0.1165 Acc: 0.9627\n",
      "val Loss: 0.1004 Acc: 0.9705\n",
      "\n",
      "Epoch 26/299\n",
      "----------\n",
      "train Loss: 0.1163 Acc: 0.9614\n",
      "val Loss: 0.0987 Acc: 0.9660\n",
      "\n",
      "Epoch 27/299\n",
      "----------\n",
      "train Loss: 0.1155 Acc: 0.9625\n",
      "val Loss: 0.0983 Acc: 0.9701\n",
      "\n",
      "Epoch 28/299\n",
      "----------\n",
      "train Loss: 0.1137 Acc: 0.9643\n",
      "val Loss: 0.0959 Acc: 0.9685\n",
      "\n",
      "Epoch 29/299\n",
      "----------\n",
      "train Loss: 0.1150 Acc: 0.9637\n",
      "val Loss: 0.0992 Acc: 0.9689\n",
      "\n",
      "Epoch 30/299\n",
      "----------\n",
      "train Loss: 0.1126 Acc: 0.9630\n",
      "val Loss: 0.0938 Acc: 0.9693\n",
      "\n",
      "Epoch 31/299\n",
      "----------\n",
      "train Loss: 0.1113 Acc: 0.9639\n",
      "val Loss: 0.0934 Acc: 0.9680\n",
      "\n",
      "Epoch 32/299\n",
      "----------\n",
      "train Loss: 0.1109 Acc: 0.9625\n",
      "val Loss: 0.0924 Acc: 0.9693\n",
      "\n",
      "Epoch 33/299\n",
      "----------\n",
      "train Loss: 0.1075 Acc: 0.9642\n",
      "val Loss: 0.0932 Acc: 0.9685\n",
      "\n",
      "Epoch 34/299\n",
      "----------\n",
      "train Loss: 0.1065 Acc: 0.9646\n",
      "val Loss: 0.0969 Acc: 0.9709\n",
      "\n",
      "Epoch 35/299\n",
      "----------\n",
      "train Loss: 0.1090 Acc: 0.9630\n",
      "val Loss: 0.0905 Acc: 0.9722\n",
      "\n",
      "Epoch 36/299\n",
      "----------\n",
      "train Loss: 0.1064 Acc: 0.9651\n",
      "val Loss: 0.0898 Acc: 0.9714\n",
      "\n",
      "Epoch 37/299\n",
      "----------\n",
      "train Loss: 0.1044 Acc: 0.9648\n",
      "val Loss: 0.0905 Acc: 0.9689\n",
      "\n",
      "Epoch 38/299\n",
      "----------\n",
      "train Loss: 0.1049 Acc: 0.9637\n",
      "val Loss: 0.0888 Acc: 0.9734\n",
      "\n",
      "Epoch 39/299\n",
      "----------\n",
      "train Loss: 0.1047 Acc: 0.9646\n",
      "val Loss: 0.0872 Acc: 0.9722\n",
      "\n",
      "Epoch 40/299\n",
      "----------\n",
      "train Loss: 0.1034 Acc: 0.9666\n",
      "val Loss: 0.0866 Acc: 0.9718\n",
      "\n",
      "Epoch 41/299\n",
      "----------\n",
      "train Loss: 0.1033 Acc: 0.9652\n",
      "val Loss: 0.0879 Acc: 0.9738\n",
      "\n",
      "Epoch 42/299\n",
      "----------\n",
      "train Loss: 0.1010 Acc: 0.9662\n",
      "val Loss: 0.0857 Acc: 0.9726\n",
      "\n",
      "Epoch 43/299\n",
      "----------\n",
      "train Loss: 0.1023 Acc: 0.9674\n",
      "val Loss: 0.0857 Acc: 0.9726\n",
      "\n",
      "Epoch 44/299\n",
      "----------\n",
      "train Loss: 0.0999 Acc: 0.9679\n",
      "val Loss: 0.0869 Acc: 0.9743\n",
      "\n",
      "Epoch 45/299\n",
      "----------\n",
      "train Loss: 0.0993 Acc: 0.9670\n",
      "val Loss: 0.0855 Acc: 0.9705\n",
      "\n",
      "Epoch 46/299\n",
      "----------\n",
      "train Loss: 0.1013 Acc: 0.9668\n",
      "val Loss: 0.0867 Acc: 0.9705\n",
      "\n",
      "Epoch 47/299\n",
      "----------\n",
      "train Loss: 0.0989 Acc: 0.9673\n",
      "val Loss: 0.0834 Acc: 0.9734\n",
      "\n",
      "Epoch 48/299\n",
      "----------\n",
      "train Loss: 0.0984 Acc: 0.9681\n",
      "val Loss: 0.0829 Acc: 0.9734\n",
      "\n",
      "Epoch 49/299\n",
      "----------\n",
      "train Loss: 0.1015 Acc: 0.9662\n",
      "val Loss: 0.0840 Acc: 0.9714\n",
      "\n",
      "Epoch 50/299\n",
      "----------\n",
      "train Loss: 0.0942 Acc: 0.9714\n",
      "val Loss: 0.0820 Acc: 0.9743\n",
      "\n",
      "Epoch 51/299\n",
      "----------\n",
      "train Loss: 0.0989 Acc: 0.9670\n",
      "val Loss: 0.0851 Acc: 0.9734\n",
      "\n",
      "Epoch 52/299\n",
      "----------\n",
      "train Loss: 0.0968 Acc: 0.9668\n",
      "val Loss: 0.0814 Acc: 0.9751\n",
      "\n",
      "Epoch 53/299\n",
      "----------\n",
      "train Loss: 0.0947 Acc: 0.9682\n",
      "val Loss: 0.0816 Acc: 0.9755\n",
      "\n",
      "Epoch 54/299\n",
      "----------\n",
      "train Loss: 0.0957 Acc: 0.9693\n",
      "val Loss: 0.0823 Acc: 0.9759\n",
      "\n",
      "Epoch 55/299\n",
      "----------\n",
      "train Loss: 0.0971 Acc: 0.9668\n",
      "val Loss: 0.0798 Acc: 0.9755\n",
      "\n",
      "Epoch 56/299\n",
      "----------\n",
      "train Loss: 0.0970 Acc: 0.9692\n",
      "val Loss: 0.0800 Acc: 0.9755\n",
      "\n",
      "Epoch 57/299\n",
      "----------\n",
      "train Loss: 0.0954 Acc: 0.9676\n",
      "val Loss: 0.0827 Acc: 0.9759\n",
      "\n",
      "Epoch 58/299\n",
      "----------\n",
      "train Loss: 0.0963 Acc: 0.9692\n",
      "val Loss: 0.0796 Acc: 0.9755\n",
      "\n",
      "Epoch 59/299\n",
      "----------\n",
      "train Loss: 0.0938 Acc: 0.9690\n",
      "val Loss: 0.0791 Acc: 0.9755\n",
      "\n",
      "Epoch 60/299\n",
      "----------\n",
      "train Loss: 0.0943 Acc: 0.9682\n",
      "val Loss: 0.0789 Acc: 0.9738\n",
      "\n",
      "Epoch 61/299\n",
      "----------\n",
      "train Loss: 0.0955 Acc: 0.9688\n",
      "val Loss: 0.0784 Acc: 0.9768\n",
      "\n",
      "Epoch 62/299\n",
      "----------\n",
      "train Loss: 0.0936 Acc: 0.9720\n",
      "val Loss: 0.0782 Acc: 0.9759\n",
      "\n",
      "Epoch 63/299\n",
      "----------\n",
      "train Loss: 0.0938 Acc: 0.9673\n",
      "val Loss: 0.0776 Acc: 0.9763\n",
      "\n",
      "Epoch 64/299\n",
      "----------\n",
      "train Loss: 0.0932 Acc: 0.9680\n",
      "val Loss: 0.0778 Acc: 0.9755\n",
      "\n",
      "Epoch 65/299\n",
      "----------\n",
      "train Loss: 0.0947 Acc: 0.9678\n",
      "val Loss: 0.0785 Acc: 0.9780\n",
      "\n",
      "Epoch 66/299\n",
      "----------\n",
      "train Loss: 0.0944 Acc: 0.9676\n",
      "val Loss: 0.0773 Acc: 0.9763\n",
      "\n",
      "Epoch 67/299\n",
      "----------\n",
      "train Loss: 0.0932 Acc: 0.9676\n",
      "val Loss: 0.0770 Acc: 0.9759\n",
      "\n",
      "Epoch 68/299\n",
      "----------\n",
      "train Loss: 0.0911 Acc: 0.9701\n",
      "val Loss: 0.0765 Acc: 0.9759\n",
      "\n",
      "Epoch 69/299\n",
      "----------\n",
      "train Loss: 0.0909 Acc: 0.9680\n",
      "val Loss: 0.0766 Acc: 0.9768\n",
      "\n",
      "Epoch 70/299\n",
      "----------\n",
      "train Loss: 0.0908 Acc: 0.9686\n",
      "val Loss: 0.0785 Acc: 0.9726\n",
      "\n",
      "Epoch 71/299\n",
      "----------\n",
      "train Loss: 0.0904 Acc: 0.9702\n",
      "val Loss: 0.0758 Acc: 0.9768\n",
      "\n",
      "Epoch 72/299\n",
      "----------\n",
      "train Loss: 0.0891 Acc: 0.9704\n",
      "val Loss: 0.0764 Acc: 0.9755\n",
      "\n",
      "Epoch 73/299\n",
      "----------\n",
      "train Loss: 0.0908 Acc: 0.9683\n",
      "val Loss: 0.0796 Acc: 0.9722\n",
      "\n",
      "Epoch 74/299\n",
      "----------\n",
      "train Loss: 0.0902 Acc: 0.9698\n",
      "val Loss: 0.0752 Acc: 0.9768\n",
      "\n",
      "Epoch 75/299\n",
      "----------\n",
      "train Loss: 0.0898 Acc: 0.9679\n",
      "val Loss: 0.0748 Acc: 0.9772\n",
      "\n",
      "Epoch 76/299\n",
      "----------\n",
      "train Loss: 0.0898 Acc: 0.9697\n",
      "val Loss: 0.0746 Acc: 0.9772\n",
      "\n",
      "Epoch 77/299\n",
      "----------\n",
      "train Loss: 0.0923 Acc: 0.9690\n",
      "val Loss: 0.0774 Acc: 0.9747\n",
      "\n",
      "Epoch 78/299\n",
      "----------\n",
      "train Loss: 0.0868 Acc: 0.9708\n",
      "val Loss: 0.0747 Acc: 0.9759\n",
      "\n",
      "Epoch 79/299\n",
      "----------\n",
      "train Loss: 0.0895 Acc: 0.9691\n",
      "val Loss: 0.0740 Acc: 0.9768\n",
      "\n",
      "Epoch 80/299\n",
      "----------\n",
      "train Loss: 0.0880 Acc: 0.9708\n",
      "val Loss: 0.0738 Acc: 0.9763\n",
      "\n",
      "Epoch 81/299\n",
      "----------\n",
      "train Loss: 0.0925 Acc: 0.9681\n",
      "val Loss: 0.0737 Acc: 0.9759\n",
      "\n",
      "Epoch 82/299\n",
      "----------\n",
      "train Loss: 0.0881 Acc: 0.9703\n",
      "val Loss: 0.0769 Acc: 0.9776\n",
      "\n",
      "Epoch 83/299\n",
      "----------\n",
      "train Loss: 0.0880 Acc: 0.9720\n",
      "val Loss: 0.0742 Acc: 0.9772\n",
      "\n",
      "Epoch 84/299\n",
      "----------\n",
      "train Loss: 0.0888 Acc: 0.9699\n",
      "val Loss: 0.0773 Acc: 0.9784\n",
      "\n",
      "Epoch 85/299\n",
      "----------\n",
      "train Loss: 0.0916 Acc: 0.9675\n",
      "val Loss: 0.0738 Acc: 0.9763\n",
      "\n",
      "Epoch 86/299\n",
      "----------\n",
      "train Loss: 0.0882 Acc: 0.9689\n",
      "val Loss: 0.0741 Acc: 0.9763\n",
      "\n",
      "Epoch 87/299\n",
      "----------\n",
      "train Loss: 0.0877 Acc: 0.9694\n",
      "val Loss: 0.0743 Acc: 0.9780\n",
      "\n",
      "Epoch 88/299\n",
      "----------\n",
      "train Loss: 0.0870 Acc: 0.9697\n",
      "val Loss: 0.0735 Acc: 0.9763\n",
      "\n",
      "Epoch 89/299\n",
      "----------\n",
      "train Loss: 0.0851 Acc: 0.9700\n",
      "val Loss: 0.0732 Acc: 0.9788\n",
      "\n",
      "Epoch 90/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0848 Acc: 0.9719\n",
      "val Loss: 0.0760 Acc: 0.9780\n",
      "\n",
      "Epoch 91/299\n",
      "----------\n",
      "train Loss: 0.0887 Acc: 0.9703\n",
      "val Loss: 0.0722 Acc: 0.9772\n",
      "\n",
      "Epoch 92/299\n",
      "----------\n",
      "train Loss: 0.0856 Acc: 0.9700\n",
      "val Loss: 0.0729 Acc: 0.9763\n",
      "\n",
      "Epoch 93/299\n",
      "----------\n",
      "train Loss: 0.0875 Acc: 0.9709\n",
      "val Loss: 0.0722 Acc: 0.9772\n",
      "\n",
      "Epoch 94/299\n",
      "----------\n",
      "train Loss: 0.0864 Acc: 0.9699\n",
      "val Loss: 0.0718 Acc: 0.9768\n",
      "\n",
      "Epoch 95/299\n",
      "----------\n",
      "train Loss: 0.0871 Acc: 0.9703\n",
      "val Loss: 0.0721 Acc: 0.9776\n",
      "\n",
      "Epoch 96/299\n",
      "----------\n",
      "train Loss: 0.0855 Acc: 0.9721\n",
      "val Loss: 0.0731 Acc: 0.9776\n",
      "\n",
      "Epoch 97/299\n",
      "----------\n",
      "train Loss: 0.0842 Acc: 0.9710\n",
      "val Loss: 0.0715 Acc: 0.9772\n",
      "\n",
      "Epoch 98/299\n",
      "----------\n",
      "train Loss: 0.0853 Acc: 0.9705\n",
      "val Loss: 0.0718 Acc: 0.9788\n",
      "\n",
      "Epoch 99/299\n",
      "----------\n",
      "train Loss: 0.0849 Acc: 0.9705\n",
      "val Loss: 0.0725 Acc: 0.9768\n",
      "\n",
      "Epoch 100/299\n",
      "----------\n",
      "train Loss: 0.0855 Acc: 0.9712\n",
      "val Loss: 0.0724 Acc: 0.9772\n",
      "\n",
      "Epoch 101/299\n",
      "----------\n",
      "train Loss: 0.0842 Acc: 0.9722\n",
      "val Loss: 0.0725 Acc: 0.9763\n",
      "\n",
      "Epoch 102/299\n",
      "----------\n",
      "train Loss: 0.0881 Acc: 0.9678\n",
      "val Loss: 0.0712 Acc: 0.9780\n",
      "\n",
      "Epoch 103/299\n",
      "----------\n",
      "train Loss: 0.0827 Acc: 0.9705\n",
      "val Loss: 0.0720 Acc: 0.9772\n",
      "\n",
      "Epoch 104/299\n",
      "----------\n",
      "train Loss: 0.0829 Acc: 0.9713\n",
      "val Loss: 0.0753 Acc: 0.9768\n",
      "\n",
      "Epoch 105/299\n",
      "----------\n",
      "train Loss: 0.0838 Acc: 0.9716\n",
      "val Loss: 0.0735 Acc: 0.9768\n",
      "\n",
      "Epoch 106/299\n",
      "----------\n",
      "train Loss: 0.0841 Acc: 0.9708\n",
      "val Loss: 0.0703 Acc: 0.9776\n",
      "\n",
      "Epoch 107/299\n",
      "----------\n",
      "train Loss: 0.0835 Acc: 0.9713\n",
      "val Loss: 0.0715 Acc: 0.9772\n",
      "\n",
      "Epoch 108/299\n",
      "----------\n",
      "train Loss: 0.0833 Acc: 0.9706\n",
      "val Loss: 0.0714 Acc: 0.9792\n",
      "\n",
      "Epoch 109/299\n",
      "----------\n",
      "train Loss: 0.0850 Acc: 0.9697\n",
      "val Loss: 0.0701 Acc: 0.9792\n",
      "\n",
      "Epoch 110/299\n",
      "----------\n",
      "train Loss: 0.0852 Acc: 0.9705\n",
      "val Loss: 0.0701 Acc: 0.9768\n",
      "\n",
      "Epoch 111/299\n",
      "----------\n",
      "train Loss: 0.0833 Acc: 0.9697\n",
      "val Loss: 0.0710 Acc: 0.9776\n",
      "\n",
      "Epoch 112/299\n",
      "----------\n",
      "train Loss: 0.0808 Acc: 0.9738\n",
      "val Loss: 0.0701 Acc: 0.9776\n",
      "\n",
      "Epoch 113/299\n",
      "----------\n",
      "train Loss: 0.0854 Acc: 0.9705\n",
      "val Loss: 0.0697 Acc: 0.9780\n",
      "\n",
      "Epoch 114/299\n",
      "----------\n",
      "train Loss: 0.0840 Acc: 0.9717\n",
      "val Loss: 0.0709 Acc: 0.9801\n",
      "\n",
      "Epoch 115/299\n",
      "----------\n",
      "train Loss: 0.0836 Acc: 0.9701\n",
      "val Loss: 0.0692 Acc: 0.9768\n",
      "\n",
      "Epoch 116/299\n",
      "----------\n",
      "train Loss: 0.0828 Acc: 0.9712\n",
      "val Loss: 0.0692 Acc: 0.9772\n",
      "\n",
      "Epoch 117/299\n",
      "----------\n",
      "train Loss: 0.0825 Acc: 0.9727\n",
      "val Loss: 0.0703 Acc: 0.9768\n",
      "\n",
      "Epoch 118/299\n",
      "----------\n",
      "train Loss: 0.0856 Acc: 0.9692\n",
      "val Loss: 0.0687 Acc: 0.9788\n",
      "\n",
      "Epoch 119/299\n",
      "----------\n",
      "train Loss: 0.0818 Acc: 0.9722\n",
      "val Loss: 0.0713 Acc: 0.9784\n",
      "\n",
      "Epoch 120/299\n",
      "----------\n",
      "train Loss: 0.0823 Acc: 0.9711\n",
      "val Loss: 0.0688 Acc: 0.9801\n",
      "\n",
      "Epoch 121/299\n",
      "----------\n",
      "train Loss: 0.0812 Acc: 0.9704\n",
      "val Loss: 0.0690 Acc: 0.9792\n",
      "\n",
      "Epoch 122/299\n",
      "----------\n",
      "train Loss: 0.0852 Acc: 0.9708\n",
      "val Loss: 0.0712 Acc: 0.9792\n",
      "\n",
      "Epoch 123/299\n",
      "----------\n",
      "train Loss: 0.0810 Acc: 0.9725\n",
      "val Loss: 0.0702 Acc: 0.9801\n",
      "\n",
      "Epoch 124/299\n",
      "----------\n",
      "train Loss: 0.0821 Acc: 0.9720\n",
      "val Loss: 0.0684 Acc: 0.9788\n",
      "\n",
      "Epoch 125/299\n",
      "----------\n",
      "train Loss: 0.0800 Acc: 0.9724\n",
      "val Loss: 0.0706 Acc: 0.9763\n",
      "\n",
      "Epoch 126/299\n",
      "----------\n",
      "train Loss: 0.0824 Acc: 0.9707\n",
      "val Loss: 0.0693 Acc: 0.9797\n",
      "\n",
      "Epoch 127/299\n",
      "----------\n",
      "train Loss: 0.0835 Acc: 0.9702\n",
      "val Loss: 0.0692 Acc: 0.9784\n",
      "\n",
      "Epoch 128/299\n",
      "----------\n",
      "train Loss: 0.0857 Acc: 0.9698\n",
      "val Loss: 0.0688 Acc: 0.9805\n",
      "\n",
      "Epoch 129/299\n",
      "----------\n",
      "train Loss: 0.0829 Acc: 0.9722\n",
      "val Loss: 0.0686 Acc: 0.9784\n",
      "\n",
      "Epoch 130/299\n",
      "----------\n",
      "train Loss: 0.0822 Acc: 0.9705\n",
      "val Loss: 0.0686 Acc: 0.9792\n",
      "\n",
      "Epoch 131/299\n",
      "----------\n",
      "train Loss: 0.0797 Acc: 0.9711\n",
      "val Loss: 0.0681 Acc: 0.9780\n",
      "\n",
      "Epoch 132/299\n",
      "----------\n",
      "train Loss: 0.0830 Acc: 0.9721\n",
      "val Loss: 0.0679 Acc: 0.9784\n",
      "\n",
      "Epoch 133/299\n",
      "----------\n",
      "train Loss: 0.0824 Acc: 0.9721\n",
      "val Loss: 0.0678 Acc: 0.9784\n",
      "\n",
      "Epoch 134/299\n",
      "----------\n",
      "train Loss: 0.0816 Acc: 0.9722\n",
      "val Loss: 0.0682 Acc: 0.9788\n",
      "\n",
      "Epoch 135/299\n",
      "----------\n",
      "train Loss: 0.0789 Acc: 0.9726\n",
      "val Loss: 0.0679 Acc: 0.9792\n",
      "\n",
      "Epoch 136/299\n",
      "----------\n",
      "train Loss: 0.0836 Acc: 0.9709\n",
      "val Loss: 0.0688 Acc: 0.9780\n",
      "\n",
      "Epoch 137/299\n",
      "----------\n",
      "train Loss: 0.0820 Acc: 0.9733\n",
      "val Loss: 0.0685 Acc: 0.9809\n",
      "\n",
      "Epoch 138/299\n",
      "----------\n",
      "train Loss: 0.0817 Acc: 0.9710\n",
      "val Loss: 0.0689 Acc: 0.9809\n",
      "\n",
      "Epoch 139/299\n",
      "----------\n",
      "train Loss: 0.0821 Acc: 0.9710\n",
      "val Loss: 0.0673 Acc: 0.9784\n",
      "\n",
      "Epoch 140/299\n",
      "----------\n",
      "train Loss: 0.0836 Acc: 0.9703\n",
      "val Loss: 0.0677 Acc: 0.9788\n",
      "\n",
      "Epoch 141/299\n",
      "----------\n",
      "train Loss: 0.0815 Acc: 0.9717\n",
      "val Loss: 0.0702 Acc: 0.9784\n",
      "\n",
      "Epoch 142/299\n",
      "----------\n",
      "train Loss: 0.0788 Acc: 0.9730\n",
      "val Loss: 0.0678 Acc: 0.9805\n",
      "\n",
      "Epoch 143/299\n",
      "----------\n",
      "train Loss: 0.0799 Acc: 0.9726\n",
      "val Loss: 0.0669 Acc: 0.9792\n",
      "\n",
      "Epoch 144/299\n",
      "----------\n",
      "train Loss: 0.0807 Acc: 0.9726\n",
      "val Loss: 0.0681 Acc: 0.9776\n",
      "\n",
      "Epoch 145/299\n",
      "----------\n",
      "train Loss: 0.0788 Acc: 0.9720\n",
      "val Loss: 0.0674 Acc: 0.9792\n",
      "\n",
      "Epoch 146/299\n",
      "----------\n",
      "train Loss: 0.0798 Acc: 0.9708\n",
      "val Loss: 0.0670 Acc: 0.9788\n",
      "\n",
      "Epoch 147/299\n",
      "----------\n",
      "train Loss: 0.0817 Acc: 0.9703\n",
      "val Loss: 0.0675 Acc: 0.9813\n",
      "\n",
      "Epoch 148/299\n",
      "----------\n",
      "train Loss: 0.0804 Acc: 0.9726\n",
      "val Loss: 0.0669 Acc: 0.9801\n",
      "\n",
      "Epoch 149/299\n",
      "----------\n",
      "train Loss: 0.0813 Acc: 0.9711\n",
      "val Loss: 0.0669 Acc: 0.9809\n",
      "\n",
      "Epoch 150/299\n",
      "----------\n",
      "train Loss: 0.0804 Acc: 0.9732\n",
      "val Loss: 0.0664 Acc: 0.9788\n",
      "\n",
      "Epoch 151/299\n",
      "----------\n",
      "train Loss: 0.0802 Acc: 0.9723\n",
      "val Loss: 0.0661 Acc: 0.9792\n",
      "\n",
      "Epoch 152/299\n",
      "----------\n",
      "train Loss: 0.0804 Acc: 0.9729\n",
      "val Loss: 0.0676 Acc: 0.9809\n",
      "\n",
      "Epoch 153/299\n",
      "----------\n",
      "train Loss: 0.0846 Acc: 0.9708\n",
      "val Loss: 0.0662 Acc: 0.9780\n",
      "\n",
      "Epoch 154/299\n",
      "----------\n",
      "train Loss: 0.0778 Acc: 0.9727\n",
      "val Loss: 0.0664 Acc: 0.9788\n",
      "\n",
      "Epoch 155/299\n",
      "----------\n",
      "train Loss: 0.0786 Acc: 0.9732\n",
      "val Loss: 0.0702 Acc: 0.9763\n",
      "\n",
      "Epoch 156/299\n",
      "----------\n",
      "train Loss: 0.0793 Acc: 0.9708\n",
      "val Loss: 0.0660 Acc: 0.9788\n",
      "\n",
      "Epoch 157/299\n",
      "----------\n",
      "train Loss: 0.0817 Acc: 0.9703\n",
      "val Loss: 0.0675 Acc: 0.9809\n",
      "\n",
      "Epoch 158/299\n",
      "----------\n",
      "train Loss: 0.0791 Acc: 0.9719\n",
      "val Loss: 0.0666 Acc: 0.9792\n",
      "\n",
      "Epoch 159/299\n",
      "----------\n",
      "train Loss: 0.0790 Acc: 0.9732\n",
      "val Loss: 0.0662 Acc: 0.9809\n",
      "\n",
      "Epoch 160/299\n",
      "----------\n",
      "train Loss: 0.0789 Acc: 0.9724\n",
      "val Loss: 0.0663 Acc: 0.9784\n",
      "\n",
      "Epoch 161/299\n",
      "----------\n",
      "train Loss: 0.0776 Acc: 0.9733\n",
      "val Loss: 0.0659 Acc: 0.9801\n",
      "\n",
      "Epoch 162/299\n",
      "----------\n",
      "train Loss: 0.0795 Acc: 0.9707\n",
      "val Loss: 0.0671 Acc: 0.9809\n",
      "\n",
      "Epoch 163/299\n",
      "----------\n",
      "train Loss: 0.0803 Acc: 0.9723\n",
      "val Loss: 0.0668 Acc: 0.9788\n",
      "\n",
      "Epoch 164/299\n",
      "----------\n",
      "train Loss: 0.0809 Acc: 0.9730\n",
      "val Loss: 0.0663 Acc: 0.9801\n",
      "\n",
      "Epoch 165/299\n",
      "----------\n",
      "train Loss: 0.0793 Acc: 0.9720\n",
      "val Loss: 0.0661 Acc: 0.9817\n",
      "\n",
      "Epoch 166/299\n",
      "----------\n",
      "train Loss: 0.0776 Acc: 0.9749\n",
      "val Loss: 0.0655 Acc: 0.9797\n",
      "\n",
      "Epoch 167/299\n",
      "----------\n",
      "train Loss: 0.0776 Acc: 0.9744\n",
      "val Loss: 0.0686 Acc: 0.9768\n",
      "\n",
      "Epoch 168/299\n",
      "----------\n",
      "train Loss: 0.0782 Acc: 0.9720\n",
      "val Loss: 0.0658 Acc: 0.9797\n",
      "\n",
      "Epoch 169/299\n",
      "----------\n",
      "train Loss: 0.0793 Acc: 0.9721\n",
      "val Loss: 0.0663 Acc: 0.9801\n",
      "\n",
      "Epoch 170/299\n",
      "----------\n",
      "train Loss: 0.0787 Acc: 0.9730\n",
      "val Loss: 0.0677 Acc: 0.9813\n",
      "\n",
      "Epoch 171/299\n",
      "----------\n",
      "train Loss: 0.0796 Acc: 0.9729\n",
      "val Loss: 0.0652 Acc: 0.9788\n",
      "\n",
      "Epoch 172/299\n",
      "----------\n",
      "train Loss: 0.0796 Acc: 0.9722\n",
      "val Loss: 0.0651 Acc: 0.9788\n",
      "\n",
      "Epoch 173/299\n",
      "----------\n",
      "train Loss: 0.0814 Acc: 0.9711\n",
      "val Loss: 0.0653 Acc: 0.9817\n",
      "\n",
      "Epoch 174/299\n",
      "----------\n",
      "train Loss: 0.0792 Acc: 0.9722\n",
      "val Loss: 0.0653 Acc: 0.9817\n",
      "\n",
      "Epoch 175/299\n",
      "----------\n",
      "train Loss: 0.0781 Acc: 0.9728\n",
      "val Loss: 0.0653 Acc: 0.9788\n",
      "\n",
      "Epoch 176/299\n",
      "----------\n",
      "train Loss: 0.0782 Acc: 0.9713\n",
      "val Loss: 0.0744 Acc: 0.9730\n",
      "\n",
      "Epoch 177/299\n",
      "----------\n",
      "train Loss: 0.0806 Acc: 0.9714\n",
      "val Loss: 0.0657 Acc: 0.9801\n",
      "\n",
      "Epoch 178/299\n",
      "----------\n",
      "train Loss: 0.0761 Acc: 0.9741\n",
      "val Loss: 0.0648 Acc: 0.9797\n",
      "\n",
      "Epoch 179/299\n",
      "----------\n",
      "train Loss: 0.0833 Acc: 0.9723\n",
      "val Loss: 0.0645 Acc: 0.9792\n",
      "\n",
      "Epoch 180/299\n",
      "----------\n",
      "train Loss: 0.0788 Acc: 0.9709\n",
      "val Loss: 0.0660 Acc: 0.9805\n",
      "\n",
      "Epoch 181/299\n",
      "----------\n",
      "train Loss: 0.0787 Acc: 0.9703\n",
      "val Loss: 0.0646 Acc: 0.9805\n",
      "\n",
      "Epoch 182/299\n",
      "----------\n",
      "train Loss: 0.0800 Acc: 0.9716\n",
      "val Loss: 0.0651 Acc: 0.9797\n",
      "\n",
      "Epoch 183/299\n",
      "----------\n",
      "train Loss: 0.0784 Acc: 0.9712\n",
      "val Loss: 0.0665 Acc: 0.9784\n",
      "\n",
      "Epoch 184/299\n",
      "----------\n",
      "train Loss: 0.0794 Acc: 0.9712\n",
      "val Loss: 0.0651 Acc: 0.9809\n",
      "\n",
      "Epoch 185/299\n",
      "----------\n",
      "train Loss: 0.0794 Acc: 0.9719\n",
      "val Loss: 0.0644 Acc: 0.9813\n",
      "\n",
      "Epoch 186/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0796 Acc: 0.9704\n",
      "val Loss: 0.0654 Acc: 0.9805\n",
      "\n",
      "Epoch 187/299\n",
      "----------\n",
      "train Loss: 0.0787 Acc: 0.9692\n",
      "val Loss: 0.0642 Acc: 0.9809\n",
      "\n",
      "Epoch 188/299\n",
      "----------\n",
      "train Loss: 0.0789 Acc: 0.9727\n",
      "val Loss: 0.0649 Acc: 0.9801\n",
      "\n",
      "Epoch 189/299\n",
      "----------\n",
      "train Loss: 0.0787 Acc: 0.9727\n",
      "val Loss: 0.0688 Acc: 0.9784\n",
      "\n",
      "Epoch 190/299\n",
      "----------\n",
      "train Loss: 0.0742 Acc: 0.9738\n",
      "val Loss: 0.0648 Acc: 0.9809\n",
      "\n",
      "Epoch 191/299\n",
      "----------\n",
      "train Loss: 0.0798 Acc: 0.9728\n",
      "val Loss: 0.0661 Acc: 0.9797\n",
      "\n",
      "Epoch 192/299\n",
      "----------\n",
      "train Loss: 0.0765 Acc: 0.9735\n",
      "val Loss: 0.0649 Acc: 0.9792\n",
      "\n",
      "Epoch 193/299\n",
      "----------\n",
      "train Loss: 0.0786 Acc: 0.9721\n",
      "val Loss: 0.0641 Acc: 0.9805\n",
      "\n",
      "Epoch 194/299\n",
      "----------\n",
      "train Loss: 0.0796 Acc: 0.9712\n",
      "val Loss: 0.0646 Acc: 0.9813\n",
      "\n",
      "Epoch 195/299\n",
      "----------\n",
      "train Loss: 0.0779 Acc: 0.9722\n",
      "val Loss: 0.0642 Acc: 0.9809\n",
      "\n",
      "Epoch 196/299\n",
      "----------\n",
      "train Loss: 0.0747 Acc: 0.9744\n",
      "val Loss: 0.0647 Acc: 0.9813\n",
      "\n",
      "Epoch 197/299\n",
      "----------\n",
      "train Loss: 0.0782 Acc: 0.9723\n",
      "val Loss: 0.0672 Acc: 0.9776\n",
      "\n",
      "Epoch 198/299\n",
      "----------\n",
      "train Loss: 0.0763 Acc: 0.9737\n",
      "val Loss: 0.0645 Acc: 0.9792\n",
      "\n",
      "Epoch 199/299\n",
      "----------\n",
      "train Loss: 0.0788 Acc: 0.9724\n",
      "val Loss: 0.0646 Acc: 0.9822\n",
      "\n",
      "Epoch 200/299\n",
      "----------\n",
      "train Loss: 0.0773 Acc: 0.9723\n",
      "val Loss: 0.0640 Acc: 0.9822\n",
      "\n",
      "Epoch 201/299\n",
      "----------\n",
      "train Loss: 0.0773 Acc: 0.9735\n",
      "val Loss: 0.0658 Acc: 0.9801\n",
      "\n",
      "Epoch 202/299\n",
      "----------\n",
      "train Loss: 0.0791 Acc: 0.9703\n",
      "val Loss: 0.0638 Acc: 0.9805\n",
      "\n",
      "Epoch 203/299\n",
      "----------\n",
      "train Loss: 0.0768 Acc: 0.9725\n",
      "val Loss: 0.0649 Acc: 0.9801\n",
      "\n",
      "Epoch 204/299\n",
      "----------\n",
      "train Loss: 0.0787 Acc: 0.9722\n",
      "val Loss: 0.0636 Acc: 0.9809\n",
      "\n",
      "Epoch 205/299\n",
      "----------\n",
      "train Loss: 0.0776 Acc: 0.9709\n",
      "val Loss: 0.0653 Acc: 0.9797\n",
      "\n",
      "Epoch 206/299\n",
      "----------\n",
      "train Loss: 0.0762 Acc: 0.9722\n",
      "val Loss: 0.0650 Acc: 0.9801\n",
      "\n",
      "Epoch 207/299\n",
      "----------\n",
      "train Loss: 0.0766 Acc: 0.9707\n",
      "val Loss: 0.0647 Acc: 0.9797\n",
      "\n",
      "Epoch 208/299\n",
      "----------\n",
      "train Loss: 0.0756 Acc: 0.9741\n",
      "val Loss: 0.0633 Acc: 0.9801\n",
      "\n",
      "Epoch 209/299\n",
      "----------\n",
      "train Loss: 0.0768 Acc: 0.9724\n",
      "val Loss: 0.0636 Acc: 0.9801\n",
      "\n",
      "Epoch 210/299\n",
      "----------\n",
      "train Loss: 0.0754 Acc: 0.9731\n",
      "val Loss: 0.0633 Acc: 0.9805\n",
      "\n",
      "Epoch 211/299\n",
      "----------\n",
      "train Loss: 0.0774 Acc: 0.9747\n",
      "val Loss: 0.0650 Acc: 0.9813\n",
      "\n",
      "Epoch 212/299\n",
      "----------\n",
      "train Loss: 0.0779 Acc: 0.9710\n",
      "val Loss: 0.0679 Acc: 0.9772\n",
      "\n",
      "Epoch 213/299\n",
      "----------\n",
      "train Loss: 0.0793 Acc: 0.9725\n",
      "val Loss: 0.0639 Acc: 0.9797\n",
      "\n",
      "Epoch 214/299\n",
      "----------\n",
      "train Loss: 0.0767 Acc: 0.9739\n",
      "val Loss: 0.0643 Acc: 0.9809\n",
      "\n",
      "Epoch 215/299\n",
      "----------\n",
      "train Loss: 0.0745 Acc: 0.9731\n",
      "val Loss: 0.0635 Acc: 0.9805\n",
      "\n",
      "Epoch 216/299\n",
      "----------\n",
      "train Loss: 0.0745 Acc: 0.9728\n",
      "val Loss: 0.0634 Acc: 0.9797\n",
      "\n",
      "Epoch 217/299\n",
      "----------\n",
      "train Loss: 0.0760 Acc: 0.9731\n",
      "val Loss: 0.0634 Acc: 0.9805\n",
      "\n",
      "Epoch 218/299\n",
      "----------\n",
      "train Loss: 0.0762 Acc: 0.9719\n",
      "val Loss: 0.0639 Acc: 0.9805\n",
      "\n",
      "Epoch 219/299\n",
      "----------\n",
      "train Loss: 0.0765 Acc: 0.9741\n",
      "val Loss: 0.0632 Acc: 0.9805\n",
      "\n",
      "Epoch 220/299\n",
      "----------\n",
      "train Loss: 0.0775 Acc: 0.9712\n",
      "val Loss: 0.0655 Acc: 0.9776\n",
      "\n",
      "Epoch 221/299\n",
      "----------\n",
      "train Loss: 0.0739 Acc: 0.9727\n",
      "val Loss: 0.0642 Acc: 0.9805\n",
      "\n",
      "Epoch 222/299\n",
      "----------\n",
      "train Loss: 0.0736 Acc: 0.9745\n",
      "val Loss: 0.0636 Acc: 0.9801\n",
      "\n",
      "Epoch 223/299\n",
      "----------\n",
      "train Loss: 0.0738 Acc: 0.9745\n",
      "val Loss: 0.0638 Acc: 0.9830\n",
      "\n",
      "Epoch 224/299\n",
      "----------\n",
      "train Loss: 0.0758 Acc: 0.9725\n",
      "val Loss: 0.0630 Acc: 0.9805\n",
      "\n",
      "Epoch 225/299\n",
      "----------\n",
      "train Loss: 0.0788 Acc: 0.9710\n",
      "val Loss: 0.0630 Acc: 0.9805\n",
      "\n",
      "Epoch 226/299\n",
      "----------\n",
      "train Loss: 0.0774 Acc: 0.9731\n",
      "val Loss: 0.0628 Acc: 0.9813\n",
      "\n",
      "Epoch 227/299\n",
      "----------\n",
      "train Loss: 0.0773 Acc: 0.9721\n",
      "val Loss: 0.0647 Acc: 0.9805\n",
      "\n",
      "Epoch 228/299\n",
      "----------\n",
      "train Loss: 0.0757 Acc: 0.9731\n",
      "val Loss: 0.0627 Acc: 0.9805\n",
      "\n",
      "Epoch 229/299\n",
      "----------\n",
      "train Loss: 0.0760 Acc: 0.9716\n",
      "val Loss: 0.0640 Acc: 0.9813\n",
      "\n",
      "Epoch 230/299\n",
      "----------\n",
      "train Loss: 0.0747 Acc: 0.9743\n",
      "val Loss: 0.0635 Acc: 0.9805\n",
      "\n",
      "Epoch 231/299\n",
      "----------\n",
      "train Loss: 0.0765 Acc: 0.9708\n",
      "val Loss: 0.0646 Acc: 0.9809\n",
      "\n",
      "Epoch 232/299\n",
      "----------\n",
      "train Loss: 0.0784 Acc: 0.9734\n",
      "val Loss: 0.0635 Acc: 0.9813\n",
      "\n",
      "Epoch 233/299\n",
      "----------\n",
      "train Loss: 0.0763 Acc: 0.9729\n",
      "val Loss: 0.0630 Acc: 0.9797\n",
      "\n",
      "Epoch 234/299\n",
      "----------\n",
      "train Loss: 0.0760 Acc: 0.9717\n",
      "val Loss: 0.0634 Acc: 0.9801\n",
      "\n",
      "Epoch 235/299\n",
      "----------\n",
      "train Loss: 0.0753 Acc: 0.9743\n",
      "val Loss: 0.0629 Acc: 0.9813\n",
      "\n",
      "Epoch 236/299\n",
      "----------\n",
      "train Loss: 0.0765 Acc: 0.9732\n",
      "val Loss: 0.0628 Acc: 0.9809\n",
      "\n",
      "Epoch 237/299\n",
      "----------\n",
      "train Loss: 0.0752 Acc: 0.9727\n",
      "val Loss: 0.0632 Acc: 0.9822\n",
      "\n",
      "Epoch 238/299\n",
      "----------\n",
      "train Loss: 0.0753 Acc: 0.9730\n",
      "val Loss: 0.0692 Acc: 0.9768\n",
      "\n",
      "Epoch 239/299\n",
      "----------\n",
      "train Loss: 0.0751 Acc: 0.9735\n",
      "val Loss: 0.0628 Acc: 0.9801\n",
      "\n",
      "Epoch 240/299\n",
      "----------\n",
      "train Loss: 0.0766 Acc: 0.9732\n",
      "val Loss: 0.0630 Acc: 0.9809\n",
      "\n",
      "Epoch 241/299\n",
      "----------\n",
      "train Loss: 0.0746 Acc: 0.9737\n",
      "val Loss: 0.0630 Acc: 0.9809\n",
      "\n",
      "Epoch 242/299\n",
      "----------\n",
      "train Loss: 0.0771 Acc: 0.9714\n",
      "val Loss: 0.0644 Acc: 0.9792\n",
      "\n",
      "Epoch 243/299\n",
      "----------\n",
      "train Loss: 0.0772 Acc: 0.9731\n",
      "val Loss: 0.0627 Acc: 0.9817\n",
      "\n",
      "Epoch 244/299\n",
      "----------\n",
      "train Loss: 0.0762 Acc: 0.9718\n",
      "val Loss: 0.0634 Acc: 0.9809\n",
      "\n",
      "Epoch 245/299\n",
      "----------\n",
      "train Loss: 0.0786 Acc: 0.9734\n",
      "val Loss: 0.0624 Acc: 0.9817\n",
      "\n",
      "Epoch 246/299\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "num_classes = 3\n",
    "\n",
    "_models = ['squeezenet', 'densenet', 'resnet', 'alexnet', 'vgg']\n",
    "lrs = [1e-4]\n",
    "_epoch = [300]\n",
    "batch_sizes = [8]\n",
    "opt = [1, 2, 3]\n",
    "crt = [1, 2]\n",
    "\n",
    "for _model in _models:\n",
    "    for _epochs in _epoch:\n",
    "        for _lrs in lrs:\n",
    "            for _batch in batch_sizes:\n",
    "                for _opt in opt:\n",
    "                    for _crt in crt:\n",
    "                               \n",
    "                        print()\n",
    "                        print('='*60)\n",
    "                        print('==== INITIALIZING WITH PARAMETERS: ====')\n",
    "                        print(f'model -> {_model}')\n",
    "                        print(f'epochs -> {_epochs}')\n",
    "                        print(f'lr -> {_lrs}')\n",
    "                        print(f'batch size -> {_batch}')\n",
    "                        print(f'optimizer -> {_opt}')\n",
    "                        print(f'criteriun -> {_crt}')\n",
    "                        print()\n",
    "\n",
    "                        feature_extract = True\n",
    "\n",
    "                        model_ft, input_size = initialize_model(_model, num_classes, \n",
    "                                                                feature_extract, use_pretrained=True)\n",
    "\n",
    "                        # Send the model to GPU\n",
    "                        model_ft = model_ft.to(device)\n",
    "\n",
    "                        print('-'*20)\n",
    "                        params_to_update = model_ft.parameters()\n",
    "                        print(\"Params to learn:\")\n",
    "                        if feature_extract:\n",
    "                            params_to_update = []\n",
    "                            for name,param in model_ft.named_parameters():\n",
    "                                if param.requires_grad == True:\n",
    "                                    params_to_update.append(param)\n",
    "                                    print(\"\\t\",name)\n",
    "\n",
    "                        else:\n",
    "                            for name,param in model_ft.named_parameters():\n",
    "                                if param.requires_grad == True:\n",
    "                                    print(\"\\t\",name)\n",
    "\n",
    "\n",
    "                        print()\n",
    "                        print('-'*20)\n",
    "                        print()\n",
    "                        print('== Epochs ==')\n",
    "\n",
    "                        if _opt == 1:\n",
    "                            optimizer_ft = optim.SGD(params_to_update, _lrs, momentum=0.9)\n",
    "                            opt_name = 'SGD'\n",
    "\n",
    "                        if _opt == 2:\n",
    "                            optimizer_ft = optim.Adam(params_to_update, _lrs)\n",
    "                            opt_name = 'ADAM'\n",
    "                            \n",
    "                        if _opt == 3:\n",
    "                            optimizer_ft = optim.RMSprop(params_to_update, _lrs)\n",
    "                            opt_name = 'RMSprop'\n",
    "\n",
    "\n",
    "                        exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)\n",
    "\n",
    "                        #tray nn.NLLLoss\n",
    "                        if _crt == 1:\n",
    "                            criterion = nn.CrossEntropyLoss()\n",
    "                            crt_name = 'CrossEntropyLoss'\n",
    "                        if _crt == 2:\n",
    "                            criterion = nn.NLLLoss()\n",
    "                            crt_name = 'NLLLoss'\n",
    "\n",
    "                        model_ft, hist = train_model(model_ft, dataloaders, criterion, optimizer_ft,\n",
    "                                                num_epochs=_epochs, model_name=_model, lr=_lrs,\n",
    "                                                batch_size=_batch, opt_name=opt_name, crt_name=crt_name)\n",
    "\n",
    "                        from sklearn.metrics import confusion_matrix\n",
    "\n",
    "                        nb_classes = 3\n",
    "\n",
    "                        # Initialize the prediction and label lists(tensors)\n",
    "                        predlist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "                        lbllist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "\n",
    "                        with torch.no_grad():\n",
    "                            for i, (inputs, classes) in enumerate(dataloaders['val']):\n",
    "                                inputs = inputs.to(device) #labels atuais\n",
    "                                classes = classes.to(device) #classes\n",
    "                                outputs = model_ft(inputs) #valores preditos = Passa o label atual e retorna o que o modelo predice\n",
    "                                _, preds = torch.max(outputs, 1) #pega o maior valor das predies\n",
    "\n",
    "                                # Append batch prediction results\n",
    "                                predlist=torch.cat([predlist,preds.view(-1).cpu()])\n",
    "                                lbllist=torch.cat([lbllist,classes.view(-1).cpu()])\n",
    "\n",
    "                        # Confusion matrix\n",
    "                        conf_mat=confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
    "                        print(conf_mat)\n",
    "                        print()\n",
    "\n",
    "                        from sklearn import metrics\n",
    "\n",
    "                        #analise dos resultados do modelo\n",
    "                        print('Sensitivity or recall total')\n",
    "                        print (metrics.recall_score(lbllist.numpy(), predlist.numpy(), average='micro'))\n",
    "\n",
    "                        print()\n",
    "                        print('Sensitivity or recall per classes')\n",
    "                        print (metrics.recall_score(lbllist.numpy(), predlist.numpy(), average=None))\n",
    "\n",
    "                        print()\n",
    "                        print('Precision')\n",
    "                        print (metrics.precision_score(lbllist.numpy(), predlist.numpy(), average=None))\n",
    "\n",
    "                        print()\n",
    "                        print('F1 Score')\n",
    "                        print (metrics.f1_score(lbllist.numpy(), predlist.numpy(), average=None))\n",
    "\n",
    "                        cm = confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
    "                        np.set_printoptions(precision=2)\n",
    "\n",
    "                        plt.figure()\n",
    "\n",
    "                        plot_confusion_matrix(cm, classes=['norm', 'covid', 'pnemo'], \n",
    "                        title='Confusion matrix model'+_model+'_opt_'+opt_name+'_criteriun_'+crt_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Os resultados na matriz de confuzo e scores esto ruins, provavelmente pelo param.requires_grad = True \n",
    "\n",
    "Testar depois com False\n",
    "\n",
    "**Peguei o cdigo original**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"------ RESULTADOS ------\")\n",
    "print()\n",
    "plt.figure(figsize=(13, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label=\"AUC Treino\")\n",
    "plt.plot(history.history['val_accuracy'], label=\"AUC VALIDAO\")\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label=\"loss validao\")\n",
    "plt.plot(history.history['val_loss'], label=\"AUC validao\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hook the feature extractor\n",
    "# features_blobs = []\n",
    "# def hook_feature(module, input, output):\n",
    "#     features_blobs.append(output.data.cpu().numpy())\n",
    "\n",
    "# model_ft._modules.get(finalconv_name).register_forward_hook(hook_feature)\n",
    "\n",
    "# # get the softmax weight\n",
    "# params = list(model_ft.parameters())\n",
    "# weight_softmax = np.squeeze(params[-2].data.numpy())\n",
    "\n",
    "# def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "#     # generate the class activation maps upsample to 256x256\n",
    "#     size_upsample = (256, 256)\n",
    "#     bz, nc, h, w = feature_conv.shape\n",
    "#     output_cam = []\n",
    "#     for idx in class_idx:\n",
    "#         cam = weight_softmax[idx].dot(feature_conv.reshape((nc, h*w)))\n",
    "#         cam = cam.reshape(h, w)\n",
    "#         cam = cam - np.min(cam)\n",
    "#         cam_img = cam / np.max(cam)\n",
    "#         cam_img = np.uint8(255 * cam_img)\n",
    "#         output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "#     return output_cam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize = transforms.Normalize(\n",
    "#    mean=[0.485, 0.456, 0.406],\n",
    "#    std=[0.229, 0.224, 0.225]\n",
    "# )\n",
    "# preprocess = transforms.Compose([\n",
    "#    transforms.Resize((224,224)),\n",
    "#    transforms.ToTensor(),\n",
    "#    normalize\n",
    "# ])\n",
    "\n",
    "# response = requests.get(IMG_URL)\n",
    "# img_pil = Image.open(io.BytesIO(response.content))\n",
    "# img_pil.save('test.jpg')\n",
    "\n",
    "# img_tensor = preprocess(img_pil)\n",
    "# img_variable = Variable(img_tensor.unsqueeze(0))\n",
    "# logit = model_ft(img_variable)\n",
    "\n",
    "# # download the imagenet category list\n",
    "# # classes = {int(key):value for (key, value)\n",
    "# #           in requests.get(LABELS_URL).json().items()}\n",
    "\n",
    "# # h_x = F.softmax(logit, dim=1).data.squeeze()\n",
    "# # probs, idx = h_x.sort(0, True)\n",
    "# # probs = probs.numpy()\n",
    "# # idx = idx.numpy()\n",
    "\n",
    "# # # output the prediction\n",
    "# # for i in range(0, 5):\n",
    "# #     print('{:.3f} -> {}'.format(probs[i], classes[idx[i]]))\n",
    "\n",
    "# # generate class activation mapping for the top1 prediction\n",
    "# CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[0]])\n",
    "\n",
    "# # render the CAM and output\n",
    "# # print('output CAM.jpg for the top1 prediction: %s'%classes[idx[0]])\n",
    "# img = cv2.imread('test.jpg')\n",
    "# height, width, _ = img.shape\n",
    "# heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
    "# result = heatmap * 0.3 + img * 0.5\n",
    "# cv2.imwrite('CAM.jpg', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import IPython.display as display\n",
    "# from PIL import Image\n",
    "# image_path = 'CAM.jpg'\n",
    "# display.display(Image.open(image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 1 FIIMMMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "nb_classes = 3\n",
    "\n",
    "# Initialize the prediction and label lists(tensors)\n",
    "predlist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "lbllist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, classes) in enumerate(dataloaders['val']):\n",
    "        inputs = inputs.to(device) #labels atuais\n",
    "        classes = classes.to(device) #classes\n",
    "        outputs = model_ft(inputs) #valores preditos = Passa o label atual e retorna o que o modelo predice\n",
    "        _, preds = torch.max(outputs, 1) #pega o maior valor das predies\n",
    "\n",
    "        # Append batch prediction results\n",
    "        predlist=torch.cat([predlist,preds.view(-1).cpu()])\n",
    "        lbllist=torch.cat([lbllist,classes.view(-1).cpu()])\n",
    "\n",
    "# Confusion matrix\n",
    "conf_mat=confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
    "print(conf_mat)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    #mean = np.array([0.485, 0.456, 0.405])\n",
    "    #std = np.array([0.229, 0.224, 0.225])\n",
    "    #inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    #plt.pause(1)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=10):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    for i, (inputs, classes) in enumerate(dataloaders['test']):\n",
    "        #inputs, labels = data\n",
    "\n",
    "        #inputs, labels = Variable(inputs), Variable(labels)\n",
    "        inputs = inputs.to(device) #labels atuais\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "        for j in range(inputs.size()[0]):\n",
    "            images_so_far += 1\n",
    "            ax = plt.figure(figsize=(20,20))\n",
    "            ax = plt.subplot(5, 2, images_so_far)\n",
    "            ax.axis('off')\n",
    "            ax.set_title('{}'.format(class_names[predlist[j]]))\n",
    "            imshow(inputs.cpu().data[j])\n",
    "\n",
    "            if images_so_far == num_images:\n",
    "                model.train(mode=was_training)\n",
    "                return\n",
    "    model.train(mode=was_training)\n",
    "\n",
    "#print(dir(model))\n",
    "visualize_model(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# savePath = \"test_model.pth\"\n",
    "# torch.save(model_ft.state_dict(), savePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRY CAM\n",
    "#do it using RESNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time\n",
    "import PIL\n",
    "import scipy.ndimage as nd\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "#transforms.RandomRotation(degrees=(-5, 5)),\n",
    "#transforms.ColorJitter(brightness=.02),\n",
    "    \n",
    "transformers = {\n",
    "    'train_transforms': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'test_transforms': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'valid_transforms': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "}\n",
    "\n",
    "trans = ['train_transforms','valid_transforms','test_transforms']\n",
    "\n",
    "path = \"/home/jimi/dissertacao/covid19/datasets/80-20/\"\n",
    "categories = ['train','val','test']\n",
    "dset = {x : torchvision.datasets.ImageFolder(path+x,\n",
    "                                             transform=transformers[y]) for x,y in zip(categories, trans)}\n",
    "\n",
    "dataset_sizes = ['train']\n",
    "\n",
    "\n",
    "num_threads = 4 \n",
    "dataloaders =  {x : torch.utils.data.DataLoader(dset[x], batch_size=16, shuffle=True, num_workers=num_threads)\n",
    "               for x in categories}\n",
    "\n",
    "dataset_sizes = {x : len(dset[x]) for x in ['train','val','test']}\n",
    "\n",
    "class_names = dset['train'].classes\n",
    "\n",
    "#class_names = image_datasets['train'].classes\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/150\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimi/.local/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  0.2888  acc: 0.8944\n",
      "val loss:  0.1284  acc: 0.9601\n",
      "epoch 2/150\n",
      "----------\n",
      "train loss:  0.2063  acc: 0.9219\n",
      "val loss:  0.1282  acc: 0.9543\n",
      "epoch 3/150\n",
      "----------\n",
      "train loss:  0.1757  acc: 0.9354\n",
      "val loss:  0.1267  acc: 0.9560\n",
      "epoch 4/150\n",
      "----------\n",
      "train loss:  0.1363  acc: 0.9510\n",
      "val loss:  0.1089  acc: 0.9626\n",
      "epoch 5/150\n",
      "----------\n",
      "train loss:  0.1402  acc: 0.9500\n",
      "val loss:  0.1049  acc: 0.9639\n",
      "epoch 6/150\n",
      "----------\n",
      "train loss:  0.1337  acc: 0.9523\n",
      "val loss:  0.1090  acc: 0.9601\n",
      "epoch 7/150\n",
      "----------\n",
      "train loss:  0.1313  acc: 0.9520\n",
      "val loss:  0.0938  acc: 0.9697\n",
      "epoch 8/150\n",
      "----------\n",
      "train loss:  0.1307  acc: 0.9532\n",
      "val loss:  0.1055  acc: 0.9597\n",
      "epoch 9/150\n",
      "----------\n",
      "train loss:  0.1345  acc: 0.9530\n",
      "val loss:  0.1076  acc: 0.9585\n",
      "epoch 10/150\n",
      "----------\n",
      "train loss:  0.1355  acc: 0.9513\n",
      "val loss:  0.0982  acc: 0.9697\n",
      "epoch 11/150\n",
      "----------\n",
      "train loss:  0.1377  acc: 0.9500\n",
      "val loss:  0.1038  acc: 0.9618\n",
      "epoch 12/150\n",
      "----------\n",
      "train loss:  0.1343  acc: 0.9495\n",
      "val loss:  0.1131  acc: 0.9585\n",
      "epoch 13/150\n",
      "----------\n",
      "train loss:  0.1269  acc: 0.9525\n",
      "val loss:  0.1124  acc: 0.9589\n",
      "epoch 14/150\n",
      "----------\n",
      "train loss:  0.1321  acc: 0.9513\n",
      "val loss:  0.1104  acc: 0.9577\n",
      "epoch 15/150\n",
      "----------\n",
      "train loss:  0.1271  acc: 0.9536\n",
      "val loss:  0.0989  acc: 0.9697\n",
      "epoch 16/150\n",
      "----------\n",
      "train loss:  0.1307  acc: 0.9529\n",
      "val loss:  0.1169  acc: 0.9556\n",
      "epoch 17/150\n",
      "----------\n",
      "train loss:  0.1320  acc: 0.9538\n",
      "val loss:  0.1008  acc: 0.9668\n",
      "epoch 18/150\n",
      "----------\n",
      "train loss:  0.1338  acc: 0.9518\n",
      "val loss:  0.1012  acc: 0.9664\n",
      "epoch 19/150\n",
      "----------\n",
      "train loss:  0.1332  acc: 0.9509\n",
      "val loss:  0.1005  acc: 0.9639\n",
      "epoch 20/150\n",
      "----------\n",
      "train loss:  0.1287  acc: 0.9534\n",
      "val loss:  0.1103  acc: 0.9597\n",
      "epoch 21/150\n",
      "----------\n",
      "train loss:  0.1246  acc: 0.9537\n",
      "val loss:  0.1049  acc: 0.9631\n",
      "epoch 22/150\n",
      "----------\n",
      "train loss:  0.1378  acc: 0.9511\n",
      "val loss:  0.1049  acc: 0.9635\n",
      "epoch 23/150\n",
      "----------\n",
      "train loss:  0.1298  acc: 0.9512\n",
      "val loss:  0.1028  acc: 0.9626\n",
      "epoch 24/150\n",
      "----------\n",
      "train loss:  0.1268  acc: 0.9554\n",
      "val loss:  0.1129  acc: 0.9597\n",
      "epoch 25/150\n",
      "----------\n",
      "train loss:  0.1306  acc: 0.9520\n",
      "val loss:  0.0966  acc: 0.9672\n",
      "epoch 26/150\n",
      "----------\n",
      "train loss:  0.1292  acc: 0.9534\n",
      "val loss:  0.0985  acc: 0.9672\n",
      "epoch 27/150\n",
      "----------\n",
      "train loss:  0.1321  acc: 0.9530\n",
      "val loss:  0.1044  acc: 0.9626\n",
      "epoch 28/150\n",
      "----------\n",
      "train loss:  0.1354  acc: 0.9476\n",
      "val loss:  0.1121  acc: 0.9585\n",
      "epoch 29/150\n",
      "----------\n",
      "train loss:  0.1337  acc: 0.9507\n",
      "val loss:  0.1021  acc: 0.9643\n",
      "epoch 30/150\n",
      "----------\n",
      "train loss:  0.1278  acc: 0.9528\n",
      "val loss:  0.0963  acc: 0.9664\n",
      "epoch 31/150\n",
      "----------\n",
      "train loss:  0.1277  acc: 0.9548\n",
      "val loss:  0.1139  acc: 0.9564\n",
      "epoch 32/150\n",
      "----------\n",
      "train loss:  0.1326  acc: 0.9518\n",
      "val loss:  0.1149  acc: 0.9568\n",
      "epoch 33/150\n",
      "----------\n",
      "train loss:  0.1358  acc: 0.9500\n",
      "val loss:  0.1048  acc: 0.9622\n",
      "epoch 34/150\n",
      "----------\n",
      "train loss:  0.1296  acc: 0.9530\n",
      "val loss:  0.1039  acc: 0.9631\n",
      "epoch 35/150\n",
      "----------\n",
      "train loss:  0.1370  acc: 0.9528\n",
      "val loss:  0.1041  acc: 0.9639\n",
      "epoch 36/150\n",
      "----------\n",
      "train loss:  0.1316  acc: 0.9533\n",
      "val loss:  0.1048  acc: 0.9631\n",
      "epoch 37/150\n",
      "----------\n",
      "train loss:  0.1308  acc: 0.9532\n",
      "val loss:  0.1052  acc: 0.9610\n",
      "epoch 38/150\n",
      "----------\n",
      "train loss:  0.1335  acc: 0.9535\n",
      "val loss:  0.1010  acc: 0.9660\n",
      "epoch 39/150\n",
      "----------\n",
      "train loss:  0.1309  acc: 0.9527\n",
      "val loss:  0.1031  acc: 0.9626\n",
      "epoch 40/150\n",
      "----------\n",
      "train loss:  0.1300  acc: 0.9528\n",
      "val loss:  0.1006  acc: 0.9651\n",
      "epoch 41/150\n",
      "----------\n",
      "train loss:  0.1325  acc: 0.9525\n",
      "val loss:  0.1057  acc: 0.9614\n",
      "epoch 42/150\n",
      "----------\n",
      "train loss:  0.1313  acc: 0.9527\n",
      "val loss:  0.1003  acc: 0.9639\n",
      "epoch 43/150\n",
      "----------\n",
      "train loss:  0.1311  acc: 0.9535\n",
      "val loss:  0.1251  acc: 0.9539\n",
      "epoch 44/150\n",
      "----------\n",
      "train loss:  0.1285  acc: 0.9560\n",
      "val loss:  0.1089  acc: 0.9589\n",
      "epoch 45/150\n",
      "----------\n",
      "train loss:  0.1286  acc: 0.9516\n",
      "val loss:  0.0981  acc: 0.9676\n",
      "epoch 46/150\n",
      "----------\n",
      "train loss:  0.1337  acc: 0.9500\n",
      "val loss:  0.1089  acc: 0.9577\n",
      "epoch 47/150\n",
      "----------\n",
      "train loss:  0.1257  acc: 0.9540\n",
      "val loss:  0.1276  acc: 0.9514\n",
      "epoch 48/150\n",
      "----------\n",
      "train loss:  0.1317  acc: 0.9541\n",
      "val loss:  0.1000  acc: 0.9655\n",
      "epoch 49/150\n",
      "----------\n",
      "train loss:  0.1310  acc: 0.9534\n",
      "val loss:  0.1032  acc: 0.9626\n",
      "epoch 50/150\n",
      "----------\n",
      "train loss:  0.1289  acc: 0.9539\n",
      "val loss:  0.0998  acc: 0.9660\n",
      "epoch 51/150\n",
      "----------\n",
      "train loss:  0.1276  acc: 0.9531\n",
      "val loss:  0.0965  acc: 0.9672\n",
      "epoch 52/150\n",
      "----------\n",
      "train loss:  0.1266  acc: 0.9538\n",
      "val loss:  0.1052  acc: 0.9639\n",
      "epoch 53/150\n",
      "----------\n",
      "train loss:  0.1318  acc: 0.9531\n",
      "val loss:  0.1090  acc: 0.9601\n",
      "epoch 54/150\n",
      "----------\n",
      "train loss:  0.1259  acc: 0.9547\n",
      "val loss:  0.1125  acc: 0.9589\n",
      "epoch 55/150\n",
      "----------\n",
      "train loss:  0.1247  acc: 0.9555\n",
      "val loss:  0.1081  acc: 0.9585\n",
      "epoch 56/150\n",
      "----------\n",
      "train loss:  0.1296  acc: 0.9544\n",
      "val loss:  0.1043  acc: 0.9606\n",
      "epoch 57/150\n",
      "----------\n",
      "train loss:  0.1368  acc: 0.9505\n",
      "val loss:  0.1089  acc: 0.9626\n",
      "epoch 58/150\n",
      "----------\n",
      "train loss:  0.1291  acc: 0.9520\n",
      "val loss:  0.1014  acc: 0.9639\n",
      "epoch 59/150\n",
      "----------\n",
      "train loss:  0.1336  acc: 0.9521\n",
      "val loss:  0.1063  acc: 0.9581\n",
      "epoch 60/150\n",
      "----------\n",
      "train loss:  0.1284  acc: 0.9541\n",
      "val loss:  0.1040  acc: 0.9631\n",
      "epoch 61/150\n",
      "----------\n",
      "train loss:  0.1283  acc: 0.9530\n",
      "val loss:  0.1035  acc: 0.9618\n",
      "epoch 62/150\n",
      "----------\n",
      "train loss:  0.1323  acc: 0.9515\n",
      "val loss:  0.1090  acc: 0.9593\n",
      "epoch 63/150\n",
      "----------\n",
      "train loss:  0.1280  acc: 0.9551\n",
      "val loss:  0.1050  acc: 0.9610\n",
      "epoch 64/150\n",
      "----------\n",
      "train loss:  0.1313  acc: 0.9530\n",
      "val loss:  0.1179  acc: 0.9556\n",
      "epoch 65/150\n",
      "----------\n",
      "train loss:  0.1312  acc: 0.9527\n",
      "val loss:  0.1006  acc: 0.9622\n",
      "epoch 66/150\n",
      "----------\n",
      "train loss:  0.1268  acc: 0.9556\n",
      "val loss:  0.1143  acc: 0.9581\n",
      "epoch 67/150\n",
      "----------\n",
      "train loss:  0.1251  acc: 0.9541\n",
      "val loss:  0.0957  acc: 0.9705\n",
      "epoch 68/150\n",
      "----------\n",
      "train loss:  0.1271  acc: 0.9538\n",
      "val loss:  0.1026  acc: 0.9622\n",
      "epoch 69/150\n",
      "----------\n",
      "train loss:  0.1339  acc: 0.9485\n",
      "val loss:  0.1013  acc: 0.9631\n",
      "epoch 70/150\n",
      "----------\n",
      "train loss:  0.1311  acc: 0.9536\n",
      "val loss:  0.1035  acc: 0.9626\n",
      "epoch 71/150\n",
      "----------\n",
      "train loss:  0.1272  acc: 0.9525\n",
      "val loss:  0.1182  acc: 0.9568\n",
      "epoch 72/150\n",
      "----------\n",
      "train loss:  0.1261  acc: 0.9532\n",
      "val loss:  0.1096  acc: 0.9593\n",
      "epoch 73/150\n",
      "----------\n",
      "train loss:  0.1318  acc: 0.9514\n",
      "val loss:  0.1160  acc: 0.9577\n",
      "epoch 74/150\n",
      "----------\n",
      "train loss:  0.1327  acc: 0.9514\n",
      "val loss:  0.1078  acc: 0.9597\n",
      "epoch 75/150\n",
      "----------\n",
      "train loss:  0.1324  acc: 0.9540\n",
      "val loss:  0.1121  acc: 0.9597\n",
      "epoch 76/150\n",
      "----------\n",
      "train loss:  0.1294  acc: 0.9523\n",
      "val loss:  0.1065  acc: 0.9635\n",
      "epoch 77/150\n",
      "----------\n",
      "train loss:  0.1316  acc: 0.9518\n",
      "val loss:  0.1033  acc: 0.9631\n",
      "epoch 78/150\n",
      "----------\n",
      "train loss:  0.1291  acc: 0.9516\n",
      "val loss:  0.1122  acc: 0.9589\n",
      "epoch 79/150\n",
      "----------\n",
      "train loss:  0.1343  acc: 0.9540\n",
      "val loss:  0.1018  acc: 0.9639\n",
      "epoch 80/150\n",
      "----------\n",
      "train loss:  0.1296  acc: 0.9559\n",
      "val loss:  0.1067  acc: 0.9606\n",
      "epoch 81/150\n",
      "----------\n",
      "train loss:  0.1336  acc: 0.9532\n",
      "val loss:  0.1042  acc: 0.9655\n",
      "epoch 82/150\n",
      "----------\n",
      "train loss:  0.1307  acc: 0.9527\n",
      "val loss:  0.1064  acc: 0.9614\n",
      "epoch 83/150\n",
      "----------\n",
      "train loss:  0.1312  acc: 0.9531\n",
      "val loss:  0.1015  acc: 0.9635\n",
      "epoch 84/150\n",
      "----------\n",
      "train loss:  0.1343  acc: 0.9492\n",
      "val loss:  0.0938  acc: 0.9689\n",
      "epoch 85/150\n",
      "----------\n",
      "train loss:  0.1322  acc: 0.9542\n",
      "val loss:  0.1135  acc: 0.9589\n",
      "epoch 86/150\n",
      "----------\n",
      "train loss:  0.1299  acc: 0.9532\n",
      "val loss:  0.1105  acc: 0.9593\n",
      "epoch 87/150\n",
      "----------\n",
      "train loss:  0.1331  acc: 0.9531\n",
      "val loss:  0.1157  acc: 0.9581\n",
      "epoch 88/150\n",
      "----------\n",
      "train loss:  0.1290  acc: 0.9544\n",
      "val loss:  0.1095  acc: 0.9606\n",
      "epoch 89/150\n",
      "----------\n",
      "train loss:  0.1298  acc: 0.9530\n",
      "val loss:  0.0964  acc: 0.9685\n",
      "epoch 90/150\n",
      "----------\n",
      "train loss:  0.1330  acc: 0.9523\n",
      "val loss:  0.1041  acc: 0.9626\n",
      "epoch 91/150\n",
      "----------\n",
      "train loss:  0.1296  acc: 0.9546\n",
      "val loss:  0.1027  acc: 0.9631\n",
      "epoch 92/150\n",
      "----------\n",
      "train loss:  0.1266  acc: 0.9549\n",
      "val loss:  0.1195  acc: 0.9552\n",
      "epoch 93/150\n",
      "----------\n",
      "train loss:  0.1295  acc: 0.9548\n",
      "val loss:  0.1123  acc: 0.9622\n",
      "epoch 94/150\n",
      "----------\n",
      "train loss:  0.1288  acc: 0.9536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:  0.1078  acc: 0.9601\n",
      "epoch 95/150\n",
      "----------\n",
      "train loss:  0.1265  acc: 0.9554\n",
      "val loss:  0.1039  acc: 0.9643\n",
      "epoch 96/150\n",
      "----------\n",
      "train loss:  0.1323  acc: 0.9530\n",
      "val loss:  0.1154  acc: 0.9577\n",
      "epoch 97/150\n",
      "----------\n",
      "train loss:  0.1332  acc: 0.9523\n",
      "val loss:  0.1198  acc: 0.9514\n",
      "epoch 98/150\n",
      "----------\n",
      "train loss:  0.1277  acc: 0.9545\n",
      "val loss:  0.1081  acc: 0.9610\n",
      "epoch 99/150\n",
      "----------\n",
      "train loss:  0.1300  acc: 0.9506\n",
      "val loss:  0.1105  acc: 0.9622\n",
      "epoch 100/150\n",
      "----------\n",
      "train loss:  0.1307  acc: 0.9541\n",
      "val loss:  0.1092  acc: 0.9606\n",
      "epoch 101/150\n",
      "----------\n",
      "train loss:  0.1305  acc: 0.9518\n",
      "val loss:  0.1016  acc: 0.9635\n",
      "epoch 102/150\n",
      "----------\n",
      "train loss:  0.1291  acc: 0.9535\n",
      "val loss:  0.0995  acc: 0.9664\n",
      "epoch 103/150\n",
      "----------\n",
      "train loss:  0.1281  acc: 0.9536\n",
      "val loss:  0.0957  acc: 0.9680\n",
      "epoch 104/150\n",
      "----------\n",
      "train loss:  0.1319  acc: 0.9496\n",
      "val loss:  0.1015  acc: 0.9643\n",
      "epoch 105/150\n",
      "----------\n",
      "train loss:  0.1318  acc: 0.9521\n",
      "val loss:  0.1059  acc: 0.9597\n",
      "epoch 106/150\n",
      "----------\n",
      "train loss:  0.1331  acc: 0.9490\n",
      "val loss:  0.0948  acc: 0.9672\n",
      "epoch 107/150\n",
      "----------\n",
      "train loss:  0.1249  acc: 0.9548\n",
      "val loss:  0.1118  acc: 0.9597\n",
      "epoch 108/150\n",
      "----------\n",
      "train loss:  0.1313  acc: 0.9515\n",
      "val loss:  0.1119  acc: 0.9589\n",
      "epoch 109/150\n",
      "----------\n",
      "train loss:  0.1262  acc: 0.9550\n",
      "val loss:  0.1042  acc: 0.9618\n",
      "epoch 110/150\n",
      "----------\n",
      "train loss:  0.1282  acc: 0.9535\n",
      "val loss:  0.1055  acc: 0.9655\n",
      "epoch 111/150\n",
      "----------\n",
      "train loss:  0.1286  acc: 0.9538\n",
      "val loss:  0.1048  acc: 0.9606\n",
      "epoch 112/150\n",
      "----------\n",
      "train loss:  0.1268  acc: 0.9557\n",
      "val loss:  0.1104  acc: 0.9593\n",
      "epoch 113/150\n",
      "----------\n",
      "train loss:  0.1263  acc: 0.9570\n",
      "val loss:  0.1040  acc: 0.9622\n",
      "epoch 114/150\n",
      "----------\n",
      "train loss:  0.1274  acc: 0.9528\n",
      "val loss:  0.1032  acc: 0.9614\n",
      "epoch 115/150\n",
      "----------\n",
      "train loss:  0.1317  acc: 0.9516\n",
      "val loss:  0.1073  acc: 0.9606\n",
      "epoch 116/150\n",
      "----------\n",
      "train loss:  0.1306  acc: 0.9542\n",
      "val loss:  0.1039  acc: 0.9618\n",
      "epoch 117/150\n",
      "----------\n",
      "train loss:  0.1328  acc: 0.9525\n",
      "val loss:  0.1034  acc: 0.9651\n",
      "epoch 118/150\n",
      "----------\n",
      "train loss:  0.1319  acc: 0.9545\n",
      "val loss:  0.1074  acc: 0.9622\n",
      "epoch 119/150\n",
      "----------\n",
      "train loss:  0.1254  acc: 0.9535\n",
      "val loss:  0.1018  acc: 0.9622\n",
      "epoch 120/150\n",
      "----------\n",
      "train loss:  0.1355  acc: 0.9525\n",
      "val loss:  0.0949  acc: 0.9714\n",
      "epoch 121/150\n",
      "----------\n",
      "train loss:  0.1254  acc: 0.9551\n",
      "val loss:  0.1060  acc: 0.9614\n",
      "epoch 122/150\n",
      "----------\n",
      "train loss:  0.1293  acc: 0.9549\n",
      "val loss:  0.1126  acc: 0.9610\n",
      "epoch 123/150\n",
      "----------\n",
      "train loss:  0.1289  acc: 0.9529\n",
      "val loss:  0.1199  acc: 0.9556\n",
      "epoch 124/150\n",
      "----------\n",
      "train loss:  0.1300  acc: 0.9511\n",
      "val loss:  0.1066  acc: 0.9618\n",
      "epoch 125/150\n",
      "----------\n",
      "train loss:  0.1301  acc: 0.9540\n",
      "val loss:  0.1008  acc: 0.9651\n",
      "epoch 126/150\n",
      "----------\n",
      "train loss:  0.1344  acc: 0.9498\n",
      "val loss:  0.1025  acc: 0.9643\n",
      "epoch 127/150\n",
      "----------\n",
      "train loss:  0.1283  acc: 0.9546\n",
      "val loss:  0.1247  acc: 0.9543\n",
      "epoch 128/150\n",
      "----------\n",
      "train loss:  0.1320  acc: 0.9555\n",
      "val loss:  0.1055  acc: 0.9614\n",
      "epoch 129/150\n",
      "----------\n",
      "train loss:  0.1330  acc: 0.9521\n",
      "val loss:  0.1091  acc: 0.9585\n",
      "epoch 130/150\n",
      "----------\n",
      "train loss:  0.1330  acc: 0.9506\n",
      "val loss:  0.1080  acc: 0.9606\n",
      "epoch 131/150\n",
      "----------\n",
      "train loss:  0.1317  acc: 0.9528\n",
      "val loss:  0.1142  acc: 0.9585\n",
      "epoch 132/150\n",
      "----------\n",
      "train loss:  0.1322  acc: 0.9528\n",
      "val loss:  0.0999  acc: 0.9639\n",
      "epoch 133/150\n",
      "----------\n",
      "train loss:  0.1302  acc: 0.9542\n",
      "val loss:  0.0980  acc: 0.9664\n",
      "epoch 134/150\n",
      "----------\n",
      "train loss:  0.1253  acc: 0.9541\n",
      "val loss:  0.1116  acc: 0.9601\n",
      "epoch 135/150\n",
      "----------\n",
      "train loss:  0.1251  acc: 0.9546\n",
      "val loss:  0.1077  acc: 0.9589\n",
      "epoch 136/150\n",
      "----------\n",
      "train loss:  0.1348  acc: 0.9512\n",
      "val loss:  0.1042  acc: 0.9626\n",
      "epoch 137/150\n",
      "----------\n",
      "train loss:  0.1319  acc: 0.9528\n",
      "val loss:  0.1075  acc: 0.9593\n",
      "epoch 138/150\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "##### RESNET\n",
    "##Build model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = torchvision.models.resnet152(pretrained=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.model.fc.in_features,3),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        for params in self.model.parameters():\n",
    "            params.requires_grad = True\n",
    "        self.model.fc = self.classifier\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def fit(self, dataloaders, num_epochs):\n",
    "        train_on_gpu = torch.cuda.is_available()\n",
    "        optimizer = optim.Adam(self.model.fc.parameters())\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, 4)\n",
    "        criterion = nn.NLLLoss()\n",
    "        since = time.time()\n",
    "        \n",
    "        best_model_wts = copy.deepcopy(self.model.state_dict())\n",
    "        best_acc = 0.0\n",
    "        \n",
    "        if train_on_gpu:\n",
    "            self.model = self.model.cuda()\n",
    "            \n",
    "        for epoch in range(1, num_epochs+1):\n",
    "            print(\"epoch {}/{}\".format(epoch, num_epochs))\n",
    "            print(\"-\" * 10)\n",
    "            \n",
    "            for phase in ['train','val']:\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "                    self.model.train()\n",
    "                else:\n",
    "                    self.model.eval()\n",
    "                \n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0.0\n",
    "                \n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    if train_on_gpu:\n",
    "                        inputs = inputs.cuda()\n",
    "                        labels = labels.cuda()\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = self.model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        \n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                    \n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "                print(\"{} loss:  {:.4f}  acc: {:.4f}\".format(phase, epoch_loss, epoch_acc))\n",
    "                \n",
    "                if phase == 'val' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(self.model.state_dict())\n",
    "        \n",
    "        time_elapsed = time.time() - since\n",
    "        print('time completed: {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 600))\n",
    "        print(\"best val acc: {:.4f}\".format(best_acc))\n",
    "        \n",
    "        self.model.load_state_dict(best_model_wts)\n",
    "        return self.model\n",
    "    \n",
    "model = Model()\n",
    "model_ft = model.fit(dataloaders,150)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "nb_classes = 3\n",
    "\n",
    "# Initialize the prediction and label lists(tensors)\n",
    "predlist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "lbllist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, classes) in enumerate(dataloaders['val']):\n",
    "        inputs = inputs.to(device) #labels atuais\n",
    "        classes = classes.to(device) #classes\n",
    "        outputs = model_ft(inputs) #valores preditos = Passa o label atual e retorna o que o modelo predice\n",
    "        _, preds = torch.max(outputs, 1) #pega o maior valor das predies\n",
    "\n",
    "        # Append batch prediction results\n",
    "        predlist=torch.cat([predlist,preds.view(-1).cpu()])\n",
    "        lbllist=torch.cat([lbllist,classes.view(-1).cpu()])\n",
    "\n",
    "# Confusion matrix\n",
    "conf_mat=confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
    "print(conf_mat)\n",
    "print()\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "#analise dos resultados do modelo\n",
    "print('Sensitivity or recall total')\n",
    "print (metrics.recall_score(lbllist.numpy(), predlist.numpy(), average='micro'))\n",
    "\n",
    "print()\n",
    "print('Sensitivity or recall per classes')\n",
    "print (metrics.recall_score(lbllist.numpy(), predlist.numpy(), average=None))\n",
    "\n",
    "print()\n",
    "print('Precision')\n",
    "print (metrics.precision_score(lbllist.numpy(), predlist.numpy(), average=None))\n",
    "\n",
    "print()\n",
    "print('F1 Score')\n",
    "print (metrics.f1_score(lbllist.numpy(), predlist.numpy(), average=None))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "nb_classes = 3\n",
    "\n",
    "# Initialize the prediction and label lists(tensors)\n",
    "predlist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "lbllist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, classes) in enumerate(dataloaders['val']):\n",
    "        inputs = inputs.to(device) #labels atuais\n",
    "        classes = classes.to(device) #classes\n",
    "        outputs = model_ft(inputs) #valores preditos = Passa o label atual e retorna o que o modelo predice\n",
    "        _, preds = torch.max(outputs, 1) #pega o maior valor das predies\n",
    "\n",
    "        # Append batch prediction results\n",
    "        predlist=torch.cat([predlist,preds.view(-1).cpu()])\n",
    "        lbllist=torch.cat([lbllist,classes.view(-1).cpu()])\n",
    "\n",
    "# Confusion matrix\n",
    "conf_mat=confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
    "print(conf_mat)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools \n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plot_confusion_matrix(cm, classes=['norm', 'covid', 'pnemo'], title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_test = \"/home/jimi/dissertacao/covid19/datasets/80-20/test/\"\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      #transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                      #                     [0.229, 0.224, 0.225])\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image):\n",
    "    image_tensor = test_transforms(image).float()\n",
    "    image_tensor = image_tensor.unsqueeze_(0)\n",
    "    input = Variable(image_tensor)\n",
    "    input = input.to(device)\n",
    "    output = model(input)\n",
    "    index = output.data.cpu().numpy().argmax()\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_images(num):\n",
    "    data = datasets.ImageFolder(data_dir_test, transform=test_transforms)\n",
    "    classes = data.classes\n",
    "    indices = list(range(len(data)))\n",
    "    np.random.shuffle(indices)\n",
    "    idx = indices[:num]\n",
    "    from torch.utils.data.sampler import SubsetRandomSampler\n",
    "    sampler = SubsetRandomSampler(idx)\n",
    "    loader = torch.utils.data.DataLoader(data,sampler=sampler, batch_size=num)\n",
    "    dataiter = iter(loader)\n",
    "    images, labels = dataiter.next()\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions\n",
    "to_pil = transforms.ToPILImage()\n",
    "images, labels = get_random_images(30)\n",
    "fig=plt.figure(figsize=(20,20))\n",
    "for ii in range(len(images)):\n",
    "    image = to_pil(images[ii])\n",
    "    index = predict_image(image)\n",
    "    \n",
    "    data = datasets.ImageFolder(data_dir_test, transform=test_transforms)\n",
    "    classes = data.classes\n",
    "    \n",
    "    #print (f'index: {index}')\n",
    "    #print (f'image: {image}')\n",
    "    #print (f'labes: {labels}')\n",
    "    #print (f'classes index :{classes[index]}')\n",
    "    #print (f'classes 2:{classes}')\n",
    "    \n",
    "    sub = fig.add_subplot(8, 4, ii+1)\n",
    "    \n",
    "    #print()\n",
    "    res = int(labels[ii]) == 1\n",
    "    #print(f'int(labels[ii]): {int(labels[ii])}')\n",
    "    #print(f'index: {index}')\n",
    "    #print(f'res = int(labels[ii]) == index: {res}')\n",
    "    #print()\n",
    "    \n",
    "    #print (f'res : {res}')\n",
    "    \n",
    "    sub.set_title(str(classes[1]) + \":\" + str(res))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(),\n",
    "                            transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])])\n",
    "def image_loader(image_name):\n",
    "    image = PIL.Image.open(image_name).convert(\"RGB\")\n",
    "    image = loader(image).float()\n",
    "    image = image.unsqueeze(0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerActivations():\n",
    "    features=[]\n",
    "    def __init__(self,model):\n",
    "        self.hooks = []\n",
    "        self.hooks.append(model.layer4.register_forward_hook(self.hook_fn))\n",
    "    def hook_fn(self,module,input,output):\n",
    "        self.features.append(output)\n",
    "    def remove(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/home/jimi/dissertacao/covid19/datasets/80-20/val/normal/1785.jpg'\n",
    "img = image_loader(image_path).cuda()\n",
    "\n",
    "acts = LayerActivations(model_ft)\n",
    "\n",
    "logps = model_ft(img).cuda()\n",
    "ps = torch.exp(logps)\n",
    "\n",
    "out_features = acts.features[0]\n",
    "out_features = torch.squeeze(out_features, dim=0)\n",
    "out_features = out_features.cpu().detach().numpy()\n",
    "out_features = np.transpose(out_features,axes=(1,2,0))\n",
    "\n",
    "W = model_ft.fc[0].weight\n",
    "top_probs, top_classes = torch.topk(ps, k=3)\n",
    "ps = ps.cpu().detach().numpy()\n",
    "pred = np.argmax(ps)\n",
    "w = W[pred,:]\n",
    "\n",
    "w = w.cpu().detach().numpy()\n",
    "cam = np.dot(out_features, w)\n",
    "#type(w)\n",
    "\n",
    "class_activation = nd.zoom(cam, zoom=(32,32),order=1)\n",
    "\n",
    "img = torch.squeeze(img,0)\n",
    "img = img.cpu().numpy()\n",
    "img = np.transpose(img,(1,2,0))\n",
    "mean = np.array([0.5,0.5,0.5])\n",
    "std =  np.array([0.5,0.5,0.5])\n",
    "#img = img.cpu().numpy()\n",
    "img = (img + mean) * std\n",
    "img = np.clip(img, a_max=1, a_min=0)\n",
    "\n",
    "plt.imshow(class_activation, cmap='jet',alpha=1)\n",
    "plt.imshow(img, alpha=0.55)\n",
    "plt.title(dset['val'].classes[pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/home/jimi/dissertacao/covid19/datasets/80-20/val/normal/860.jpg'\n",
    "img = image_loader(image_path).cuda()\n",
    "\n",
    "acts = LayerActivations(model_ft)\n",
    "\n",
    "logps = model_ft(img).cuda()\n",
    "ps = torch.exp(logps)\n",
    "\n",
    "out_features = acts.features[0]\n",
    "out_features = torch.squeeze(out_features, dim=0)\n",
    "out_features = out_features.cpu().detach().numpy()\n",
    "out_features = np.transpose(out_features,axes=(1,2,0))\n",
    "\n",
    "W = model_ft.fc[0].weight\n",
    "top_probs, top_classes = torch.topk(ps, k=3)\n",
    "ps = ps.cpu().detach().numpy()\n",
    "pred = np.argmax(ps)\n",
    "w = W[pred,:]\n",
    "\n",
    "w = w.cpu().detach().numpy()\n",
    "cam = np.dot(out_features, w)\n",
    "#type(w)\n",
    "\n",
    "class_activation = nd.zoom(cam, zoom=(32,32),order=1)\n",
    "\n",
    "img = torch.squeeze(img,0)\n",
    "img = img.cpu().numpy()\n",
    "img = np.transpose(img,(1,2,0))\n",
    "mean = np.array([0.5,0.5,0.5])\n",
    "std =  np.array([0.5,0.5,0.5])\n",
    "#img = img.cpu().numpy()\n",
    "img = (img + mean) * std\n",
    "img = np.clip(img, a_max=1, a_min=0)\n",
    "\n",
    "plt.imshow(class_activation, cmap='jet',alpha=1)\n",
    "plt.imshow(img, alpha=0.55)\n",
    "plt.title(dset['val'].classes[pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/home/jimi/dissertacao/covid19/datasets/80-20/val/normal/2480.jpg'\n",
    "img = image_loader(image_path).cuda()\n",
    "\n",
    "acts = LayerActivations(model_ft)\n",
    "\n",
    "logps = model_ft(img).cuda()\n",
    "ps = torch.exp(logps)\n",
    "\n",
    "out_features = acts.features[0]\n",
    "out_features = torch.squeeze(out_features, dim=0)\n",
    "out_features = out_features.cpu().detach().numpy()\n",
    "out_features = np.transpose(out_features,axes=(1,2,0))\n",
    "\n",
    "W = model_ft.fc[0].weight\n",
    "top_probs, top_classes = torch.topk(ps, k=3)\n",
    "ps = ps.cpu().detach().numpy()\n",
    "pred = np.argmax(ps)\n",
    "w = W[pred,:]\n",
    "\n",
    "w = w.cpu().detach().numpy()\n",
    "cam = np.dot(out_features, w)\n",
    "#type(w)\n",
    "\n",
    "class_activation = nd.zoom(cam, zoom=(32,32),order=1)\n",
    "\n",
    "img = torch.squeeze(img,0)\n",
    "img = img.cpu().numpy()\n",
    "img = np.transpose(img,(1,2,0))\n",
    "mean = np.array([0.5,0.5,0.5])\n",
    "std =  np.array([0.5,0.5,0.5])\n",
    "#img = img.cpu().numpy()\n",
    "img = (img + mean) * std\n",
    "img = np.clip(img, a_max=1, a_min=0)\n",
    "\n",
    "plt.imshow(class_activation, cmap='jet',alpha=1)\n",
    "plt.imshow(img, alpha=0.55)\n",
    "plt.title(dset['val'].classes[pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/home/jimi/dissertacao/covid19/datasets/80-20/test/covid/000001.png'\n",
    "img = image_loader(image_path).cuda()\n",
    "\n",
    "acts = LayerActivations(model_ft)\n",
    "\n",
    "logps = model_ft(img).cuda()\n",
    "ps = torch.exp(logps)\n",
    "\n",
    "out_features = acts.features[0]\n",
    "out_features = torch.squeeze(out_features, dim=0)\n",
    "out_features = out_features.cpu().detach().numpy()\n",
    "out_features = np.transpose(out_features,axes=(1,2,0))\n",
    "\n",
    "W = model_ft.fc[0].weight\n",
    "top_probs, top_classes = torch.topk(ps, k=3)\n",
    "ps = ps.cpu().detach().numpy()\n",
    "pred = np.argmax(ps)\n",
    "w = W[pred,:]\n",
    "\n",
    "w = w.cpu().detach().numpy()\n",
    "cam = np.dot(out_features, w)\n",
    "#type(w)\n",
    "\n",
    "class_activation = nd.zoom(cam, zoom=(32,32),order=1)\n",
    "\n",
    "img = torch.squeeze(img,0)\n",
    "img = img.cpu().numpy()\n",
    "img = np.transpose(img,(1,2,0))\n",
    "mean = np.array([0.5,0.5,0.5])\n",
    "std =  np.array([0.5,0.5,0.5])\n",
    "#img = img.cpu().numpy()\n",
    "img = (img + mean) * std\n",
    "img = np.clip(img, a_max=1, a_min=0)\n",
    "\n",
    "plt.imshow(class_activation, cmap='jet',alpha=1)\n",
    "plt.imshow(img, alpha=0.55)\n",
    "plt.title(dset['val'].classes[pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/home/jimi/dissertacao/covid19/datasets/80-20/test/covid/000001-9-a.jpg'\n",
    "img = image_loader(image_path).cuda()\n",
    "\n",
    "acts = LayerActivations(model_ft)\n",
    "\n",
    "logps = model_ft(img).cuda()\n",
    "ps = torch.exp(logps)\n",
    "\n",
    "out_features = acts.features[0]\n",
    "out_features = torch.squeeze(out_features, dim=0)\n",
    "out_features = out_features.cpu().detach().numpy()\n",
    "out_features = np.transpose(out_features,axes=(1,2,0))\n",
    "\n",
    "W = model_ft.fc[0].weight\n",
    "top_probs, top_classes = torch.topk(ps, k=3)\n",
    "ps = ps.cpu().detach().numpy()\n",
    "pred = np.argmax(ps)\n",
    "w = W[pred,:]\n",
    "\n",
    "w = w.cpu().detach().numpy()\n",
    "cam = np.dot(out_features, w)\n",
    "#type(w)\n",
    "\n",
    "class_activation = nd.zoom(cam, zoom=(32,32),order=1)\n",
    "\n",
    "img = torch.squeeze(img,0)\n",
    "img = img.cpu().numpy()\n",
    "img = np.transpose(img,(1,2,0))\n",
    "mean = np.array([0.5,0.5,0.5])\n",
    "std =  np.array([0.5,0.5,0.5])\n",
    "#img = img.cpu().numpy()\n",
    "img = (img + mean) * std\n",
    "img = np.clip(img, a_max=1, a_min=0)\n",
    "\n",
    "plt.imshow(class_activation, cmap='jet',alpha=1)\n",
    "plt.imshow(img, alpha=0.55)\n",
    "plt.title(dset['val'].classes[pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/home/jimi/dissertacao/covid19/datasets/80-20/val/covid/3010b.png'\n",
    "img = image_loader(image_path).cuda()\n",
    "\n",
    "acts = LayerActivations(model_ft)\n",
    "\n",
    "logps = model_ft(img).cuda()\n",
    "ps = torch.exp(logps)\n",
    "\n",
    "out_features = acts.features[0]\n",
    "out_features = torch.squeeze(out_features, dim=0)\n",
    "out_features = out_features.cpu().detach().numpy()\n",
    "out_features = np.transpose(out_features,axes=(1,2,0))\n",
    "\n",
    "W = model_ft.fc[0].weight\n",
    "top_probs, top_classes = torch.topk(ps, k=3)\n",
    "ps = ps.cpu().detach().numpy()\n",
    "pred = np.argmax(ps)\n",
    "w = W[pred,:]\n",
    "\n",
    "w = w.cpu().detach().numpy()\n",
    "cam = np.dot(out_features, w)\n",
    "#type(w)\n",
    "\n",
    "class_activation = nd.zoom(cam, zoom=(32,32),order=1)\n",
    "\n",
    "img = torch.squeeze(img,0)\n",
    "img = img.cpu().numpy()\n",
    "img = np.transpose(img,(1,2,0))\n",
    "mean = np.array([0.5,0.5,0.5])\n",
    "std =  np.array([0.5,0.5,0.5])\n",
    "#img = img.cpu().numpy()\n",
    "img = (img + mean) * std\n",
    "img = np.clip(img, a_max=1, a_min=0)\n",
    "\n",
    "plt.imshow(class_activation, cmap='jet',alpha=1)\n",
    "plt.imshow(img, alpha=0.55)\n",
    "plt.title(dset['val'].classes[pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/home/jimi/dissertacao/covid19/datasets/80-20/val/pneumonia/1705.jpg'\n",
    "img = image_loader(image_path).cuda()\n",
    "\n",
    "acts = LayerActivations(model_ft)\n",
    "\n",
    "logps = model_ft(img).cuda()\n",
    "ps = torch.exp(logps)\n",
    "\n",
    "out_features = acts.features[0]\n",
    "out_features = torch.squeeze(out_features, dim=0)\n",
    "out_features = out_features.cpu().detach().numpy()\n",
    "out_features = np.transpose(out_features,axes=(1,2,0))\n",
    "\n",
    "W = model_ft.fc[0].weight\n",
    "top_probs, top_classes = torch.topk(ps, k=3)\n",
    "ps = ps.cpu().detach().numpy()\n",
    "pred = np.argmax(ps)\n",
    "w = W[pred,:]\n",
    "\n",
    "w = w.cpu().detach().numpy()\n",
    "cam = np.dot(out_features, w)\n",
    "#type(w)\n",
    "\n",
    "class_activation = nd.zoom(cam, zoom=(32,32),order=1)\n",
    "\n",
    "img = torch.squeeze(img,0)\n",
    "img = img.cpu().numpy()\n",
    "img = np.transpose(img,(1,2,0))\n",
    "mean = np.array([0.5,0.5,0.5])\n",
    "std =  np.array([0.5,0.5,0.5])\n",
    "#img = img.cpu().numpy()\n",
    "img = (img + mean) * std\n",
    "img = np.clip(img, a_max=1, a_min=0)\n",
    "\n",
    "plt.imshow(class_activation, cmap='jet',alpha=1)\n",
    "plt.imshow(img, alpha=0.55)\n",
    "plt.title(dset['val'].classes[pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/home/jimi/dissertacao/covid19/datasets/80-20/val/pneumonia/100.jpg'\n",
    "img = image_loader(image_path).cuda()\n",
    "\n",
    "acts = LayerActivations(model_ft)\n",
    "\n",
    "logps = model_ft(img).cuda()\n",
    "ps = torch.exp(logps)\n",
    "\n",
    "out_features = acts.features[0]\n",
    "out_features = torch.squeeze(out_features, dim=0)\n",
    "out_features = out_features.cpu().detach().numpy()\n",
    "out_features = np.transpose(out_features,axes=(1,2,0))\n",
    "\n",
    "W = model_ft.fc[0].weight\n",
    "top_probs, top_classes = torch.topk(ps, k=3)\n",
    "ps = ps.cpu().detach().numpy()\n",
    "pred = np.argmax(ps)\n",
    "w = W[pred,:]\n",
    "\n",
    "w = w.cpu().detach().numpy()\n",
    "cam = np.dot(out_features, w)\n",
    "#type(w)\n",
    "\n",
    "class_activation = nd.zoom(cam, zoom=(32,32),order=1)\n",
    "\n",
    "img = torch.squeeze(img,0)\n",
    "img = img.cpu().numpy()\n",
    "img = np.transpose(img,(1,2,0))\n",
    "mean = np.array([0.5,0.5,0.5])\n",
    "std =  np.array([0.5,0.5,0.5])\n",
    "#img = img.cpu().numpy()\n",
    "img = (img + mean) * std\n",
    "img = np.clip(img, a_max=1, a_min=0)\n",
    "\n",
    "plt.imshow(class_activation, cmap='jet',alpha=1)\n",
    "plt.imshow(img, alpha=0.55)\n",
    "plt.title(dset['val'].classes[pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/home/jimi/dissertacao/covid19/datasets/80-20/val/pneumonia/3104.jpg'\n",
    "img = image_loader(image_path).cuda()\n",
    "\n",
    "acts = LayerActivations(model_ft)\n",
    "\n",
    "logps = model_ft(img).cuda()\n",
    "ps = torch.exp(logps)\n",
    "\n",
    "out_features = acts.features[0]\n",
    "out_features = torch.squeeze(out_features, dim=0)\n",
    "out_features = out_features.cpu().detach().numpy()\n",
    "out_features = np.transpose(out_features,axes=(1,2,0))\n",
    "\n",
    "W = model_ft.fc[0].weight\n",
    "top_probs, top_classes = torch.topk(ps, k=3)\n",
    "ps = ps.cpu().detach().numpy()\n",
    "pred = np.argmax(ps)\n",
    "w = W[pred,:]\n",
    "\n",
    "w = w.cpu().detach().numpy()\n",
    "cam = np.dot(out_features, w)\n",
    "#type(w)\n",
    "\n",
    "class_activation = nd.zoom(cam, zoom=(32,32),order=1)\n",
    "\n",
    "img = torch.squeeze(img,0)\n",
    "img = img.cpu().numpy()\n",
    "img = np.transpose(img,(1,2,0))\n",
    "mean = np.array([0.5,0.5,0.5])\n",
    "std =  np.array([0.5,0.5,0.5])\n",
    "#img = img.cpu().numpy()\n",
    "img = (img + mean) * std\n",
    "img = np.clip(img, a_max=1, a_min=0)\n",
    "\n",
    "plt.imshow(class_activation, cmap='jet',alpha=1)\n",
    "plt.imshow(img, alpha=0.55)\n",
    "plt.title(dset['val'].classes[pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 2 FIIMMMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 2 FIIMMMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 2 FIIMMMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 3 ref - https://github.com/ironWolf1990/pytorch-covid19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 3 ref- https://github.com/ironWolf1990/pytorch-covid19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 3 ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 3 ref- https://github.com/ironWolf1990/pytorch-covid19\n",
    "########## CAM try 3 ref\n",
    "########## CAM try 3 ref\n",
    "########## CAM try 3 ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm, gridspec\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "def plot_with_labels(lowDWeights, labels):\n",
    "    plt.cla()\n",
    "    X, Y = lowDWeights[:, 0], lowDWeights[:, 1]\n",
    "    for x, y, s in zip(X, Y, labels):\n",
    "        cmap = cm.get_cmap(\"rainbow\")\n",
    "        c = cmap(int(255 * s / 9))\n",
    "        plt.text(x, y, s, backgroundcolor=c, fontsize=9)\n",
    "    plt.xlim(X.min(), X.max())\n",
    "    plt.ylim(Y.min(), Y.max())\n",
    "    plt.show()\n",
    "    plt.pause(0.01)\n",
    "\n",
    "def data_viz(layer, label):\n",
    "    # https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents-notebooks/401_CNN.ipynb\n",
    "    tsne = TSNE(perplexity=30, n_components=2, init=\"pca\", n_iter=5000)\n",
    "    plot_only = 500\n",
    "    low_dim_embs = tsne.fit_transform(layer.data.numpy()[:plot_only, :])\n",
    "    labels = label.numpy()[:plot_only]\n",
    "    plot_with_labels(low_dim_embs, labels)\n",
    "\n",
    "\n",
    "def plot_test_image_result(img, ps, le, cam=None):\n",
    "\n",
    "    _ = plt.figure(figsize=(8, 6))\n",
    "    gs = gridspec.GridSpec(1, 2, width_ratios=[3, 1])\n",
    "    ax1, ax2 = plt.subplot(gs[0]), plt.subplot(gs[1])\n",
    "\n",
    "    if cam is not None:\n",
    "        ax1.imshow(cam, alpha=0.6)\n",
    "        ax1.imshow(img, alpha=0.4)\n",
    "    else:\n",
    "        ax1.imshow(img)\n",
    "\n",
    "    ax2.barh(np.arange(len(ps)), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(len(ps)))\n",
    "\n",
    "    for i, v in enumerate(ps):\n",
    "        ax2.text(\n",
    "            .01,\n",
    "            i-0.1,\n",
    "            f'{v:.3f}',\n",
    "            color='blue',\n",
    "            fontweight='bold')\n",
    "\n",
    "    if le is None:\n",
    "        ax2.set_yticklabels(np.arange(len(ps)))\n",
    "    else:\n",
    "        ax2.set_yticklabels(le.inverse_transform(np.arange(len(ps))))\n",
    "\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makedataset.py\n",
    "\n",
    "from os.path import isfile, join\n",
    "from os import walk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import tee\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    path = \"./data\"\n",
    "    sample_per_category = 500\n",
    "    seed = 24\n",
    "    split_frac = 0.20\n",
    "\n",
    "    df_raw = None\n",
    "\n",
    "    genFiles = (\n",
    "        (dirpath, dirnames, filenames) for (dirpath, dirnames, filenames) in walk(path)\n",
    "    )\n",
    "\n",
    "    files, genFiles = tee(genFiles)\n",
    "    file_count = sum(len(f) for _, _, f in files)\n",
    "\n",
    "    df_raw = pd.DataFrame(\n",
    "        data=np.nan, index=np.arange(0, file_count - 1), columns=[\"LABEL\", \"FILE\"]\n",
    "    )\n",
    "\n",
    "    files, genFiles = tee(genFiles)\n",
    "    idx = 0\n",
    "    for r, _, f in files:\n",
    "        for _f in f:\n",
    "            if isfile(join(r, _f)) and _f.endswith(\n",
    "                (\".jpeg\", \".png\", \"jpg\", \".JPEG\", \".PNG\", \"JPG\")\n",
    "            ):\n",
    "                path = \"/\".join((r, _f))\n",
    "                *_, label = r.split(\"/\")\n",
    "                df_raw.iloc[idx] = [label, path]\n",
    "                idx += 1\n",
    "\n",
    "    df_raw.to_csv(\"./data/raw.csv\", index=False)\n",
    "\n",
    "    # # 3-class\n",
    "    df_main = pd.DataFrame(\n",
    "        data=np.nan,\n",
    "        index=np.arange(0, sample_per_category * 3),\n",
    "        columns=[\"FILE\", \"LABEL\"],\n",
    "    )\n",
    "\n",
    "    df_main = df_raw.groupby(\"LABEL\").apply(\n",
    "        lambda s: s.sample(n=min(len(s), sample_per_category), random_state=seed)\n",
    "    )\n",
    "\n",
    "    train_df, test_df = train_test_split(\n",
    "        df_main, random_state=seed, test_size=split_frac, shuffle=True\n",
    "    )\n",
    "\n",
    "    train_df.to_csv(\"./data/3_class_train_df.csv\", index=False)\n",
    "    test_df.to_csv(\"./data/3_class_test_df.csv\", index=False)\n",
    "\n",
    "    print(f\"\\n3_class_train_df:\\n{train_df['LABEL'].value_counts()}\")\n",
    "    print(f\"3_class_test_df:\\n{test_df['LABEL'].value_counts()}\")\n",
    "\n",
    "    # 2-class\n",
    "    df_main = pd.DataFrame(\n",
    "        data=np.nan,\n",
    "        index=np.arange(0, sample_per_category * 2),\n",
    "        columns=[\"FILE\", \"LABEL\"],\n",
    "    )\n",
    "\n",
    "    index = df_raw[df_raw[\"LABEL\"] == \"pneumonia\"].index\n",
    "    df_raw.drop(index, inplace=True)\n",
    "\n",
    "    df_main = df_raw.groupby(\"LABEL\").apply(\n",
    "        lambda s: s.sample(n=min(len(s), sample_per_category), random_state=seed)\n",
    "    )\n",
    "\n",
    "    train_df, test_df = train_test_split(\n",
    "        df_main, random_state=seed, test_size=split_frac, shuffle=True\n",
    "    )\n",
    "\n",
    "    train_df.to_csv(\"./data/2_class_train_df.csv\", index=False)\n",
    "    test_df.to_csv(\"./data/2_class_test_df.csv\", index=False)\n",
    "\n",
    "    print(f\"\\n2_class_train_df:\\n{train_df['LABEL'].value_counts()}\")\n",
    "    print(f\"2_class_test_df:\\n{test_df['LABEL'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# architectures.py\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class Rn50(nn.Module):\n",
    "    def __init__(self, device, train_base=False, classes=2):\n",
    "        super(Rn50, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.net_back = resnet50(pretrained=True).to(self.device)\n",
    "        self._trainable(train_base)\n",
    "\n",
    "        fc_size = self.net_back.fc.in_features\n",
    "        self.net_back.fc = Identity()\n",
    "\n",
    "        self.net_head = nn.Sequential(\n",
    "            nn.Linear(in_features=fc_size, out_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=1024, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=512, out_features=classes),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        ).to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net_back(x.to(self.device))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.net_head(x.to(self.device))\n",
    "\n",
    "    def _trainable(self, flag):\n",
    "        for param in self.net_back.parameters():\n",
    "            param.requires_grad = flag\n",
    "\n",
    "\n",
    "# old way\n",
    "# modules = list(resnet50(pretrained=True).children())[:-1]\n",
    "# self.net_back = nn.Sequential(*modules).to(self.device)\n",
    "# fc_size = list(self.net_back.parameters())[-1].size(0)\n",
    "# self.net_head = nn.Sequential(...).to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activationmap.py\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class FeatureBuffer():\n",
    "\n",
    "    features=None\n",
    "\n",
    "    def __init__(self, m):\n",
    "        self.hook = m.register_forward_hook(self.hook_fn)\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        # self.features = ((output.cpu()).data).numpy()\n",
    "        self.features = output\n",
    "\n",
    "    def remove(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "\n",
    "# def GradCam(model, input_image_tensor):\n",
    "\n",
    "#     https://github.com/tyui592/class_activation_map/blob/master/cam.py\n",
    "#     https://github.com/daixiangzi/Grad_Cam-pytorch-resnet50/blob/578db29d13b0e7d17aa53d9bac116674771618ec/test_grad_cam.py#L19\n",
    "#     https://snappishproductions.com/blog/2018/01/03/class-activation-mapping-in-pytorch.html.html\n",
    "#     https://github.com/MarcoCBA/Class-Activation-Maps-PyTorch/blob/master/class_activation_maps.ipynb\n",
    "\n",
    "#     print(model)\n",
    "\n",
    "#     final_conv_layer = model.net_back._modules.get('layer4')\n",
    "#     fc_layer = model.net_head._modules.get('0')\n",
    "#     fb = FeatureBuffer(final_conv_layer)\n",
    "\n",
    "#     model = model.eval()\n",
    "#     out = model(input_image_tensor)\n",
    "\n",
    "#      # based on model caluculate output!!!\n",
    "#     probabilities = torch.exp(out)\n",
    "#     _, predicted = torch.max(probabilities, 1)\n",
    "#     feature_maps = fb.features\n",
    "\n",
    "#     print(\"Output's shape: \", out.shape)\n",
    "#     print(\"Feature maps's shape: \", feature_maps.shape)\n",
    "\n",
    "#     weights_and_biases = list(fc_layer.parameters())\n",
    "#     class_weights = weights_and_biases[0][predicted]\n",
    "#     print(\"Weights's shape: \", weights_and_biases[0].shape)\n",
    "#     print(\"Biases's shape: \", weights_and_biases[1].shape)\n",
    "#     print(\"Class weights's shape :\", class_weights.shape)\n",
    "\n",
    "#     class_weights = class_weights.reshape((-1, 1, 1))\n",
    "#     feature_maps = feature_maps.flatten(start_dim=0, end_dim=1)\n",
    "#     print(\"Class weights's shape :\", class_weights.shape)\n",
    "#     print(\"Feature maps's shape: \", feature_maps.shape)\n",
    "\n",
    "#     class_activation_maps = np.array(torch.sum(feature_maps * class_weights, dim=0).detach(), dtype=np.float32)\n",
    "#     plt.figure(figsize=(6, 6))\n",
    "#     plt.imshow(class_activation_maps)\n",
    "#     plt.show()\n",
    "\n",
    "#     resized_cam = cv2.resize(class_activation_maps, dsize=(224, 224), interpolation=cv2.INTER_LANCZOS4)\n",
    "#     plt.figure(figsize=(6, 6))\n",
    "#     plt.imshow(resized_cam)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "\n",
    "import math\n",
    "from time import time\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    from torchsummary import summary\n",
    "except:\n",
    "    MODELSUMMARY = False\n",
    "else:\n",
    "    MODELSUMMARY = True\n",
    "\n",
    "try:\n",
    "    from sklearn.manifold import TSNE\n",
    "except:\n",
    "    VIZTSNE = False\n",
    "else:\n",
    "    VIZTSNE = True\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    device,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    dataloaders,\n",
    "    dataloader_len,\n",
    "    input_shape,\n",
    "    scheduler=None,\n",
    "    num_epochs=50,\n",
    "):\n",
    "\n",
    "    if MODELSUMMARY:\n",
    "        summary(model, input_data=input_shape)\n",
    "\n",
    "    start = time()\n",
    "    best_model_wts = deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        t_epoch = time()\n",
    "        print(f\"epoch: {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for idx, (labels, inputs) in enumerate(dataloaders[phase]):\n",
    "                iter_batch = math.ceil(\n",
    "                    dataloader_len[phase] / dataloaders[phase].batch_size\n",
    "                )\n",
    "                print(f\"[phase: {phase}] batch: {idx+1}/{iter_batch}\", end=\"\\r\")\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase == \"train\":\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataloader_len[phase]\n",
    "            epoch_acc = running_corrects.double() / dataloader_len[phase]\n",
    "            print(f\"[phase: {phase}] Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "            if phase == \"val\" and epoch_acc > best_acc:\n",
    "                print(f\"[saving model] epoch: {epoch+1} Acc: {epoch_acc:.4f}\")\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = deepcopy(model.state_dict())\n",
    "\n",
    "        t_elapsed = time() - t_epoch\n",
    "        print(f\"epoch training complete in {t_elapsed//60:.0f}m {t_elapsed%60:.0f}s\")\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time() - start\n",
    "    print(f\"training complete in {time_elapsed//60:.0f}m {time_elapsed%60:.0f}s\")\n",
    "    print(f\"best val Acc: {best_acc:4f}\")\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    checkpoint = {\n",
    "        \"input_shape\": input_shape,\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, \"./models/checkpoint.pth\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.py\n",
    "\n",
    "import math\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from scripts.activationmap import FeatureBuffer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#from .utils import plot_test_image_result\n",
    "\n",
    "\n",
    "def test_model(model, testloader, device, encoder=None):\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    true_list = list()\n",
    "    pred_list = list()\n",
    "\n",
    "    for idx, (labels, inputs) in enumerate(testloader):\n",
    "        iter_batch = math.ceil(len(testloader.dataset)/testloader.batch_size)\n",
    "        print(f'[phase: test] batch: {idx+1}/{iter_batch}', end='\\r')\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            probabilities = torch.exp(outputs)\n",
    "            _, predicted = torch.max(probabilities, 1)\n",
    "\n",
    "            total = idx + 1\n",
    "            correct += torch.sum(predicted == labels.data)\n",
    "            true_list.append((labels.data.cpu()).numpy().item())\n",
    "            pred_list.append((predicted.cpu()).numpy().item())\n",
    "\n",
    "    acc = 100*(correct.item()/total)\n",
    "    print(f\"[phase: test] total: {total}, correct: {correct}, acc: {acc:.3f}\")\n",
    "\n",
    "    print(classification_report(tuple(true_list), tuple(pred_list)))\n",
    "\n",
    "    y_true = pd.Series(true_list, name='Actual')\n",
    "    y_pred = pd.Series(pred_list, name='Predicted')\n",
    "    cm = pd.crosstab(y_true, y_pred,  margins=True)\n",
    "\n",
    "    print(\"confusion matrix\")\n",
    "    if encoder is not None:\n",
    "        print({i : encoder.classes_[i] for i in range(0, len(encoder.classes_))})\n",
    "    print(cm)\n",
    "\n",
    "\n",
    "\n",
    "def test_image(model, image, in_shape, transform, device, labelencoder=None, cam=None):\n",
    "    \"\"\"\n",
    "    GradCam\n",
    "    \"\"\"\n",
    "\n",
    "    if cam is not None:\n",
    "        final_conv_layer = model.net_back._modules.get('layer4')\n",
    "        fc_layer = model.net_head._modules.get('0')\n",
    "        fb = FeatureBuffer(final_conv_layer)\n",
    "\n",
    "    input_tensor = transform(image).unsqueeze(0)\n",
    "    inputs = input_tensor.to(device)\n",
    "\n",
    "    model = model.eval()\n",
    "    outputs = model(inputs)\n",
    "    probabilities = torch.exp(outputs)\n",
    "    prob = (probabilities.cpu()).detach().numpy().flatten()\n",
    "\n",
    "    if cam is not None:\n",
    "        _, predicted = torch.max(probabilities, 1)\n",
    "        feature_maps = fb.features\n",
    "\n",
    "        weights_and_biases = list(fc_layer.parameters())\n",
    "        class_weights = weights_and_biases[0][predicted]\n",
    "\n",
    "        class_weights = class_weights.reshape((-1, 1, 1))\n",
    "        feature_maps = feature_maps.flatten(start_dim=0, end_dim=1)\n",
    "\n",
    "        class_activation_maps = np.array(\n",
    "            torch.sum(feature_maps * class_weights, dim=0).cpu().detach(),\n",
    "            dtype=np.float32)\n",
    "\n",
    "        cam_map = cv2.resize(\n",
    "            class_activation_maps,\n",
    "            dsize=in_shape,\n",
    "            interpolation=cv2.INTER_LANCZOS4)\n",
    "\n",
    "    if cam is not None:\n",
    "        plot_test_image_result(image.resize(in_shape), prob, labelencoder, cam_map)\n",
    "    else:\n",
    "        plot_test_image_result(image, prob, labelencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate.py \n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "from scripts.datagen import Datagen\n",
    "from scripts.architectures import Rn50\n",
    "from scripts.test import test_model, test_image\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda:0\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "test_file = \"data/3_class_test_df.csv\"\n",
    "image_file = \"data/raw/covid/covid_001.jpg\"\n",
    "num_workers = 2\n",
    "batch_size = 1\n",
    "input_shape = (256, 256)\n",
    "le = LabelEncoder()\n",
    "\n",
    "df = pd.read_csv(test_file)\n",
    "\n",
    "test_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(input_shape),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_set = Datagen(df, l_encoder=le, transforms=test_transforms)\n",
    "label_enc = test_set.get_le()\n",
    "device = get_device()\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set, batch_size=batch_size, num_workers=num_workers,\n",
    ")\n",
    "\n",
    "model = Rn50(device=device, classes=3)\n",
    "model.load_state_dict(torch.load(\"./models/checkpoint.pth\")[\"state_dict\"])\n",
    "\n",
    "test_model(\n",
    "    model=model,\n",
    "    testloader=test_loader,\n",
    "    device=device,\n",
    "    encoder=label_enc)\n",
    "\n",
    "input_image = Image.open(image_file).convert(\"RGB\")\n",
    "test_image(\n",
    "    model=model,\n",
    "    image=input_image,\n",
    "    in_shape=input_shape,\n",
    "    transform=test_transforms,\n",
    "    device=device,\n",
    "    labelencoder=label_enc,\n",
    "    cam=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datagen.py\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class Datagen(Dataset):\n",
    "    def __init__(self, dataframe, transforms=None, l_encoder=None):\n",
    "        self.df = dataframe\n",
    "        self.transforms = transforms\n",
    "        self.encoder = l_encoder\n",
    "\n",
    "        if self.encoder is not None:\n",
    "            self.df[\"LABEL\"] = self.encoder.fit_transform(self.df[\"LABEL\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        label = self.df.iloc[idx, 0]\n",
    "        image_file = self.df.iloc[idx, 1]\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        # print(image.shape)\n",
    "        return (label, image)\n",
    "\n",
    "    def get_le(self):\n",
    "        return self.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from scripts.datagen import Datagen\n",
    "from scripts.architectures import Rn50\n",
    "from scripts.train import train_model\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda:0\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "\n",
    "train_file = \"data/3_class_train_df.csv\"\n",
    "num_workers = 2\n",
    "val_split = 0.2\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "input_shape = (3, 256, 256)\n",
    "le = LabelEncoder()\n",
    "\n",
    "df = pd.read_csv(train_file)\n",
    "\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "validation_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_set = Datagen(df, l_encoder=le, transforms=train_transforms)\n",
    "validation_set = Datagen(df, l_encoder=le, transforms=validation_transforms)\n",
    "\n",
    "train_idx, val_idx = train_test_split(list(range(len(df))), test_size=val_split)\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    # shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    sampler=train_sampler,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    validation_set,\n",
    "    # shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    sampler=valid_sampler,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "device = get_device()\n",
    "net = Rn50(device=device, classes=3)\n",
    "\n",
    "dataloaders = {\"train\": train_loader, \"val\": valid_loader}\n",
    "dataloader_len = {\"train\": len(train_idx), \"val\": len(val_idx)}\n",
    "\n",
    "criteration = nn.NLLLoss()\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "model = train_model(\n",
    "    model=net,\n",
    "    device=device,\n",
    "    criterion=criteration,\n",
    "    optimizer=optimizer,\n",
    "    dataloaders=dataloaders,\n",
    "    dataloader_len=dataloader_len,\n",
    "    input_shape=input_shape,\n",
    "    num_epochs=num_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 3 FIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 3 FIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 3 FIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 3 FIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 3 FIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 3 FIM\n",
    "########## CAM try 3 FIM\n",
    "########## CAM try 3 FIM\n",
    "########## CAM try 3 FIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple implementation of CAM in PyTorch for the networks such as ResNet, DenseNet, SqueezeNet, Inception\n",
    "\n",
    "import io\n",
    "import requests\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pdb\n",
    "\n",
    "# input image\n",
    "LABELS_URL = 'https://s3.amazonaws.com/outcome-blog/imagenet/labels.json'\n",
    "IMG_URL = 'https://diariodonordeste.verdesmares.com.br/image/contentid/policy:1.2966908:1594933666/ferramenta-sesa.jpg'\n",
    "\n",
    "# networks such as googlenet, resnet, densenet already use global average pooling at the end, so CAM could be used directly.\n",
    "model_id = 1\n",
    "if model_id == 1:\n",
    "    net = models.squeezenet1_1(pretrained=True)\n",
    "    finalconv_name = 'features' # this is the last conv layer of the network\n",
    "elif model_id == 2:\n",
    "    net = models.resnet18(pretrained=True)\n",
    "    finalconv_name = 'layer4'\n",
    "elif model_id == 3:\n",
    "    net = models.densenet161(pretrained=True)\n",
    "    finalconv_name = 'features'\n",
    "\n",
    "#net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hook the feature extractor\n",
    "features_blobs = []\n",
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(output.data.cpu().numpy())\n",
    "\n",
    "net._modules.get(finalconv_name).register_forward_hook(hook_feature)\n",
    "\n",
    "# get the softmax weight\n",
    "params = list(net.parameters())\n",
    "weight_softmax = np.squeeze(params[-2].data.numpy())\n",
    "\n",
    "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "    # generate the class activation maps upsample to 256x256\n",
    "    size_upsample = (256, 256)\n",
    "    bz, nc, h, w = feature_conv.shape\n",
    "    output_cam = []\n",
    "    for idx in class_idx:\n",
    "        cam = weight_softmax[idx].dot(feature_conv.reshape((nc, h*w)))\n",
    "        cam = cam.reshape(h, w)\n",
    "        cam = cam - np.min(cam)\n",
    "        cam_img = cam / np.max(cam)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "    return output_cam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(\n",
    "   mean=[0.485, 0.456, 0.406],\n",
    "   std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "preprocess = transforms.Compose([\n",
    "   transforms.Resize((224,224)),\n",
    "   transforms.ToTensor(),\n",
    "   normalize\n",
    "])\n",
    "\n",
    "response = requests.get(IMG_URL)\n",
    "img_pil = Image.open(io.BytesIO(response.content))\n",
    "img_pil.save('test.jpg')\n",
    "\n",
    "img_tensor = preprocess(img_pil)\n",
    "img_variable = Variable(img_tensor.unsqueeze(0))\n",
    "logit = net(img_variable) ## aqui tem algo\n",
    "\n",
    "# # download the imagenet category list\n",
    "classes = {int(key):value for (key, value)\n",
    "          in requests.get(LABELS_URL).json().items()}\n",
    "\n",
    "h_x = F.softmax(logit, dim=1).data.squeeze()\n",
    "probs, idx = h_x.sort(0, True)\n",
    "probs = probs.numpy()\n",
    "idx = idx.numpy()\n",
    "\n",
    "# # output the prediction\n",
    "for i in range(0, 5):\n",
    "    print('{:.3f} -> {}'.format(probs[i], classes[idx[i]]))\n",
    "\n",
    "# generate class activation mapping for the top1 prediction\n",
    "CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[0]])\n",
    "\n",
    "# render the CAM and output\n",
    "print('output CAM.jpg for the top1 prediction: %s'%classes[idx[0]])\n",
    "img = cv2.imread('test.jpg')\n",
    "height, width, _ = img.shape\n",
    "heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
    "result = heatmap * 0.3 + img * 0.5\n",
    "cv2.imwrite('CAM.jpg', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "image_path = 'CAM.jpg'\n",
    "display.display(Image.open(image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
