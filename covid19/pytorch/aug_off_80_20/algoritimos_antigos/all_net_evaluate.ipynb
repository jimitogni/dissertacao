{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hatmap\n",
    "- Melhorar o plot de imagens\n",
    "- Propor algo novo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy \n",
    "import cv2\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import shutil, sys\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = '/home/jimi/dissertacao/covid19/datasets/80-20/'\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "#model_name = \"densenet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "#num_classes = 3\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 16\n",
    "\n",
    "# Number of epochs to train for\n",
    "\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True\n",
    "\n",
    "#num_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "#transforms.RandomRotation(degrees=(-5, 5)),\n",
    "#transforms.ColorJitter(brightness=.02),\n",
    "    \n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x]) for x in ['train', 'val', 'test']}\n",
    "\n",
    "# Create training and validation dataloaders\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
    "                    batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "\n",
    "# trans = ['train','val','test']\n",
    "# categories = ['train','val','test']\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files per classes\n",
      "----------------------------------------\n",
      "normal :  4023\n",
      "pneumonia :  4035\n",
      "covid :  4105\n",
      "--------------------\n",
      "Train, test, validation\n",
      "--------------------\n",
      "len_train_dir :  9632\n",
      "len_test_dir :  31\n",
      "len_val_dir :  2409\n"
     ]
    }
   ],
   "source": [
    "# Path to data\n",
    "data_dir = '/home/jimi/dissertacao/covid19/datasets/80-20/'\n",
    "train_dir = data_dir+'train/'\n",
    "test_dir = data_dir+'test/'\n",
    "val_dir = data_dir+'val/'\n",
    "\n",
    "normal_dir = data_dir+'normal/'\n",
    "pneumonia_dir = data_dir+'pneumonia/'\n",
    "covid_dir = data_dir+'covid/'\n",
    "\n",
    "len_covid = len([iq for iq in os.scandir(normal_dir)])\n",
    "len_normal = len([iq for iq in os.scandir(pneumonia_dir)])\n",
    "len_pneumonia = len([iq for iq in os.scandir(covid_dir)])\n",
    "\n",
    "len_train_dir = len([iq for iq in os.scandir(train_dir+'covid/')]) + len([iq for iq in os.scandir(train_dir+'normal/')]) + len([iq for iq in os.scandir(train_dir+'pneumonia/')])\n",
    "len_test_dir = len([iq for iq in os.scandir(test_dir+'covid/')]) + len([iq for iq in os.scandir(test_dir+'normal/')]) + len([iq for iq in os.scandir(test_dir+'pneumonia/')])\n",
    "len_val_dir = len([iq for iq in os.scandir(val_dir+'covid/')]) + len([iq for iq in os.scandir(val_dir+'normal/')]) + len([iq for iq in os.scandir(val_dir+'pneumonia/')])\n",
    "\n",
    "print('Files per classes')\n",
    "print(\"----\"*10)\n",
    "print(\"normal : \", len_covid)\n",
    "\n",
    "print(\"pneumonia : \", len_normal)\n",
    "\n",
    "print(\"covid : \", len_pneumonia)\n",
    "\n",
    "print(\"-\"*20)\n",
    "\n",
    "print('Train, test, validation')\n",
    "\n",
    "print(\"-\"*20)\n",
    "\n",
    "print(\"len_train_dir : \", len_train_dir)\n",
    "\n",
    "print(\"len_test_dir : \", len_test_dir)\n",
    "\n",
    "print(\"len_val_dir : \", len_val_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs, model_name, lr, batch_size):\n",
    "    since = time.time()\n",
    "    is_inception = False\n",
    "    \n",
    "    #tensorboard\n",
    "    writer = SummaryWriter(f'runs/dg_{model_name}_lr={lr}_epoch={num_epochs}_batch_size={batch_size}')\n",
    "    step = 0\n",
    "\n",
    "    val_acc_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        \n",
    "        print('-' * 10)\n",
    "        \n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "\n",
    "            writer.add_scalar('training loss', loss, global_step=step)\n",
    "            writer.add_scalar('training accuracy', epoch_acc, global_step=step)\n",
    "            step += 1\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "                val_loss_history.append(epoch_loss)\n",
    "\n",
    "        print()\n",
    "        \n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print()\n",
    "\n",
    "    print('#'*30)\n",
    "\n",
    "    print('------ Summary ------')\n",
    "\n",
    "    print(f'model -> {_model}')\n",
    "\n",
    "    print(f'epochs -> {_epochs}')\n",
    "\n",
    "    print(f'lr -> {_lrs}')\n",
    "\n",
    "    print(f'batch size -> {_batch}')\n",
    "\n",
    "    print()\n",
    "\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    print('#'*30)\n",
    "    \n",
    "    plt.figure(figsize=(13, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(val_acc_history, label=\"Validation_Accuracy\")\n",
    "    plt.plot(val_loss_history, label=\"Validation_Loss\")\n",
    "    plt.title('Accuracy and Loss in Validation Dataset')\n",
    "    plt.legend()\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.plot(loss, label=\"loss\")\n",
    "#     plt.plot(phase, label=\"phase\")\n",
    "#     plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('hist_'+_model+'.png')\n",
    "    plt.show()\n",
    "\n",
    "    print()\n",
    "\n",
    "    print('==== END ====')\n",
    "\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=4):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 3, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools \n",
    "\n",
    "def plot_confusion_matrix(cm, classes, net,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig('cm_'+net+'.png')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "==== INITIALIZING WITH PARAMETERS: ====\n",
      "model -> squeezenet\n",
      "epochs -> 300\n",
      "lr -> 0.0001\n",
      "batch size -> 8\n",
      "\n",
      "--------------------\n",
      "Params to learn:\n",
      "\t classifier.1.weight\n",
      "\t classifier.1.bias\n",
      "\n",
      "--------------------\n",
      "\n",
      "== Epochs ==\n",
      "Epoch 0/299\n",
      "----------\n",
      "train Loss: 0.5270 Acc: 0.8083\n",
      "val Loss: 0.3153 Acc: 0.9049\n",
      "\n",
      "Epoch 1/299\n",
      "----------\n",
      "train Loss: 0.3022 Acc: 0.9069\n",
      "val Loss: 0.2393 Acc: 0.9348\n",
      "\n",
      "Epoch 2/299\n",
      "----------\n",
      "train Loss: 0.2558 Acc: 0.9141\n",
      "val Loss: 0.2063 Acc: 0.9444\n",
      "\n",
      "Epoch 3/299\n",
      "----------\n",
      "train Loss: 0.2330 Acc: 0.9263\n",
      "val Loss: 0.1941 Acc: 0.9477\n",
      "\n",
      "Epoch 4/299\n",
      "----------\n",
      "train Loss: 0.2163 Acc: 0.9287\n",
      "val Loss: 0.1923 Acc: 0.9435\n",
      "\n",
      "Epoch 5/299\n",
      "----------\n",
      "train Loss: 0.2055 Acc: 0.9338\n",
      "val Loss: 0.1700 Acc: 0.9489\n",
      "\n",
      "Epoch 6/299\n",
      "----------\n",
      "train Loss: 0.1945 Acc: 0.9378\n",
      "val Loss: 0.1634 Acc: 0.9539\n",
      "\n",
      "Epoch 7/299\n",
      "----------\n",
      "train Loss: 0.1876 Acc: 0.9372\n",
      "val Loss: 0.1564 Acc: 0.9543\n",
      "\n",
      "Epoch 8/299\n",
      "----------\n",
      "train Loss: 0.1830 Acc: 0.9387\n",
      "val Loss: 0.1582 Acc: 0.9523\n",
      "\n",
      "Epoch 9/299\n",
      "----------\n",
      "train Loss: 0.1788 Acc: 0.9413\n",
      "val Loss: 0.1569 Acc: 0.9535\n",
      "\n",
      "Epoch 10/299\n",
      "----------\n",
      "train Loss: 0.1742 Acc: 0.9410\n",
      "val Loss: 0.1426 Acc: 0.9589\n",
      "\n",
      "Epoch 11/299\n",
      "----------\n",
      "train Loss: 0.1722 Acc: 0.9406\n",
      "val Loss: 0.1389 Acc: 0.9589\n",
      "\n",
      "Epoch 12/299\n",
      "----------\n",
      "train Loss: 0.1673 Acc: 0.9441\n",
      "val Loss: 0.1373 Acc: 0.9593\n",
      "\n",
      "Epoch 13/299\n",
      "----------\n",
      "train Loss: 0.1663 Acc: 0.9440\n",
      "val Loss: 0.1348 Acc: 0.9577\n",
      "\n",
      "Epoch 14/299\n",
      "----------\n",
      "train Loss: 0.1633 Acc: 0.9434\n",
      "val Loss: 0.1325 Acc: 0.9610\n",
      "\n",
      "Epoch 15/299\n",
      "----------\n",
      "train Loss: 0.1629 Acc: 0.9446\n",
      "val Loss: 0.1294 Acc: 0.9606\n",
      "\n",
      "Epoch 16/299\n",
      "----------\n",
      "train Loss: 0.1572 Acc: 0.9468\n",
      "val Loss: 0.1317 Acc: 0.9618\n",
      "\n",
      "Epoch 17/299\n",
      "----------\n",
      "train Loss: 0.1587 Acc: 0.9458\n",
      "val Loss: 0.1256 Acc: 0.9618\n",
      "\n",
      "Epoch 18/299\n",
      "----------\n",
      "train Loss: 0.1531 Acc: 0.9481\n",
      "val Loss: 0.1258 Acc: 0.9622\n",
      "\n",
      "Epoch 19/299\n",
      "----------\n",
      "train Loss: 0.1547 Acc: 0.9469\n",
      "val Loss: 0.1227 Acc: 0.9610\n",
      "\n",
      "Epoch 20/299\n",
      "----------\n",
      "train Loss: 0.1510 Acc: 0.9496\n",
      "val Loss: 0.1253 Acc: 0.9626\n",
      "\n",
      "Epoch 21/299\n",
      "----------\n",
      "train Loss: 0.1481 Acc: 0.9486\n",
      "val Loss: 0.1206 Acc: 0.9622\n",
      "\n",
      "Epoch 22/299\n",
      "----------\n",
      "train Loss: 0.1491 Acc: 0.9492\n",
      "val Loss: 0.1196 Acc: 0.9626\n",
      "\n",
      "Epoch 23/299\n",
      "----------\n",
      "train Loss: 0.1476 Acc: 0.9510\n",
      "val Loss: 0.1181 Acc: 0.9635\n",
      "\n",
      "Epoch 24/299\n",
      "----------\n",
      "train Loss: 0.1441 Acc: 0.9523\n",
      "val Loss: 0.1164 Acc: 0.9643\n",
      "\n",
      "Epoch 25/299\n",
      "----------\n",
      "train Loss: 0.1441 Acc: 0.9509\n",
      "val Loss: 0.1187 Acc: 0.9606\n",
      "\n",
      "Epoch 26/299\n",
      "----------\n",
      "train Loss: 0.1433 Acc: 0.9508\n",
      "val Loss: 0.1140 Acc: 0.9631\n",
      "\n",
      "Epoch 27/299\n",
      "----------\n",
      "train Loss: 0.1453 Acc: 0.9506\n",
      "val Loss: 0.1142 Acc: 0.9647\n",
      "\n",
      "Epoch 28/299\n",
      "----------\n",
      "train Loss: 0.1418 Acc: 0.9531\n",
      "val Loss: 0.1126 Acc: 0.9639\n",
      "\n",
      "Epoch 29/299\n",
      "----------\n",
      "train Loss: 0.1393 Acc: 0.9523\n",
      "val Loss: 0.1115 Acc: 0.9651\n",
      "\n",
      "Epoch 30/299\n",
      "----------\n",
      "train Loss: 0.1395 Acc: 0.9517\n",
      "val Loss: 0.1157 Acc: 0.9622\n",
      "\n",
      "Epoch 31/299\n",
      "----------\n",
      "train Loss: 0.1411 Acc: 0.9522\n",
      "val Loss: 0.1103 Acc: 0.9655\n",
      "\n",
      "Epoch 32/299\n",
      "----------\n",
      "train Loss: 0.1401 Acc: 0.9541\n",
      "val Loss: 0.1095 Acc: 0.9647\n",
      "\n",
      "Epoch 33/299\n",
      "----------\n",
      "train Loss: 0.1366 Acc: 0.9548\n",
      "val Loss: 0.1151 Acc: 0.9618\n",
      "\n",
      "Epoch 34/299\n",
      "----------\n",
      "train Loss: 0.1331 Acc: 0.9548\n",
      "val Loss: 0.1103 Acc: 0.9647\n",
      "\n",
      "Epoch 35/299\n",
      "----------\n",
      "train Loss: 0.1363 Acc: 0.9537\n",
      "val Loss: 0.1075 Acc: 0.9647\n",
      "\n",
      "Epoch 36/299\n",
      "----------\n",
      "train Loss: 0.1364 Acc: 0.9513\n",
      "val Loss: 0.1091 Acc: 0.9660\n",
      "\n",
      "Epoch 37/299\n",
      "----------\n",
      "train Loss: 0.1358 Acc: 0.9548\n",
      "val Loss: 0.1060 Acc: 0.9664\n",
      "\n",
      "Epoch 38/299\n",
      "----------\n",
      "train Loss: 0.1318 Acc: 0.9540\n",
      "val Loss: 0.1102 Acc: 0.9639\n",
      "\n",
      "Epoch 39/299\n",
      "----------\n",
      "train Loss: 0.1330 Acc: 0.9536\n",
      "val Loss: 0.1060 Acc: 0.9676\n",
      "\n",
      "Epoch 40/299\n",
      "----------\n",
      "train Loss: 0.1333 Acc: 0.9534\n",
      "val Loss: 0.1047 Acc: 0.9668\n",
      "\n",
      "Epoch 41/299\n",
      "----------\n",
      "train Loss: 0.1319 Acc: 0.9538\n",
      "val Loss: 0.1058 Acc: 0.9680\n",
      "\n",
      "Epoch 42/299\n",
      "----------\n",
      "train Loss: 0.1318 Acc: 0.9539\n",
      "val Loss: 0.1057 Acc: 0.9639\n",
      "\n",
      "Epoch 43/299\n",
      "----------\n",
      "train Loss: 0.1335 Acc: 0.9548\n",
      "val Loss: 0.1035 Acc: 0.9647\n",
      "\n",
      "Epoch 44/299\n",
      "----------\n",
      "train Loss: 0.1284 Acc: 0.9561\n",
      "val Loss: 0.1027 Acc: 0.9660\n",
      "\n",
      "Epoch 45/299\n",
      "----------\n",
      "train Loss: 0.1334 Acc: 0.9531\n",
      "val Loss: 0.1036 Acc: 0.9655\n",
      "\n",
      "Epoch 46/299\n",
      "----------\n",
      "train Loss: 0.1325 Acc: 0.9566\n",
      "val Loss: 0.1024 Acc: 0.9660\n",
      "\n",
      "Epoch 47/299\n",
      "----------\n",
      "train Loss: 0.1286 Acc: 0.9573\n",
      "val Loss: 0.1037 Acc: 0.9643\n",
      "\n",
      "Epoch 48/299\n",
      "----------\n",
      "train Loss: 0.1258 Acc: 0.9555\n",
      "val Loss: 0.1013 Acc: 0.9676\n",
      "\n",
      "Epoch 49/299\n",
      "----------\n",
      "train Loss: 0.1268 Acc: 0.9556\n",
      "val Loss: 0.1068 Acc: 0.9664\n",
      "\n",
      "Epoch 50/299\n",
      "----------\n",
      "train Loss: 0.1287 Acc: 0.9571\n",
      "val Loss: 0.1014 Acc: 0.9680\n",
      "\n",
      "Epoch 51/299\n",
      "----------\n",
      "train Loss: 0.1230 Acc: 0.9578\n",
      "val Loss: 0.1034 Acc: 0.9685\n",
      "\n",
      "Epoch 52/299\n",
      "----------\n",
      "train Loss: 0.1241 Acc: 0.9565\n",
      "val Loss: 0.0993 Acc: 0.9668\n",
      "\n",
      "Epoch 53/299\n",
      "----------\n",
      "train Loss: 0.1254 Acc: 0.9585\n",
      "val Loss: 0.1013 Acc: 0.9676\n",
      "\n",
      "Epoch 54/299\n",
      "----------\n",
      "train Loss: 0.1291 Acc: 0.9558\n",
      "val Loss: 0.1006 Acc: 0.9655\n",
      "\n",
      "Epoch 55/299\n",
      "----------\n",
      "train Loss: 0.1266 Acc: 0.9566\n",
      "val Loss: 0.1036 Acc: 0.9664\n",
      "\n",
      "Epoch 56/299\n",
      "----------\n",
      "train Loss: 0.1243 Acc: 0.9565\n",
      "val Loss: 0.0983 Acc: 0.9660\n",
      "\n",
      "Epoch 57/299\n",
      "----------\n",
      "train Loss: 0.1241 Acc: 0.9585\n",
      "val Loss: 0.1038 Acc: 0.9647\n",
      "\n",
      "Epoch 58/299\n",
      "----------\n",
      "train Loss: 0.1228 Acc: 0.9585\n",
      "val Loss: 0.0984 Acc: 0.9685\n",
      "\n",
      "Epoch 59/299\n",
      "----------\n",
      "train Loss: 0.1202 Acc: 0.9589\n",
      "val Loss: 0.1136 Acc: 0.9601\n",
      "\n",
      "Epoch 60/299\n",
      "----------\n",
      "train Loss: 0.1180 Acc: 0.9583\n",
      "val Loss: 0.1017 Acc: 0.9660\n",
      "\n",
      "Epoch 61/299\n",
      "----------\n",
      "train Loss: 0.1230 Acc: 0.9588\n",
      "val Loss: 0.0984 Acc: 0.9697\n",
      "\n",
      "Epoch 62/299\n",
      "----------\n",
      "train Loss: 0.1247 Acc: 0.9565\n",
      "val Loss: 0.0969 Acc: 0.9680\n",
      "\n",
      "Epoch 63/299\n",
      "----------\n",
      "train Loss: 0.1234 Acc: 0.9569\n",
      "val Loss: 0.0966 Acc: 0.9668\n",
      "\n",
      "Epoch 64/299\n",
      "----------\n",
      "train Loss: 0.1215 Acc: 0.9594\n",
      "val Loss: 0.0955 Acc: 0.9705\n",
      "\n",
      "Epoch 65/299\n",
      "----------\n",
      "train Loss: 0.1198 Acc: 0.9600\n",
      "val Loss: 0.1016 Acc: 0.9668\n",
      "\n",
      "Epoch 66/299\n",
      "----------\n",
      "train Loss: 0.1203 Acc: 0.9583\n",
      "val Loss: 0.0968 Acc: 0.9680\n",
      "\n",
      "Epoch 67/299\n",
      "----------\n",
      "train Loss: 0.1207 Acc: 0.9590\n",
      "val Loss: 0.1089 Acc: 0.9626\n",
      "\n",
      "Epoch 68/299\n",
      "----------\n",
      "train Loss: 0.1188 Acc: 0.9588\n",
      "val Loss: 0.1013 Acc: 0.9655\n",
      "\n",
      "Epoch 69/299\n",
      "----------\n",
      "train Loss: 0.1196 Acc: 0.9599\n",
      "val Loss: 0.0972 Acc: 0.9664\n",
      "\n",
      "Epoch 70/299\n",
      "----------\n",
      "train Loss: 0.1191 Acc: 0.9595\n",
      "val Loss: 0.0973 Acc: 0.9705\n",
      "\n",
      "Epoch 71/299\n",
      "----------\n",
      "train Loss: 0.1171 Acc: 0.9603\n",
      "val Loss: 0.0949 Acc: 0.9705\n",
      "\n",
      "Epoch 72/299\n",
      "----------\n",
      "train Loss: 0.1180 Acc: 0.9593\n",
      "val Loss: 0.0944 Acc: 0.9660\n",
      "\n",
      "Epoch 73/299\n",
      "----------\n",
      "train Loss: 0.1199 Acc: 0.9577\n",
      "val Loss: 0.1146 Acc: 0.9610\n",
      "\n",
      "Epoch 74/299\n",
      "----------\n",
      "train Loss: 0.1201 Acc: 0.9577\n",
      "val Loss: 0.0952 Acc: 0.9672\n",
      "\n",
      "Epoch 75/299\n",
      "----------\n",
      "train Loss: 0.1203 Acc: 0.9611\n",
      "val Loss: 0.0974 Acc: 0.9693\n",
      "\n",
      "Epoch 76/299\n",
      "----------\n",
      "train Loss: 0.1173 Acc: 0.9597\n",
      "val Loss: 0.0938 Acc: 0.9680\n",
      "\n",
      "Epoch 77/299\n",
      "----------\n",
      "train Loss: 0.1195 Acc: 0.9583\n",
      "val Loss: 0.0933 Acc: 0.9685\n",
      "\n",
      "Epoch 78/299\n",
      "----------\n",
      "train Loss: 0.1184 Acc: 0.9603\n",
      "val Loss: 0.0933 Acc: 0.9709\n",
      "\n",
      "Epoch 79/299\n",
      "----------\n",
      "train Loss: 0.1142 Acc: 0.9619\n",
      "val Loss: 0.0929 Acc: 0.9680\n",
      "\n",
      "Epoch 80/299\n",
      "----------\n",
      "train Loss: 0.1157 Acc: 0.9583\n",
      "val Loss: 0.0975 Acc: 0.9664\n",
      "\n",
      "Epoch 81/299\n",
      "----------\n",
      "train Loss: 0.1151 Acc: 0.9602\n",
      "val Loss: 0.0926 Acc: 0.9714\n",
      "\n",
      "Epoch 82/299\n",
      "----------\n",
      "train Loss: 0.1154 Acc: 0.9595\n",
      "val Loss: 0.0919 Acc: 0.9668\n",
      "\n",
      "Epoch 83/299\n",
      "----------\n",
      "train Loss: 0.1134 Acc: 0.9620\n",
      "val Loss: 0.0954 Acc: 0.9680\n",
      "\n",
      "Epoch 84/299\n",
      "----------\n",
      "train Loss: 0.1140 Acc: 0.9620\n",
      "val Loss: 0.0906 Acc: 0.9726\n",
      "\n",
      "Epoch 85/299\n",
      "----------\n",
      "train Loss: 0.1162 Acc: 0.9593\n",
      "val Loss: 0.0906 Acc: 0.9718\n",
      "\n",
      "Epoch 86/299\n",
      "----------\n",
      "train Loss: 0.1173 Acc: 0.9589\n",
      "val Loss: 0.0908 Acc: 0.9714\n",
      "\n",
      "Epoch 87/299\n",
      "----------\n",
      "train Loss: 0.1144 Acc: 0.9602\n",
      "val Loss: 0.0905 Acc: 0.9701\n",
      "\n",
      "Epoch 88/299\n",
      "----------\n",
      "train Loss: 0.1164 Acc: 0.9592\n",
      "val Loss: 0.0941 Acc: 0.9697\n",
      "\n",
      "Epoch 89/299\n",
      "----------\n",
      "train Loss: 0.1173 Acc: 0.9580\n",
      "val Loss: 0.0939 Acc: 0.9664\n",
      "\n",
      "Epoch 90/299\n",
      "----------\n",
      "train Loss: 0.1136 Acc: 0.9589\n",
      "val Loss: 0.1059 Acc: 0.9626\n",
      "\n",
      "Epoch 91/299\n",
      "----------\n",
      "train Loss: 0.1143 Acc: 0.9610\n",
      "val Loss: 0.0910 Acc: 0.9685\n",
      "\n",
      "Epoch 92/299\n",
      "----------\n",
      "train Loss: 0.1158 Acc: 0.9585\n",
      "val Loss: 0.0898 Acc: 0.9697\n",
      "\n",
      "Epoch 93/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1153 Acc: 0.9605\n",
      "val Loss: 0.0918 Acc: 0.9714\n",
      "\n",
      "Epoch 94/299\n",
      "----------\n",
      "train Loss: 0.1113 Acc: 0.9610\n",
      "val Loss: 0.0888 Acc: 0.9714\n",
      "\n",
      "Epoch 95/299\n",
      "----------\n",
      "train Loss: 0.1147 Acc: 0.9588\n",
      "val Loss: 0.0897 Acc: 0.9730\n",
      "\n",
      "Epoch 96/299\n",
      "----------\n",
      "train Loss: 0.1118 Acc: 0.9612\n",
      "val Loss: 0.0912 Acc: 0.9738\n",
      "\n",
      "Epoch 97/299\n",
      "----------\n",
      "train Loss: 0.1121 Acc: 0.9622\n",
      "val Loss: 0.0889 Acc: 0.9714\n",
      "\n",
      "Epoch 98/299\n",
      "----------\n",
      "train Loss: 0.1132 Acc: 0.9608\n",
      "val Loss: 0.0925 Acc: 0.9685\n",
      "\n",
      "Epoch 99/299\n",
      "----------\n",
      "train Loss: 0.1113 Acc: 0.9614\n",
      "val Loss: 0.0895 Acc: 0.9747\n",
      "\n",
      "Epoch 100/299\n",
      "----------\n",
      "train Loss: 0.1164 Acc: 0.9600\n",
      "val Loss: 0.0878 Acc: 0.9726\n",
      "\n",
      "Epoch 101/299\n",
      "----------\n",
      "train Loss: 0.1109 Acc: 0.9621\n",
      "val Loss: 0.0895 Acc: 0.9701\n",
      "\n",
      "Epoch 102/299\n",
      "----------\n",
      "train Loss: 0.1113 Acc: 0.9617\n",
      "val Loss: 0.0882 Acc: 0.9730\n",
      "\n",
      "Epoch 103/299\n",
      "----------\n",
      "train Loss: 0.1140 Acc: 0.9603\n",
      "val Loss: 0.0879 Acc: 0.9722\n",
      "\n",
      "Epoch 104/299\n",
      "----------\n",
      "train Loss: 0.1136 Acc: 0.9617\n",
      "val Loss: 0.0911 Acc: 0.9705\n",
      "\n",
      "Epoch 105/299\n",
      "----------\n",
      "train Loss: 0.1141 Acc: 0.9594\n",
      "val Loss: 0.0897 Acc: 0.9705\n",
      "\n",
      "Epoch 106/299\n",
      "----------\n",
      "train Loss: 0.1116 Acc: 0.9607\n",
      "val Loss: 0.0931 Acc: 0.9693\n",
      "\n",
      "Epoch 107/299\n",
      "----------\n",
      "train Loss: 0.1096 Acc: 0.9616\n",
      "val Loss: 0.0932 Acc: 0.9672\n",
      "\n",
      "Epoch 108/299\n",
      "----------\n",
      "train Loss: 0.1107 Acc: 0.9608\n",
      "val Loss: 0.0873 Acc: 0.9709\n",
      "\n",
      "Epoch 109/299\n",
      "----------\n",
      "train Loss: 0.1081 Acc: 0.9639\n",
      "val Loss: 0.1021 Acc: 0.9655\n",
      "\n",
      "Epoch 110/299\n",
      "----------\n",
      "train Loss: 0.1074 Acc: 0.9650\n",
      "val Loss: 0.0882 Acc: 0.9693\n",
      "\n",
      "Epoch 111/299\n",
      "----------\n",
      "train Loss: 0.1120 Acc: 0.9622\n",
      "val Loss: 0.0912 Acc: 0.9672\n",
      "\n",
      "Epoch 112/299\n",
      "----------\n",
      "train Loss: 0.1082 Acc: 0.9612\n",
      "val Loss: 0.0867 Acc: 0.9714\n",
      "\n",
      "Epoch 113/299\n",
      "----------\n",
      "train Loss: 0.1082 Acc: 0.9635\n",
      "val Loss: 0.0955 Acc: 0.9676\n",
      "\n",
      "Epoch 114/299\n",
      "----------\n",
      "train Loss: 0.1085 Acc: 0.9604\n",
      "val Loss: 0.0866 Acc: 0.9738\n",
      "\n",
      "Epoch 115/299\n",
      "----------\n",
      "train Loss: 0.1091 Acc: 0.9621\n",
      "val Loss: 0.0954 Acc: 0.9664\n",
      "\n",
      "Epoch 116/299\n",
      "----------\n",
      "train Loss: 0.1086 Acc: 0.9630\n",
      "val Loss: 0.0880 Acc: 0.9722\n",
      "\n",
      "Epoch 117/299\n",
      "----------\n",
      "train Loss: 0.1098 Acc: 0.9612\n",
      "val Loss: 0.0886 Acc: 0.9726\n",
      "\n",
      "Epoch 118/299\n",
      "----------\n",
      "train Loss: 0.1050 Acc: 0.9639\n",
      "val Loss: 0.0875 Acc: 0.9709\n",
      "\n",
      "Epoch 119/299\n",
      "----------\n",
      "train Loss: 0.1087 Acc: 0.9609\n",
      "val Loss: 0.0862 Acc: 0.9705\n",
      "\n",
      "Epoch 120/299\n",
      "----------\n",
      "train Loss: 0.1090 Acc: 0.9613\n",
      "val Loss: 0.0865 Acc: 0.9705\n",
      "\n",
      "Epoch 121/299\n",
      "----------\n",
      "train Loss: 0.1099 Acc: 0.9625\n",
      "val Loss: 0.0878 Acc: 0.9726\n",
      "\n",
      "Epoch 122/299\n",
      "----------\n",
      "train Loss: 0.1073 Acc: 0.9637\n",
      "val Loss: 0.0950 Acc: 0.9680\n",
      "\n",
      "Epoch 123/299\n",
      "----------\n",
      "train Loss: 0.1085 Acc: 0.9619\n",
      "val Loss: 0.0928 Acc: 0.9680\n",
      "\n",
      "Epoch 124/299\n",
      "----------\n",
      "train Loss: 0.1092 Acc: 0.9639\n",
      "val Loss: 0.0854 Acc: 0.9714\n",
      "\n",
      "Epoch 125/299\n",
      "----------\n",
      "train Loss: 0.1076 Acc: 0.9625\n",
      "val Loss: 0.0878 Acc: 0.9697\n",
      "\n",
      "Epoch 126/299\n",
      "----------\n",
      "train Loss: 0.1085 Acc: 0.9613\n",
      "val Loss: 0.0900 Acc: 0.9718\n",
      "\n",
      "Epoch 127/299\n",
      "----------\n",
      "train Loss: 0.1102 Acc: 0.9609\n",
      "val Loss: 0.0892 Acc: 0.9726\n",
      "\n",
      "Epoch 128/299\n",
      "----------\n",
      "train Loss: 0.1091 Acc: 0.9615\n",
      "val Loss: 0.0885 Acc: 0.9693\n",
      "\n",
      "Epoch 129/299\n",
      "----------\n",
      "train Loss: 0.1086 Acc: 0.9595\n",
      "val Loss: 0.0873 Acc: 0.9697\n",
      "\n",
      "Epoch 130/299\n",
      "----------\n",
      "train Loss: 0.1087 Acc: 0.9623\n",
      "val Loss: 0.0903 Acc: 0.9685\n",
      "\n",
      "Epoch 131/299\n",
      "----------\n",
      "train Loss: 0.1065 Acc: 0.9626\n",
      "val Loss: 0.0870 Acc: 0.9693\n",
      "\n",
      "Epoch 132/299\n",
      "----------\n",
      "train Loss: 0.1087 Acc: 0.9623\n",
      "val Loss: 0.0859 Acc: 0.9755\n",
      "\n",
      "Epoch 133/299\n",
      "----------\n",
      "train Loss: 0.1079 Acc: 0.9628\n",
      "val Loss: 0.0844 Acc: 0.9726\n",
      "\n",
      "Epoch 134/299\n",
      "----------\n",
      "train Loss: 0.1053 Acc: 0.9639\n",
      "val Loss: 0.0846 Acc: 0.9722\n",
      "\n",
      "Epoch 135/299\n",
      "----------\n",
      "train Loss: 0.1073 Acc: 0.9620\n",
      "val Loss: 0.0848 Acc: 0.9759\n",
      "\n",
      "Epoch 136/299\n",
      "----------\n",
      "train Loss: 0.1076 Acc: 0.9628\n",
      "val Loss: 0.0919 Acc: 0.9709\n",
      "\n",
      "Epoch 137/299\n",
      "----------\n",
      "train Loss: 0.1056 Acc: 0.9632\n",
      "val Loss: 0.0851 Acc: 0.9718\n",
      "\n",
      "Epoch 138/299\n",
      "----------\n",
      "train Loss: 0.1087 Acc: 0.9630\n",
      "val Loss: 0.0928 Acc: 0.9676\n",
      "\n",
      "Epoch 139/299\n",
      "----------\n",
      "train Loss: 0.1085 Acc: 0.9607\n",
      "val Loss: 0.0955 Acc: 0.9685\n",
      "\n",
      "Epoch 140/299\n",
      "----------\n",
      "train Loss: 0.1071 Acc: 0.9626\n",
      "val Loss: 0.0985 Acc: 0.9635\n",
      "\n",
      "Epoch 141/299\n",
      "----------\n",
      "train Loss: 0.1051 Acc: 0.9632\n",
      "val Loss: 0.0842 Acc: 0.9743\n",
      "\n",
      "Epoch 142/299\n",
      "----------\n",
      "train Loss: 0.1063 Acc: 0.9639\n",
      "val Loss: 0.0838 Acc: 0.9743\n",
      "\n",
      "Epoch 143/299\n",
      "----------\n",
      "train Loss: 0.1068 Acc: 0.9635\n",
      "val Loss: 0.0871 Acc: 0.9734\n",
      "\n",
      "Epoch 144/299\n",
      "----------\n",
      "train Loss: 0.1049 Acc: 0.9645\n",
      "val Loss: 0.0904 Acc: 0.9714\n",
      "\n",
      "Epoch 145/299\n",
      "----------\n",
      "train Loss: 0.1077 Acc: 0.9619\n",
      "val Loss: 0.0853 Acc: 0.9705\n",
      "\n",
      "Epoch 146/299\n",
      "----------\n",
      "train Loss: 0.1043 Acc: 0.9634\n",
      "val Loss: 0.0834 Acc: 0.9726\n",
      "\n",
      "Epoch 147/299\n",
      "----------\n",
      "train Loss: 0.1076 Acc: 0.9636\n",
      "val Loss: 0.0833 Acc: 0.9743\n",
      "\n",
      "Epoch 148/299\n",
      "----------\n",
      "train Loss: 0.1033 Acc: 0.9642\n",
      "val Loss: 0.0842 Acc: 0.9743\n",
      "\n",
      "Epoch 149/299\n",
      "----------\n",
      "train Loss: 0.1042 Acc: 0.9635\n",
      "val Loss: 0.0832 Acc: 0.9743\n",
      "\n",
      "Epoch 150/299\n",
      "----------\n",
      "train Loss: 0.1032 Acc: 0.9636\n",
      "val Loss: 0.0830 Acc: 0.9730\n",
      "\n",
      "Epoch 151/299\n",
      "----------\n",
      "train Loss: 0.1068 Acc: 0.9629\n",
      "val Loss: 0.0836 Acc: 0.9755\n",
      "\n",
      "Epoch 152/299\n",
      "----------\n",
      "train Loss: 0.1060 Acc: 0.9647\n",
      "val Loss: 0.0922 Acc: 0.9676\n",
      "\n",
      "Epoch 153/299\n",
      "----------\n",
      "train Loss: 0.1022 Acc: 0.9645\n",
      "val Loss: 0.0990 Acc: 0.9643\n",
      "\n",
      "Epoch 154/299\n",
      "----------\n",
      "train Loss: 0.1025 Acc: 0.9659\n",
      "val Loss: 0.0873 Acc: 0.9726\n",
      "\n",
      "Epoch 155/299\n",
      "----------\n",
      "train Loss: 0.1069 Acc: 0.9626\n",
      "val Loss: 0.0874 Acc: 0.9693\n",
      "\n",
      "Epoch 156/299\n",
      "----------\n",
      "train Loss: 0.1035 Acc: 0.9638\n",
      "val Loss: 0.0823 Acc: 0.9751\n",
      "\n",
      "Epoch 157/299\n",
      "----------\n",
      "train Loss: 0.1037 Acc: 0.9638\n",
      "val Loss: 0.0926 Acc: 0.9705\n",
      "\n",
      "Epoch 158/299\n",
      "----------\n",
      "train Loss: 0.1031 Acc: 0.9644\n",
      "val Loss: 0.0819 Acc: 0.9751\n",
      "\n",
      "Epoch 159/299\n",
      "----------\n",
      "train Loss: 0.1050 Acc: 0.9634\n",
      "val Loss: 0.0819 Acc: 0.9755\n",
      "\n",
      "Epoch 160/299\n",
      "----------\n",
      "train Loss: 0.1058 Acc: 0.9630\n",
      "val Loss: 0.0844 Acc: 0.9718\n",
      "\n",
      "Epoch 161/299\n",
      "----------\n",
      "train Loss: 0.1036 Acc: 0.9643\n",
      "val Loss: 0.0823 Acc: 0.9763\n",
      "\n",
      "Epoch 162/299\n",
      "----------\n",
      "train Loss: 0.1059 Acc: 0.9615\n",
      "val Loss: 0.0866 Acc: 0.9680\n",
      "\n",
      "Epoch 163/299\n",
      "----------\n",
      "train Loss: 0.1056 Acc: 0.9638\n",
      "val Loss: 0.0823 Acc: 0.9755\n",
      "\n",
      "Epoch 164/299\n",
      "----------\n",
      "train Loss: 0.1012 Acc: 0.9639\n",
      "val Loss: 0.0850 Acc: 0.9697\n",
      "\n",
      "Epoch 165/299\n",
      "----------\n",
      "train Loss: 0.1036 Acc: 0.9638\n",
      "val Loss: 0.0828 Acc: 0.9768\n",
      "\n",
      "Epoch 166/299\n",
      "----------\n",
      "train Loss: 0.1034 Acc: 0.9636\n",
      "val Loss: 0.0826 Acc: 0.9743\n",
      "\n",
      "Epoch 167/299\n",
      "----------\n",
      "train Loss: 0.1059 Acc: 0.9644\n",
      "val Loss: 0.0819 Acc: 0.9730\n",
      "\n",
      "Epoch 168/299\n",
      "----------\n",
      "train Loss: 0.1050 Acc: 0.9625\n",
      "val Loss: 0.0815 Acc: 0.9730\n",
      "\n",
      "Epoch 169/299\n",
      "----------\n",
      "train Loss: 0.1044 Acc: 0.9651\n",
      "val Loss: 0.0814 Acc: 0.9730\n",
      "\n",
      "Epoch 170/299\n",
      "----------\n",
      "train Loss: 0.1017 Acc: 0.9654\n",
      "val Loss: 0.0828 Acc: 0.9722\n",
      "\n",
      "Epoch 171/299\n",
      "----------\n",
      "train Loss: 0.1033 Acc: 0.9649\n",
      "val Loss: 0.0810 Acc: 0.9759\n",
      "\n",
      "Epoch 172/299\n",
      "----------\n",
      "train Loss: 0.0988 Acc: 0.9642\n",
      "val Loss: 0.0827 Acc: 0.9734\n",
      "\n",
      "Epoch 173/299\n",
      "----------\n",
      "train Loss: 0.0998 Acc: 0.9654\n",
      "val Loss: 0.0815 Acc: 0.9726\n",
      "\n",
      "Epoch 174/299\n",
      "----------\n",
      "train Loss: 0.1042 Acc: 0.9639\n",
      "val Loss: 0.0828 Acc: 0.9772\n",
      "\n",
      "Epoch 175/299\n",
      "----------\n",
      "train Loss: 0.1016 Acc: 0.9656\n",
      "val Loss: 0.0824 Acc: 0.9726\n",
      "\n",
      "Epoch 176/299\n",
      "----------\n",
      "train Loss: 0.1036 Acc: 0.9631\n",
      "val Loss: 0.0843 Acc: 0.9697\n",
      "\n",
      "Epoch 177/299\n",
      "----------\n",
      "train Loss: 0.1085 Acc: 0.9623\n",
      "val Loss: 0.0858 Acc: 0.9693\n",
      "\n",
      "Epoch 178/299\n",
      "----------\n",
      "train Loss: 0.1040 Acc: 0.9628\n",
      "val Loss: 0.0811 Acc: 0.9734\n",
      "\n",
      "Epoch 179/299\n",
      "----------\n",
      "train Loss: 0.1019 Acc: 0.9659\n",
      "val Loss: 0.0812 Acc: 0.9734\n",
      "\n",
      "Epoch 180/299\n",
      "----------\n",
      "train Loss: 0.1026 Acc: 0.9640\n",
      "val Loss: 0.0842 Acc: 0.9697\n",
      "\n",
      "Epoch 181/299\n",
      "----------\n",
      "train Loss: 0.1017 Acc: 0.9655\n",
      "val Loss: 0.0802 Acc: 0.9747\n",
      "\n",
      "Epoch 182/299\n",
      "----------\n",
      "train Loss: 0.1021 Acc: 0.9642\n",
      "val Loss: 0.0822 Acc: 0.9718\n",
      "\n",
      "Epoch 183/299\n",
      "----------\n",
      "train Loss: 0.1011 Acc: 0.9657\n",
      "val Loss: 0.0804 Acc: 0.9755\n",
      "\n",
      "Epoch 184/299\n",
      "----------\n",
      "train Loss: 0.1024 Acc: 0.9635\n",
      "val Loss: 0.0835 Acc: 0.9743\n",
      "\n",
      "Epoch 185/299\n",
      "----------\n",
      "train Loss: 0.1010 Acc: 0.9656\n",
      "val Loss: 0.0830 Acc: 0.9701\n",
      "\n",
      "Epoch 186/299\n",
      "----------\n",
      "train Loss: 0.1023 Acc: 0.9653\n",
      "val Loss: 0.0809 Acc: 0.9743\n",
      "\n",
      "Epoch 187/299\n",
      "----------\n",
      "train Loss: 0.1020 Acc: 0.9650\n",
      "val Loss: 0.0810 Acc: 0.9759\n",
      "\n",
      "Epoch 188/299\n",
      "----------\n",
      "train Loss: 0.0983 Acc: 0.9648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0810 Acc: 0.9718\n",
      "\n",
      "Epoch 189/299\n",
      "----------\n",
      "train Loss: 0.0999 Acc: 0.9644\n",
      "val Loss: 0.0815 Acc: 0.9730\n",
      "\n",
      "Epoch 190/299\n",
      "----------\n",
      "train Loss: 0.1005 Acc: 0.9641\n",
      "val Loss: 0.0806 Acc: 0.9734\n",
      "\n",
      "Epoch 191/299\n",
      "----------\n",
      "train Loss: 0.1032 Acc: 0.9639\n",
      "val Loss: 0.0807 Acc: 0.9759\n",
      "\n",
      "Epoch 192/299\n",
      "----------\n",
      "train Loss: 0.1026 Acc: 0.9629\n",
      "val Loss: 0.0798 Acc: 0.9747\n",
      "\n",
      "Epoch 193/299\n",
      "----------\n",
      "train Loss: 0.1006 Acc: 0.9648\n",
      "val Loss: 0.0840 Acc: 0.9697\n",
      "\n",
      "Epoch 194/299\n",
      "----------\n",
      "train Loss: 0.1032 Acc: 0.9630\n",
      "val Loss: 0.0899 Acc: 0.9672\n",
      "\n",
      "Epoch 195/299\n",
      "----------\n",
      "train Loss: 0.1008 Acc: 0.9628\n",
      "val Loss: 0.0802 Acc: 0.9747\n",
      "\n",
      "Epoch 196/299\n",
      "----------\n",
      "train Loss: 0.1007 Acc: 0.9655\n",
      "val Loss: 0.0850 Acc: 0.9697\n",
      "\n",
      "Epoch 197/299\n",
      "----------\n",
      "train Loss: 0.1029 Acc: 0.9645\n",
      "val Loss: 0.0796 Acc: 0.9755\n",
      "\n",
      "Epoch 198/299\n",
      "----------\n",
      "train Loss: 0.1016 Acc: 0.9635\n",
      "val Loss: 0.0803 Acc: 0.9776\n",
      "\n",
      "Epoch 199/299\n",
      "----------\n",
      "train Loss: 0.1010 Acc: 0.9640\n",
      "val Loss: 0.0829 Acc: 0.9751\n",
      "\n",
      "Epoch 200/299\n",
      "----------\n",
      "train Loss: 0.1031 Acc: 0.9640\n",
      "val Loss: 0.0796 Acc: 0.9738\n",
      "\n",
      "Epoch 201/299\n",
      "----------\n",
      "train Loss: 0.1032 Acc: 0.9622\n",
      "val Loss: 0.0820 Acc: 0.9755\n",
      "\n",
      "Epoch 202/299\n",
      "----------\n",
      "train Loss: 0.1013 Acc: 0.9645\n",
      "val Loss: 0.0793 Acc: 0.9772\n",
      "\n",
      "Epoch 203/299\n",
      "----------\n",
      "train Loss: 0.0998 Acc: 0.9662\n",
      "val Loss: 0.0791 Acc: 0.9759\n",
      "\n",
      "Epoch 204/299\n",
      "----------\n",
      "train Loss: 0.0993 Acc: 0.9674\n",
      "val Loss: 0.0795 Acc: 0.9763\n",
      "\n",
      "Epoch 205/299\n",
      "----------\n",
      "train Loss: 0.1010 Acc: 0.9639\n",
      "val Loss: 0.0792 Acc: 0.9768\n",
      "\n",
      "Epoch 206/299\n",
      "----------\n",
      "train Loss: 0.1042 Acc: 0.9641\n",
      "val Loss: 0.0807 Acc: 0.9734\n",
      "\n",
      "Epoch 207/299\n",
      "----------\n",
      "train Loss: 0.0998 Acc: 0.9651\n",
      "val Loss: 0.0812 Acc: 0.9709\n",
      "\n",
      "Epoch 208/299\n",
      "----------\n",
      "train Loss: 0.1027 Acc: 0.9666\n",
      "val Loss: 0.0806 Acc: 0.9722\n",
      "\n",
      "Epoch 209/299\n",
      "----------\n",
      "train Loss: 0.1030 Acc: 0.9629\n",
      "val Loss: 0.0803 Acc: 0.9772\n",
      "\n",
      "Epoch 210/299\n",
      "----------\n",
      "train Loss: 0.0994 Acc: 0.9652\n",
      "val Loss: 0.0806 Acc: 0.9722\n",
      "\n",
      "Epoch 211/299\n",
      "----------\n",
      "train Loss: 0.1013 Acc: 0.9630\n",
      "val Loss: 0.0787 Acc: 0.9755\n",
      "\n",
      "Epoch 212/299\n",
      "----------\n",
      "train Loss: 0.1005 Acc: 0.9635\n",
      "val Loss: 0.0787 Acc: 0.9755\n",
      "\n",
      "Epoch 213/299\n",
      "----------\n",
      "train Loss: 0.1015 Acc: 0.9643\n",
      "val Loss: 0.0788 Acc: 0.9768\n",
      "\n",
      "Epoch 214/299\n",
      "----------\n",
      "train Loss: 0.1010 Acc: 0.9661\n",
      "val Loss: 0.0837 Acc: 0.9701\n",
      "\n",
      "Epoch 215/299\n",
      "----------\n",
      "train Loss: 0.1011 Acc: 0.9643\n",
      "val Loss: 0.0868 Acc: 0.9672\n",
      "\n",
      "Epoch 216/299\n",
      "----------\n",
      "train Loss: 0.1018 Acc: 0.9651\n",
      "val Loss: 0.0817 Acc: 0.9763\n",
      "\n",
      "Epoch 217/299\n",
      "----------\n",
      "train Loss: 0.0950 Acc: 0.9688\n",
      "val Loss: 0.0795 Acc: 0.9743\n",
      "\n",
      "Epoch 218/299\n",
      "----------\n",
      "train Loss: 0.0975 Acc: 0.9657\n",
      "val Loss: 0.0829 Acc: 0.9693\n",
      "\n",
      "Epoch 219/299\n",
      "----------\n",
      "train Loss: 0.1014 Acc: 0.9648\n",
      "val Loss: 0.0782 Acc: 0.9768\n",
      "\n",
      "Epoch 220/299\n",
      "----------\n",
      "train Loss: 0.1002 Acc: 0.9655\n",
      "val Loss: 0.0900 Acc: 0.9672\n",
      "\n",
      "Epoch 221/299\n",
      "----------\n",
      "train Loss: 0.0994 Acc: 0.9654\n",
      "val Loss: 0.0803 Acc: 0.9722\n",
      "\n",
      "Epoch 222/299\n",
      "----------\n",
      "train Loss: 0.1006 Acc: 0.9641\n",
      "val Loss: 0.0783 Acc: 0.9768\n",
      "\n",
      "Epoch 223/299\n",
      "----------\n",
      "train Loss: 0.0972 Acc: 0.9653\n",
      "val Loss: 0.0786 Acc: 0.9763\n",
      "\n",
      "Epoch 224/299\n",
      "----------\n",
      "train Loss: 0.1014 Acc: 0.9638\n",
      "val Loss: 0.0790 Acc: 0.9743\n",
      "\n",
      "Epoch 225/299\n",
      "----------\n",
      "train Loss: 0.0962 Acc: 0.9662\n",
      "val Loss: 0.0784 Acc: 0.9751\n",
      "\n",
      "Epoch 226/299\n",
      "----------\n",
      "train Loss: 0.0985 Acc: 0.9671\n",
      "val Loss: 0.0851 Acc: 0.9738\n",
      "\n",
      "Epoch 227/299\n",
      "----------\n",
      "train Loss: 0.0973 Acc: 0.9675\n",
      "val Loss: 0.0789 Acc: 0.9743\n",
      "\n",
      "Epoch 228/299\n",
      "----------\n",
      "train Loss: 0.0973 Acc: 0.9652\n",
      "val Loss: 0.0788 Acc: 0.9738\n",
      "\n",
      "Epoch 229/299\n",
      "----------\n",
      "train Loss: 0.0975 Acc: 0.9659\n",
      "val Loss: 0.0778 Acc: 0.9759\n",
      "\n",
      "Epoch 230/299\n",
      "----------\n",
      "train Loss: 0.1004 Acc: 0.9637\n",
      "val Loss: 0.0785 Acc: 0.9763\n",
      "\n",
      "Epoch 231/299\n",
      "----------\n",
      "train Loss: 0.1019 Acc: 0.9620\n",
      "val Loss: 0.0784 Acc: 0.9743\n",
      "\n",
      "Epoch 232/299\n",
      "----------\n",
      "train Loss: 0.0979 Acc: 0.9657\n",
      "val Loss: 0.0778 Acc: 0.9768\n",
      "\n",
      "Epoch 233/299\n",
      "----------\n",
      "train Loss: 0.0961 Acc: 0.9652\n",
      "val Loss: 0.0834 Acc: 0.9743\n",
      "\n",
      "Epoch 234/299\n",
      "----------\n",
      "train Loss: 0.0990 Acc: 0.9663\n",
      "val Loss: 0.0937 Acc: 0.9689\n",
      "\n",
      "Epoch 235/299\n",
      "----------\n",
      "train Loss: 0.1001 Acc: 0.9653\n",
      "val Loss: 0.0774 Acc: 0.9755\n",
      "\n",
      "Epoch 236/299\n",
      "----------\n",
      "train Loss: 0.0973 Acc: 0.9662\n",
      "val Loss: 0.0778 Acc: 0.9743\n",
      "\n",
      "Epoch 237/299\n",
      "----------\n",
      "train Loss: 0.0978 Acc: 0.9659\n",
      "val Loss: 0.0776 Acc: 0.9763\n",
      "\n",
      "Epoch 238/299\n",
      "----------\n",
      "train Loss: 0.1012 Acc: 0.9653\n",
      "val Loss: 0.0793 Acc: 0.9734\n",
      "\n",
      "Epoch 239/299\n",
      "----------\n",
      "train Loss: 0.0989 Acc: 0.9635\n",
      "val Loss: 0.0771 Acc: 0.9755\n",
      "\n",
      "Epoch 240/299\n",
      "----------\n",
      "train Loss: 0.0996 Acc: 0.9652\n",
      "val Loss: 0.0788 Acc: 0.9722\n",
      "\n",
      "Epoch 241/299\n",
      "----------\n",
      "train Loss: 0.0966 Acc: 0.9658\n",
      "val Loss: 0.0776 Acc: 0.9751\n",
      "\n",
      "Epoch 242/299\n",
      "----------\n",
      "train Loss: 0.0997 Acc: 0.9652\n",
      "val Loss: 0.0776 Acc: 0.9759\n",
      "\n",
      "Epoch 243/299\n",
      "----------\n",
      "train Loss: 0.0972 Acc: 0.9664\n",
      "val Loss: 0.0774 Acc: 0.9768\n",
      "\n",
      "Epoch 244/299\n",
      "----------\n",
      "train Loss: 0.0980 Acc: 0.9638\n",
      "val Loss: 0.0775 Acc: 0.9772\n",
      "\n",
      "Epoch 245/299\n",
      "----------\n",
      "train Loss: 0.0988 Acc: 0.9665\n",
      "val Loss: 0.0787 Acc: 0.9772\n",
      "\n",
      "Epoch 246/299\n",
      "----------\n",
      "train Loss: 0.1011 Acc: 0.9645\n",
      "val Loss: 0.0805 Acc: 0.9759\n",
      "\n",
      "Epoch 247/299\n",
      "----------\n",
      "train Loss: 0.0991 Acc: 0.9649\n",
      "val Loss: 0.0797 Acc: 0.9768\n",
      "\n",
      "Epoch 248/299\n",
      "----------\n",
      "train Loss: 0.0979 Acc: 0.9661\n",
      "val Loss: 0.0814 Acc: 0.9714\n",
      "\n",
      "Epoch 249/299\n",
      "----------\n",
      "train Loss: 0.0967 Acc: 0.9656\n",
      "val Loss: 0.0769 Acc: 0.9768\n",
      "\n",
      "Epoch 250/299\n",
      "----------\n",
      "train Loss: 0.0985 Acc: 0.9658\n",
      "val Loss: 0.0797 Acc: 0.9730\n",
      "\n",
      "Epoch 251/299\n",
      "----------\n",
      "train Loss: 0.0968 Acc: 0.9656\n",
      "val Loss: 0.0784 Acc: 0.9780\n",
      "\n",
      "Epoch 252/299\n",
      "----------\n",
      "train Loss: 0.0986 Acc: 0.9648\n",
      "val Loss: 0.0819 Acc: 0.9701\n",
      "\n",
      "Epoch 253/299\n",
      "----------\n",
      "train Loss: 0.0976 Acc: 0.9659\n",
      "val Loss: 0.0892 Acc: 0.9668\n",
      "\n",
      "Epoch 254/299\n",
      "----------\n",
      "train Loss: 0.0964 Acc: 0.9654\n",
      "val Loss: 0.0790 Acc: 0.9734\n",
      "\n",
      "Epoch 255/299\n",
      "----------\n",
      "train Loss: 0.0996 Acc: 0.9652\n",
      "val Loss: 0.0784 Acc: 0.9768\n",
      "\n",
      "Epoch 256/299\n",
      "----------\n",
      "train Loss: 0.0964 Acc: 0.9659\n",
      "val Loss: 0.0762 Acc: 0.9755\n",
      "\n",
      "Epoch 257/299\n",
      "----------\n",
      "train Loss: 0.0948 Acc: 0.9669\n",
      "val Loss: 0.0791 Acc: 0.9768\n",
      "\n",
      "Epoch 258/299\n",
      "----------\n",
      "train Loss: 0.0951 Acc: 0.9665\n",
      "val Loss: 0.0767 Acc: 0.9768\n",
      "\n",
      "Epoch 259/299\n",
      "----------\n",
      "train Loss: 0.1012 Acc: 0.9640\n",
      "val Loss: 0.0766 Acc: 0.9763\n",
      "\n",
      "Epoch 260/299\n",
      "----------\n",
      "train Loss: 0.0964 Acc: 0.9665\n",
      "val Loss: 0.0766 Acc: 0.9776\n",
      "\n",
      "Epoch 261/299\n",
      "----------\n",
      "train Loss: 0.0984 Acc: 0.9646\n",
      "val Loss: 0.0840 Acc: 0.9747\n",
      "\n",
      "Epoch 262/299\n",
      "----------\n",
      "train Loss: 0.0951 Acc: 0.9673\n",
      "val Loss: 0.0764 Acc: 0.9759\n",
      "\n",
      "Epoch 263/299\n",
      "----------\n",
      "train Loss: 0.0990 Acc: 0.9676\n",
      "val Loss: 0.0778 Acc: 0.9743\n",
      "\n",
      "Epoch 264/299\n",
      "----------\n",
      "train Loss: 0.0946 Acc: 0.9672\n",
      "val Loss: 0.0787 Acc: 0.9768\n",
      "\n",
      "Epoch 265/299\n",
      "----------\n",
      "train Loss: 0.0997 Acc: 0.9639\n",
      "val Loss: 0.0772 Acc: 0.9776\n",
      "\n",
      "Epoch 266/299\n",
      "----------\n",
      "train Loss: 0.0956 Acc: 0.9668\n",
      "val Loss: 0.0781 Acc: 0.9730\n",
      "\n",
      "Epoch 267/299\n",
      "----------\n",
      "train Loss: 0.0963 Acc: 0.9650\n",
      "val Loss: 0.0770 Acc: 0.9768\n",
      "\n",
      "Epoch 268/299\n",
      "----------\n",
      "train Loss: 0.0967 Acc: 0.9662\n",
      "val Loss: 0.0857 Acc: 0.9680\n",
      "\n",
      "Epoch 269/299\n",
      "----------\n",
      "train Loss: 0.0954 Acc: 0.9672\n",
      "val Loss: 0.0763 Acc: 0.9747\n",
      "\n",
      "Epoch 270/299\n",
      "----------\n",
      "train Loss: 0.0991 Acc: 0.9639\n",
      "val Loss: 0.0763 Acc: 0.9768\n",
      "\n",
      "Epoch 271/299\n",
      "----------\n",
      "train Loss: 0.0944 Acc: 0.9686\n",
      "val Loss: 0.0762 Acc: 0.9772\n",
      "\n",
      "Epoch 272/299\n",
      "----------\n",
      "train Loss: 0.0954 Acc: 0.9668\n",
      "val Loss: 0.0846 Acc: 0.9738\n",
      "\n",
      "Epoch 273/299\n",
      "----------\n",
      "train Loss: 0.1000 Acc: 0.9646\n",
      "val Loss: 0.0777 Acc: 0.9768\n",
      "\n",
      "Epoch 274/299\n",
      "----------\n",
      "train Loss: 0.0960 Acc: 0.9653\n",
      "val Loss: 0.0762 Acc: 0.9772\n",
      "\n",
      "Epoch 275/299\n",
      "----------\n",
      "train Loss: 0.0990 Acc: 0.9631\n",
      "val Loss: 0.0761 Acc: 0.9743\n",
      "\n",
      "Epoch 276/299\n",
      "----------\n",
      "train Loss: 0.0946 Acc: 0.9690\n",
      "val Loss: 0.0761 Acc: 0.9772\n",
      "\n",
      "Epoch 277/299\n",
      "----------\n",
      "train Loss: 0.0919 Acc: 0.9678\n",
      "val Loss: 0.0829 Acc: 0.9714\n",
      "\n",
      "Epoch 278/299\n",
      "----------\n",
      "train Loss: 0.0977 Acc: 0.9648\n",
      "val Loss: 0.0796 Acc: 0.9759\n",
      "\n",
      "Epoch 279/299\n",
      "----------\n",
      "train Loss: 0.0994 Acc: 0.9645\n",
      "val Loss: 0.0779 Acc: 0.9776\n",
      "\n",
      "Epoch 280/299\n",
      "----------\n",
      "train Loss: 0.0971 Acc: 0.9659\n",
      "val Loss: 0.0760 Acc: 0.9755\n",
      "\n",
      "Epoch 281/299\n",
      "----------\n",
      "train Loss: 0.0944 Acc: 0.9676\n",
      "val Loss: 0.0764 Acc: 0.9763\n",
      "\n",
      "Epoch 282/299\n",
      "----------\n",
      "train Loss: 0.0956 Acc: 0.9671\n",
      "val Loss: 0.0756 Acc: 0.9772\n",
      "\n",
      "Epoch 283/299\n",
      "----------\n",
      "train Loss: 0.0956 Acc: 0.9662\n",
      "val Loss: 0.0789 Acc: 0.9714\n",
      "\n",
      "Epoch 284/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0989 Acc: 0.9659\n",
      "val Loss: 0.0793 Acc: 0.9722\n",
      "\n",
      "Epoch 285/299\n",
      "----------\n",
      "train Loss: 0.0979 Acc: 0.9645\n",
      "val Loss: 0.0757 Acc: 0.9751\n",
      "\n",
      "Epoch 286/299\n",
      "----------\n",
      "train Loss: 0.0987 Acc: 0.9665\n",
      "val Loss: 0.0756 Acc: 0.9772\n",
      "\n",
      "Epoch 287/299\n",
      "----------\n",
      "train Loss: 0.0942 Acc: 0.9646\n",
      "val Loss: 0.0831 Acc: 0.9689\n",
      "\n",
      "Epoch 288/299\n",
      "----------\n",
      "train Loss: 0.0972 Acc: 0.9655\n",
      "val Loss: 0.0758 Acc: 0.9755\n",
      "\n",
      "Epoch 289/299\n",
      "----------\n",
      "train Loss: 0.0971 Acc: 0.9667\n",
      "val Loss: 0.0806 Acc: 0.9709\n",
      "\n",
      "Epoch 290/299\n",
      "----------\n",
      "train Loss: 0.0926 Acc: 0.9664\n",
      "val Loss: 0.0773 Acc: 0.9747\n",
      "\n",
      "Epoch 291/299\n",
      "----------\n",
      "train Loss: 0.0956 Acc: 0.9667\n",
      "val Loss: 0.0803 Acc: 0.9718\n",
      "\n",
      "Epoch 292/299\n",
      "----------\n",
      "train Loss: 0.0937 Acc: 0.9675\n",
      "val Loss: 0.0768 Acc: 0.9784\n",
      "\n",
      "Epoch 293/299\n",
      "----------\n",
      "train Loss: 0.0948 Acc: 0.9654\n",
      "val Loss: 0.0760 Acc: 0.9768\n",
      "\n",
      "Epoch 294/299\n",
      "----------\n",
      "train Loss: 0.0977 Acc: 0.9667\n",
      "val Loss: 0.0775 Acc: 0.9747\n",
      "\n",
      "Epoch 295/299\n",
      "----------\n",
      "train Loss: 0.0946 Acc: 0.9664\n",
      "val Loss: 0.0802 Acc: 0.9709\n",
      "\n",
      "Epoch 296/299\n",
      "----------\n",
      "train Loss: 0.0939 Acc: 0.9661\n",
      "val Loss: 0.0759 Acc: 0.9755\n",
      "\n",
      "Epoch 297/299\n",
      "----------\n",
      "train Loss: 0.0966 Acc: 0.9655\n",
      "val Loss: 0.0765 Acc: 0.9780\n",
      "\n",
      "Epoch 298/299\n",
      "----------\n",
      "train Loss: 0.0979 Acc: 0.9662\n",
      "val Loss: 0.0948 Acc: 0.9639\n",
      "\n",
      "Epoch 299/299\n",
      "----------\n",
      "train Loss: 0.0972 Acc: 0.9658\n",
      "val Loss: 0.0779 Acc: 0.9734\n",
      "\n",
      "\n",
      "##############################\n",
      "------ Summary ------\n",
      "model -> squeezenet\n",
      "epochs -> 300\n",
      "lr -> 0.0001\n",
      "batch size -> 8\n",
      "\n",
      "Training complete in 68m 25s\n",
      "Best val Acc: 0.978414\n",
      "##############################\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== END ====\n",
      "[[791   0   0]\n",
      " [  1 772  26]\n",
      " [  4  21 794]]\n",
      "\n",
      "Sensitivity or recall total\n",
      "0.9784142797841427\n",
      "\n",
      "Sensitivity or recall per classes\n",
      "[1.   0.97 0.97]\n",
      "\n",
      "Precision\n",
      "[0.99 0.97 0.97]\n",
      "\n",
      "F1 Score\n",
      "[1.   0.97 0.97]\n",
      "Confusion matrix, without normalization\n",
      "\n",
      "============================================================\n",
      "==== INITIALIZING WITH PARAMETERS: ====\n",
      "model -> densenet\n",
      "epochs -> 300\n",
      "lr -> 0.0001\n",
      "batch size -> 8\n",
      "\n",
      "--------------------\n",
      "Params to learn:\n",
      "\t classifier.weight\n",
      "\t classifier.bias\n",
      "\n",
      "--------------------\n",
      "\n",
      "== Epochs ==\n",
      "Epoch 0/299\n",
      "----------\n",
      "train Loss: 0.6390 Acc: 0.8184\n",
      "val Loss: 0.3793 Acc: 0.9348\n",
      "\n",
      "Epoch 1/299\n",
      "----------\n",
      "train Loss: 0.3661 Acc: 0.9107\n",
      "val Loss: 0.2782 Acc: 0.9435\n",
      "\n",
      "Epoch 2/299\n",
      "----------\n",
      "train Loss: 0.2942 Acc: 0.9196\n",
      "val Loss: 0.2281 Acc: 0.9485\n",
      "\n",
      "Epoch 3/299\n",
      "----------\n",
      "train Loss: 0.2666 Acc: 0.9224\n",
      "val Loss: 0.2081 Acc: 0.9494\n",
      "\n",
      "Epoch 4/299\n",
      "----------\n",
      "train Loss: 0.2500 Acc: 0.9250\n",
      "val Loss: 0.1917 Acc: 0.9514\n",
      "\n",
      "Epoch 5/299\n",
      "----------\n",
      "train Loss: 0.2300 Acc: 0.9316\n",
      "val Loss: 0.1805 Acc: 0.9523\n",
      "\n",
      "Epoch 6/299\n",
      "----------\n",
      "train Loss: 0.2207 Acc: 0.9318\n",
      "val Loss: 0.1641 Acc: 0.9597\n",
      "\n",
      "Epoch 7/299\n",
      "----------\n",
      "train Loss: 0.2170 Acc: 0.9321\n",
      "val Loss: 0.1646 Acc: 0.9552\n",
      "\n",
      "Epoch 8/299\n",
      "----------\n",
      "train Loss: 0.2074 Acc: 0.9335\n",
      "val Loss: 0.1538 Acc: 0.9601\n",
      "\n",
      "Epoch 9/299\n",
      "----------\n",
      "train Loss: 0.2003 Acc: 0.9349\n",
      "val Loss: 0.1512 Acc: 0.9601\n",
      "\n",
      "Epoch 10/299\n",
      "----------\n",
      "train Loss: 0.1970 Acc: 0.9365\n",
      "val Loss: 0.1422 Acc: 0.9606\n",
      "\n",
      "Epoch 11/299\n",
      "----------\n",
      "train Loss: 0.1869 Acc: 0.9387\n",
      "val Loss: 0.1442 Acc: 0.9585\n",
      "\n",
      "Epoch 12/299\n",
      "----------\n",
      "train Loss: 0.1909 Acc: 0.9371\n",
      "val Loss: 0.1394 Acc: 0.9597\n",
      "\n",
      "Epoch 13/299\n",
      "----------\n",
      "train Loss: 0.1811 Acc: 0.9395\n",
      "val Loss: 0.1335 Acc: 0.9639\n",
      "\n",
      "Epoch 14/299\n",
      "----------\n",
      "train Loss: 0.1788 Acc: 0.9411\n",
      "val Loss: 0.1314 Acc: 0.9618\n",
      "\n",
      "Epoch 15/299\n",
      "----------\n",
      "train Loss: 0.1786 Acc: 0.9405\n",
      "val Loss: 0.1281 Acc: 0.9610\n",
      "\n",
      "Epoch 16/299\n",
      "----------\n",
      "train Loss: 0.1779 Acc: 0.9388\n",
      "val Loss: 0.1273 Acc: 0.9626\n",
      "\n",
      "Epoch 17/299\n",
      "----------\n",
      "train Loss: 0.1732 Acc: 0.9404\n",
      "val Loss: 0.1277 Acc: 0.9651\n",
      "\n",
      "Epoch 18/299\n",
      "----------\n",
      "train Loss: 0.1773 Acc: 0.9394\n",
      "val Loss: 0.1283 Acc: 0.9626\n",
      "\n",
      "Epoch 19/299\n",
      "----------\n",
      "train Loss: 0.1665 Acc: 0.9467\n",
      "val Loss: 0.1229 Acc: 0.9647\n",
      "\n",
      "Epoch 20/299\n",
      "----------\n",
      "train Loss: 0.1686 Acc: 0.9428\n",
      "val Loss: 0.1170 Acc: 0.9643\n",
      "\n",
      "Epoch 21/299\n",
      "----------\n",
      "train Loss: 0.1660 Acc: 0.9433\n",
      "val Loss: 0.1190 Acc: 0.9635\n",
      "\n",
      "Epoch 22/299\n",
      "----------\n",
      "train Loss: 0.1679 Acc: 0.9440\n",
      "val Loss: 0.1176 Acc: 0.9647\n",
      "\n",
      "Epoch 23/299\n",
      "----------\n",
      "train Loss: 0.1627 Acc: 0.9454\n",
      "val Loss: 0.1154 Acc: 0.9660\n",
      "\n",
      "Epoch 24/299\n",
      "----------\n",
      "train Loss: 0.1673 Acc: 0.9449\n",
      "val Loss: 0.1190 Acc: 0.9622\n",
      "\n",
      "Epoch 25/299\n",
      "----------\n",
      "train Loss: 0.1577 Acc: 0.9458\n",
      "val Loss: 0.1141 Acc: 0.9635\n",
      "\n",
      "Epoch 26/299\n",
      "----------\n",
      "train Loss: 0.1610 Acc: 0.9442\n",
      "val Loss: 0.1128 Acc: 0.9647\n",
      "\n",
      "Epoch 27/299\n",
      "----------\n",
      "train Loss: 0.1619 Acc: 0.9447\n",
      "val Loss: 0.1128 Acc: 0.9651\n",
      "\n",
      "Epoch 28/299\n",
      "----------\n",
      "train Loss: 0.1597 Acc: 0.9439\n",
      "val Loss: 0.1106 Acc: 0.9647\n",
      "\n",
      "Epoch 29/299\n",
      "----------\n",
      "train Loss: 0.1551 Acc: 0.9464\n",
      "val Loss: 0.1104 Acc: 0.9672\n",
      "\n",
      "Epoch 30/299\n",
      "----------\n",
      "train Loss: 0.1569 Acc: 0.9438\n",
      "val Loss: 0.1167 Acc: 0.9651\n",
      "\n",
      "Epoch 31/299\n",
      "----------\n",
      "train Loss: 0.1501 Acc: 0.9481\n",
      "val Loss: 0.1099 Acc: 0.9664\n",
      "\n",
      "Epoch 32/299\n",
      "----------\n",
      "train Loss: 0.1512 Acc: 0.9482\n",
      "val Loss: 0.1069 Acc: 0.9668\n",
      "\n",
      "Epoch 33/299\n",
      "----------\n",
      "train Loss: 0.1510 Acc: 0.9487\n",
      "val Loss: 0.1101 Acc: 0.9676\n",
      "\n",
      "Epoch 34/299\n",
      "----------\n",
      "train Loss: 0.1488 Acc: 0.9482\n",
      "val Loss: 0.1051 Acc: 0.9664\n",
      "\n",
      "Epoch 35/299\n",
      "----------\n",
      "train Loss: 0.1593 Acc: 0.9421\n",
      "val Loss: 0.1052 Acc: 0.9655\n",
      "\n",
      "Epoch 36/299\n",
      "----------\n",
      "train Loss: 0.1484 Acc: 0.9472\n",
      "val Loss: 0.1054 Acc: 0.9668\n",
      "\n",
      "Epoch 37/299\n",
      "----------\n",
      "train Loss: 0.1508 Acc: 0.9481\n",
      "val Loss: 0.1034 Acc: 0.9680\n",
      "\n",
      "Epoch 38/299\n",
      "----------\n",
      "train Loss: 0.1518 Acc: 0.9461\n",
      "val Loss: 0.1025 Acc: 0.9664\n",
      "\n",
      "Epoch 39/299\n",
      "----------\n",
      "train Loss: 0.1482 Acc: 0.9474\n",
      "val Loss: 0.1014 Acc: 0.9668\n",
      "\n",
      "Epoch 40/299\n",
      "----------\n",
      "train Loss: 0.1501 Acc: 0.9466\n",
      "val Loss: 0.1026 Acc: 0.9685\n",
      "\n",
      "Epoch 41/299\n",
      "----------\n",
      "train Loss: 0.1508 Acc: 0.9446\n",
      "val Loss: 0.1031 Acc: 0.9660\n",
      "\n",
      "Epoch 42/299\n",
      "----------\n",
      "train Loss: 0.1452 Acc: 0.9481\n",
      "val Loss: 0.1033 Acc: 0.9664\n",
      "\n",
      "Epoch 43/299\n",
      "----------\n",
      "train Loss: 0.1412 Acc: 0.9499\n",
      "val Loss: 0.1014 Acc: 0.9655\n",
      "\n",
      "Epoch 44/299\n",
      "----------\n",
      "train Loss: 0.1419 Acc: 0.9516\n",
      "val Loss: 0.1046 Acc: 0.9660\n",
      "\n",
      "Epoch 45/299\n",
      "----------\n",
      "train Loss: 0.1440 Acc: 0.9526\n",
      "val Loss: 0.1004 Acc: 0.9697\n",
      "\n",
      "Epoch 46/299\n",
      "----------\n",
      "train Loss: 0.1422 Acc: 0.9490\n",
      "val Loss: 0.1038 Acc: 0.9672\n",
      "\n",
      "Epoch 47/299\n",
      "----------\n",
      "train Loss: 0.1412 Acc: 0.9514\n",
      "val Loss: 0.0989 Acc: 0.9693\n",
      "\n",
      "Epoch 48/299\n",
      "----------\n",
      "train Loss: 0.1455 Acc: 0.9491\n",
      "val Loss: 0.1023 Acc: 0.9664\n",
      "\n",
      "Epoch 49/299\n",
      "----------\n",
      "train Loss: 0.1410 Acc: 0.9508\n",
      "val Loss: 0.1059 Acc: 0.9635\n",
      "\n",
      "Epoch 50/299\n",
      "----------\n",
      "train Loss: 0.1363 Acc: 0.9541\n",
      "val Loss: 0.0998 Acc: 0.9701\n",
      "\n",
      "Epoch 51/299\n",
      "----------\n",
      "train Loss: 0.1438 Acc: 0.9498\n",
      "val Loss: 0.1025 Acc: 0.9676\n",
      "\n",
      "Epoch 52/299\n",
      "----------\n",
      "train Loss: 0.1424 Acc: 0.9490\n",
      "val Loss: 0.0969 Acc: 0.9680\n",
      "\n",
      "Epoch 53/299\n",
      "----------\n",
      "train Loss: 0.1342 Acc: 0.9530\n",
      "val Loss: 0.0990 Acc: 0.9701\n",
      "\n",
      "Epoch 54/299\n",
      "----------\n",
      "train Loss: 0.1382 Acc: 0.9525\n",
      "val Loss: 0.0973 Acc: 0.9689\n",
      "\n",
      "Epoch 55/299\n",
      "----------\n",
      "train Loss: 0.1428 Acc: 0.9509\n",
      "val Loss: 0.0989 Acc: 0.9697\n",
      "\n",
      "Epoch 56/299\n",
      "----------\n",
      "train Loss: 0.1444 Acc: 0.9489\n",
      "val Loss: 0.0972 Acc: 0.9680\n",
      "\n",
      "Epoch 57/299\n",
      "----------\n",
      "train Loss: 0.1413 Acc: 0.9492\n",
      "val Loss: 0.0964 Acc: 0.9697\n",
      "\n",
      "Epoch 58/299\n",
      "----------\n",
      "train Loss: 0.1413 Acc: 0.9468\n",
      "val Loss: 0.1000 Acc: 0.9655\n",
      "\n",
      "Epoch 59/299\n",
      "----------\n",
      "train Loss: 0.1361 Acc: 0.9511\n",
      "val Loss: 0.0951 Acc: 0.9693\n",
      "\n",
      "Epoch 60/299\n",
      "----------\n",
      "train Loss: 0.1374 Acc: 0.9512\n",
      "val Loss: 0.0974 Acc: 0.9680\n",
      "\n",
      "Epoch 61/299\n",
      "----------\n",
      "train Loss: 0.1343 Acc: 0.9530\n",
      "val Loss: 0.0951 Acc: 0.9685\n",
      "\n",
      "Epoch 62/299\n",
      "----------\n",
      "train Loss: 0.1314 Acc: 0.9531\n",
      "val Loss: 0.0941 Acc: 0.9693\n",
      "\n",
      "Epoch 63/299\n",
      "----------\n",
      "train Loss: 0.1383 Acc: 0.9519\n",
      "val Loss: 0.1025 Acc: 0.9651\n",
      "\n",
      "Epoch 64/299\n",
      "----------\n",
      "train Loss: 0.1360 Acc: 0.9506\n",
      "val Loss: 0.0965 Acc: 0.9680\n",
      "\n",
      "Epoch 65/299\n",
      "----------\n",
      "train Loss: 0.1346 Acc: 0.9516\n",
      "val Loss: 0.0937 Acc: 0.9676\n",
      "\n",
      "Epoch 66/299\n",
      "----------\n",
      "train Loss: 0.1350 Acc: 0.9516\n",
      "val Loss: 0.0934 Acc: 0.9705\n",
      "\n",
      "Epoch 67/299\n",
      "----------\n",
      "train Loss: 0.1340 Acc: 0.9505\n",
      "val Loss: 0.0968 Acc: 0.9697\n",
      "\n",
      "Epoch 68/299\n",
      "----------\n",
      "train Loss: 0.1340 Acc: 0.9519\n",
      "val Loss: 0.0943 Acc: 0.9672\n",
      "\n",
      "Epoch 69/299\n",
      "----------\n",
      "train Loss: 0.1362 Acc: 0.9535\n",
      "val Loss: 0.0944 Acc: 0.9689\n",
      "\n",
      "Epoch 70/299\n",
      "----------\n",
      "train Loss: 0.1327 Acc: 0.9532\n",
      "val Loss: 0.0940 Acc: 0.9701\n",
      "\n",
      "Epoch 71/299\n",
      "----------\n",
      "train Loss: 0.1297 Acc: 0.9567\n",
      "val Loss: 0.0930 Acc: 0.9680\n",
      "\n",
      "Epoch 72/299\n",
      "----------\n",
      "train Loss: 0.1353 Acc: 0.9518\n",
      "val Loss: 0.0942 Acc: 0.9676\n",
      "\n",
      "Epoch 73/299\n",
      "----------\n",
      "train Loss: 0.1326 Acc: 0.9546\n",
      "val Loss: 0.0921 Acc: 0.9672\n",
      "\n",
      "Epoch 74/299\n",
      "----------\n",
      "train Loss: 0.1356 Acc: 0.9538\n",
      "val Loss: 0.0948 Acc: 0.9664\n",
      "\n",
      "Epoch 75/299\n",
      "----------\n",
      "train Loss: 0.1308 Acc: 0.9537\n",
      "val Loss: 0.0937 Acc: 0.9676\n",
      "\n",
      "Epoch 76/299\n",
      "----------\n",
      "train Loss: 0.1316 Acc: 0.9533\n",
      "val Loss: 0.0922 Acc: 0.9697\n",
      "\n",
      "Epoch 77/299\n",
      "----------\n",
      "train Loss: 0.1315 Acc: 0.9542\n",
      "val Loss: 0.0910 Acc: 0.9718\n",
      "\n",
      "Epoch 78/299\n",
      "----------\n",
      "train Loss: 0.1332 Acc: 0.9539\n",
      "val Loss: 0.0948 Acc: 0.9697\n",
      "\n",
      "Epoch 79/299\n",
      "----------\n",
      "train Loss: 0.1305 Acc: 0.9534\n",
      "val Loss: 0.0911 Acc: 0.9714\n",
      "\n",
      "Epoch 80/299\n",
      "----------\n",
      "train Loss: 0.1291 Acc: 0.9540\n",
      "val Loss: 0.0905 Acc: 0.9714\n",
      "\n",
      "Epoch 81/299\n",
      "----------\n",
      "train Loss: 0.1336 Acc: 0.9500\n",
      "val Loss: 0.0933 Acc: 0.9689\n",
      "\n",
      "Epoch 82/299\n",
      "----------\n",
      "train Loss: 0.1305 Acc: 0.9558\n",
      "val Loss: 0.0931 Acc: 0.9693\n",
      "\n",
      "Epoch 83/299\n",
      "----------\n",
      "train Loss: 0.1287 Acc: 0.9530\n",
      "val Loss: 0.0908 Acc: 0.9689\n",
      "\n",
      "Epoch 84/299\n",
      "----------\n",
      "train Loss: 0.1296 Acc: 0.9535\n",
      "val Loss: 0.0900 Acc: 0.9680\n",
      "\n",
      "Epoch 85/299\n",
      "----------\n",
      "train Loss: 0.1257 Acc: 0.9553\n",
      "val Loss: 0.0911 Acc: 0.9701\n",
      "\n",
      "Epoch 86/299\n",
      "----------\n",
      "train Loss: 0.1293 Acc: 0.9549\n",
      "val Loss: 0.0910 Acc: 0.9705\n",
      "\n",
      "Epoch 87/299\n",
      "----------\n",
      "train Loss: 0.1345 Acc: 0.9522\n",
      "val Loss: 0.0980 Acc: 0.9643\n",
      "\n",
      "Epoch 88/299\n",
      "----------\n",
      "train Loss: 0.1296 Acc: 0.9543\n",
      "val Loss: 0.0905 Acc: 0.9672\n",
      "\n",
      "Epoch 89/299\n",
      "----------\n",
      "train Loss: 0.1301 Acc: 0.9530\n",
      "val Loss: 0.0939 Acc: 0.9685\n",
      "\n",
      "Epoch 90/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1284 Acc: 0.9532\n",
      "val Loss: 0.0900 Acc: 0.9685\n",
      "\n",
      "Epoch 91/299\n",
      "----------\n",
      "train Loss: 0.1331 Acc: 0.9528\n",
      "val Loss: 0.0891 Acc: 0.9676\n",
      "\n",
      "Epoch 92/299\n",
      "----------\n",
      "train Loss: 0.1282 Acc: 0.9530\n",
      "val Loss: 0.0941 Acc: 0.9685\n",
      "\n",
      "Epoch 93/299\n",
      "----------\n",
      "train Loss: 0.1298 Acc: 0.9526\n",
      "val Loss: 0.0910 Acc: 0.9660\n",
      "\n",
      "Epoch 94/299\n",
      "----------\n",
      "train Loss: 0.1242 Acc: 0.9564\n",
      "val Loss: 0.0891 Acc: 0.9685\n",
      "\n",
      "Epoch 95/299\n",
      "----------\n",
      "train Loss: 0.1286 Acc: 0.9538\n",
      "val Loss: 0.0888 Acc: 0.9685\n",
      "\n",
      "Epoch 96/299\n",
      "----------\n",
      "train Loss: 0.1306 Acc: 0.9541\n",
      "val Loss: 0.0890 Acc: 0.9672\n",
      "\n",
      "Epoch 97/299\n",
      "----------\n",
      "train Loss: 0.1248 Acc: 0.9546\n",
      "val Loss: 0.0900 Acc: 0.9709\n",
      "\n",
      "Epoch 98/299\n",
      "----------\n",
      "train Loss: 0.1262 Acc: 0.9535\n",
      "val Loss: 0.0898 Acc: 0.9709\n",
      "\n",
      "Epoch 99/299\n",
      "----------\n",
      "train Loss: 0.1249 Acc: 0.9558\n",
      "val Loss: 0.0890 Acc: 0.9655\n",
      "\n",
      "Epoch 100/299\n",
      "----------\n",
      "train Loss: 0.1262 Acc: 0.9544\n",
      "val Loss: 0.0896 Acc: 0.9693\n",
      "\n",
      "Epoch 101/299\n",
      "----------\n",
      "train Loss: 0.1228 Acc: 0.9559\n",
      "val Loss: 0.0921 Acc: 0.9680\n",
      "\n",
      "Epoch 102/299\n",
      "----------\n",
      "train Loss: 0.1204 Acc: 0.9589\n",
      "val Loss: 0.0889 Acc: 0.9685\n",
      "\n",
      "Epoch 103/299\n",
      "----------\n",
      "train Loss: 0.1276 Acc: 0.9541\n",
      "val Loss: 0.0932 Acc: 0.9660\n",
      "\n",
      "Epoch 104/299\n",
      "----------\n",
      "train Loss: 0.1231 Acc: 0.9567\n",
      "val Loss: 0.0892 Acc: 0.9672\n",
      "\n",
      "Epoch 105/299\n",
      "----------\n",
      "train Loss: 0.1225 Acc: 0.9562\n",
      "val Loss: 0.0880 Acc: 0.9689\n",
      "\n",
      "Epoch 106/299\n",
      "----------\n",
      "train Loss: 0.1258 Acc: 0.9567\n",
      "val Loss: 0.0885 Acc: 0.9709\n",
      "\n",
      "Epoch 107/299\n",
      "----------\n",
      "train Loss: 0.1225 Acc: 0.9568\n",
      "val Loss: 0.0892 Acc: 0.9680\n",
      "\n",
      "Epoch 108/299\n",
      "----------\n",
      "train Loss: 0.1251 Acc: 0.9560\n",
      "val Loss: 0.0887 Acc: 0.9689\n",
      "\n",
      "Epoch 109/299\n",
      "----------\n",
      "train Loss: 0.1240 Acc: 0.9561\n",
      "val Loss: 0.0908 Acc: 0.9676\n",
      "\n",
      "Epoch 110/299\n",
      "----------\n",
      "train Loss: 0.1308 Acc: 0.9533\n",
      "val Loss: 0.0953 Acc: 0.9668\n",
      "\n",
      "Epoch 111/299\n",
      "----------\n",
      "train Loss: 0.1205 Acc: 0.9572\n",
      "val Loss: 0.0905 Acc: 0.9697\n",
      "\n",
      "Epoch 112/299\n",
      "----------\n",
      "train Loss: 0.1239 Acc: 0.9557\n",
      "val Loss: 0.0920 Acc: 0.9672\n",
      "\n",
      "Epoch 113/299\n",
      "----------\n",
      "train Loss: 0.1288 Acc: 0.9523\n",
      "val Loss: 0.0874 Acc: 0.9697\n",
      "\n",
      "Epoch 114/299\n",
      "----------\n",
      "train Loss: 0.1269 Acc: 0.9544\n",
      "val Loss: 0.0887 Acc: 0.9685\n",
      "\n",
      "Epoch 115/299\n",
      "----------\n",
      "train Loss: 0.1256 Acc: 0.9555\n",
      "val Loss: 0.0895 Acc: 0.9693\n",
      "\n",
      "Epoch 116/299\n",
      "----------\n",
      "train Loss: 0.1247 Acc: 0.9566\n",
      "val Loss: 0.0891 Acc: 0.9693\n",
      "\n",
      "Epoch 117/299\n",
      "----------\n",
      "train Loss: 0.1232 Acc: 0.9561\n",
      "val Loss: 0.0952 Acc: 0.9689\n",
      "\n",
      "Epoch 118/299\n",
      "----------\n",
      "train Loss: 0.1245 Acc: 0.9540\n",
      "val Loss: 0.0909 Acc: 0.9685\n",
      "\n",
      "Epoch 119/299\n",
      "----------\n",
      "train Loss: 0.1221 Acc: 0.9564\n",
      "val Loss: 0.0884 Acc: 0.9701\n",
      "\n",
      "Epoch 120/299\n",
      "----------\n",
      "train Loss: 0.1286 Acc: 0.9532\n",
      "val Loss: 0.0880 Acc: 0.9714\n",
      "\n",
      "Epoch 121/299\n",
      "----------\n",
      "train Loss: 0.1159 Acc: 0.9610\n",
      "val Loss: 0.0878 Acc: 0.9693\n",
      "\n",
      "Epoch 122/299\n",
      "----------\n",
      "train Loss: 0.1282 Acc: 0.9545\n",
      "val Loss: 0.0888 Acc: 0.9668\n",
      "\n",
      "Epoch 123/299\n",
      "----------\n",
      "train Loss: 0.1242 Acc: 0.9536\n",
      "val Loss: 0.0871 Acc: 0.9693\n",
      "\n",
      "Epoch 124/299\n",
      "----------\n",
      "train Loss: 0.1284 Acc: 0.9541\n",
      "val Loss: 0.0876 Acc: 0.9672\n",
      "\n",
      "Epoch 125/299\n",
      "----------\n",
      "train Loss: 0.1220 Acc: 0.9567\n",
      "val Loss: 0.0866 Acc: 0.9705\n",
      "\n",
      "Epoch 126/299\n",
      "----------\n",
      "train Loss: 0.1252 Acc: 0.9539\n",
      "val Loss: 0.0917 Acc: 0.9693\n",
      "\n",
      "Epoch 127/299\n",
      "----------\n",
      "train Loss: 0.1207 Acc: 0.9570\n",
      "val Loss: 0.0852 Acc: 0.9701\n",
      "\n",
      "Epoch 128/299\n",
      "----------\n",
      "train Loss: 0.1179 Acc: 0.9571\n",
      "val Loss: 0.0901 Acc: 0.9697\n",
      "\n",
      "Epoch 129/299\n",
      "----------\n",
      "train Loss: 0.1209 Acc: 0.9558\n",
      "val Loss: 0.0854 Acc: 0.9714\n",
      "\n",
      "Epoch 130/299\n",
      "----------\n",
      "train Loss: 0.1269 Acc: 0.9539\n",
      "val Loss: 0.0843 Acc: 0.9718\n",
      "\n",
      "Epoch 131/299\n",
      "----------\n",
      "train Loss: 0.1285 Acc: 0.9526\n",
      "val Loss: 0.0904 Acc: 0.9680\n",
      "\n",
      "Epoch 132/299\n",
      "----------\n",
      "train Loss: 0.1154 Acc: 0.9583\n",
      "val Loss: 0.0851 Acc: 0.9697\n",
      "\n",
      "Epoch 133/299\n",
      "----------\n",
      "train Loss: 0.1182 Acc: 0.9568\n",
      "val Loss: 0.0858 Acc: 0.9697\n",
      "\n",
      "Epoch 134/299\n",
      "----------\n",
      "train Loss: 0.1234 Acc: 0.9534\n",
      "val Loss: 0.0868 Acc: 0.9689\n",
      "\n",
      "Epoch 135/299\n",
      "----------\n",
      "train Loss: 0.1218 Acc: 0.9533\n",
      "val Loss: 0.0864 Acc: 0.9680\n",
      "\n",
      "Epoch 136/299\n",
      "----------\n",
      "train Loss: 0.1227 Acc: 0.9558\n",
      "val Loss: 0.0846 Acc: 0.9697\n",
      "\n",
      "Epoch 137/299\n",
      "----------\n",
      "train Loss: 0.1183 Acc: 0.9581\n",
      "val Loss: 0.0860 Acc: 0.9689\n",
      "\n",
      "Epoch 138/299\n",
      "----------\n",
      "train Loss: 0.1199 Acc: 0.9574\n",
      "val Loss: 0.0871 Acc: 0.9693\n",
      "\n",
      "Epoch 139/299\n",
      "----------\n",
      "train Loss: 0.1191 Acc: 0.9588\n",
      "val Loss: 0.0851 Acc: 0.9701\n",
      "\n",
      "Epoch 140/299\n",
      "----------\n",
      "train Loss: 0.1208 Acc: 0.9573\n",
      "val Loss: 0.0875 Acc: 0.9676\n",
      "\n",
      "Epoch 141/299\n",
      "----------\n",
      "train Loss: 0.1186 Acc: 0.9532\n",
      "val Loss: 0.0857 Acc: 0.9689\n",
      "\n",
      "Epoch 142/299\n",
      "----------\n",
      "train Loss: 0.1243 Acc: 0.9549\n",
      "val Loss: 0.0878 Acc: 0.9668\n",
      "\n",
      "Epoch 143/299\n",
      "----------\n",
      "train Loss: 0.1173 Acc: 0.9580\n",
      "val Loss: 0.0882 Acc: 0.9680\n",
      "\n",
      "Epoch 144/299\n",
      "----------\n",
      "train Loss: 0.1214 Acc: 0.9584\n",
      "val Loss: 0.0878 Acc: 0.9668\n",
      "\n",
      "Epoch 145/299\n",
      "----------\n",
      "train Loss: 0.1183 Acc: 0.9592\n",
      "val Loss: 0.0852 Acc: 0.9718\n",
      "\n",
      "Epoch 146/299\n",
      "----------\n",
      "train Loss: 0.1232 Acc: 0.9538\n",
      "val Loss: 0.0851 Acc: 0.9693\n",
      "\n",
      "Epoch 147/299\n",
      "----------\n",
      "train Loss: 0.1183 Acc: 0.9572\n",
      "val Loss: 0.0864 Acc: 0.9701\n",
      "\n",
      "Epoch 148/299\n",
      "----------\n",
      "train Loss: 0.1150 Acc: 0.9585\n",
      "val Loss: 0.0840 Acc: 0.9701\n",
      "\n",
      "Epoch 149/299\n",
      "----------\n",
      "train Loss: 0.1134 Acc: 0.9602\n",
      "val Loss: 0.0904 Acc: 0.9660\n",
      "\n",
      "Epoch 150/299\n",
      "----------\n",
      "train Loss: 0.1138 Acc: 0.9582\n",
      "val Loss: 0.0837 Acc: 0.9714\n",
      "\n",
      "Epoch 151/299\n",
      "----------\n",
      "train Loss: 0.1202 Acc: 0.9562\n",
      "val Loss: 0.0875 Acc: 0.9718\n",
      "\n",
      "Epoch 152/299\n",
      "----------\n",
      "train Loss: 0.1161 Acc: 0.9567\n",
      "val Loss: 0.0845 Acc: 0.9709\n",
      "\n",
      "Epoch 153/299\n",
      "----------\n",
      "train Loss: 0.1196 Acc: 0.9560\n",
      "val Loss: 0.0845 Acc: 0.9714\n",
      "\n",
      "Epoch 154/299\n",
      "----------\n",
      "train Loss: 0.1154 Acc: 0.9585\n",
      "val Loss: 0.0883 Acc: 0.9701\n",
      "\n",
      "Epoch 155/299\n",
      "----------\n",
      "train Loss: 0.1152 Acc: 0.9577\n",
      "val Loss: 0.0840 Acc: 0.9726\n",
      "\n",
      "Epoch 156/299\n",
      "----------\n",
      "train Loss: 0.1187 Acc: 0.9561\n",
      "val Loss: 0.0857 Acc: 0.9685\n",
      "\n",
      "Epoch 157/299\n",
      "----------\n",
      "train Loss: 0.1181 Acc: 0.9570\n",
      "val Loss: 0.0881 Acc: 0.9664\n",
      "\n",
      "Epoch 158/299\n",
      "----------\n",
      "train Loss: 0.1148 Acc: 0.9576\n",
      "val Loss: 0.0838 Acc: 0.9709\n",
      "\n",
      "Epoch 159/299\n",
      "----------\n",
      "train Loss: 0.1141 Acc: 0.9594\n",
      "val Loss: 0.0835 Acc: 0.9701\n",
      "\n",
      "Epoch 160/299\n",
      "----------\n",
      "train Loss: 0.1134 Acc: 0.9608\n",
      "val Loss: 0.0876 Acc: 0.9701\n",
      "\n",
      "Epoch 161/299\n",
      "----------\n",
      "train Loss: 0.1190 Acc: 0.9560\n",
      "val Loss: 0.0861 Acc: 0.9693\n",
      "\n",
      "Epoch 162/299\n",
      "----------\n",
      "train Loss: 0.1194 Acc: 0.9577\n",
      "val Loss: 0.0902 Acc: 0.9668\n",
      "\n",
      "Epoch 163/299\n",
      "----------\n",
      "train Loss: 0.1188 Acc: 0.9566\n",
      "val Loss: 0.0834 Acc: 0.9709\n",
      "\n",
      "Epoch 164/299\n",
      "----------\n",
      "train Loss: 0.1194 Acc: 0.9567\n",
      "val Loss: 0.0854 Acc: 0.9693\n",
      "\n",
      "Epoch 165/299\n",
      "----------\n",
      "train Loss: 0.1252 Acc: 0.9558\n",
      "val Loss: 0.0900 Acc: 0.9689\n",
      "\n",
      "Epoch 166/299\n",
      "----------\n",
      "train Loss: 0.1228 Acc: 0.9557\n",
      "val Loss: 0.0865 Acc: 0.9689\n",
      "\n",
      "Epoch 167/299\n",
      "----------\n",
      "train Loss: 0.1156 Acc: 0.9584\n",
      "val Loss: 0.0882 Acc: 0.9672\n",
      "\n",
      "Epoch 168/299\n",
      "----------\n",
      "train Loss: 0.1155 Acc: 0.9580\n",
      "val Loss: 0.0873 Acc: 0.9697\n",
      "\n",
      "Epoch 169/299\n",
      "----------\n",
      "train Loss: 0.1221 Acc: 0.9564\n",
      "val Loss: 0.0870 Acc: 0.9689\n",
      "\n",
      "Epoch 170/299\n",
      "----------\n",
      "train Loss: 0.1086 Acc: 0.9597\n",
      "val Loss: 0.0860 Acc: 0.9718\n",
      "\n",
      "Epoch 171/299\n",
      "----------\n",
      "train Loss: 0.1196 Acc: 0.9560\n",
      "val Loss: 0.0830 Acc: 0.9722\n",
      "\n",
      "Epoch 172/299\n",
      "----------\n",
      "train Loss: 0.1113 Acc: 0.9611\n",
      "val Loss: 0.0858 Acc: 0.9697\n",
      "\n",
      "Epoch 173/299\n",
      "----------\n",
      "train Loss: 0.1155 Acc: 0.9587\n",
      "val Loss: 0.0830 Acc: 0.9697\n",
      "\n",
      "Epoch 174/299\n",
      "----------\n",
      "train Loss: 0.1137 Acc: 0.9590\n",
      "val Loss: 0.0831 Acc: 0.9697\n",
      "\n",
      "Epoch 175/299\n",
      "----------\n",
      "train Loss: 0.1154 Acc: 0.9583\n",
      "val Loss: 0.0852 Acc: 0.9685\n",
      "\n",
      "Epoch 176/299\n",
      "----------\n",
      "train Loss: 0.1154 Acc: 0.9575\n",
      "val Loss: 0.0829 Acc: 0.9718\n",
      "\n",
      "Epoch 177/299\n",
      "----------\n",
      "train Loss: 0.1204 Acc: 0.9556\n",
      "val Loss: 0.0839 Acc: 0.9685\n",
      "\n",
      "Epoch 178/299\n",
      "----------\n",
      "train Loss: 0.1206 Acc: 0.9548\n",
      "val Loss: 0.0862 Acc: 0.9689\n",
      "\n",
      "Epoch 179/299\n",
      "----------\n",
      "train Loss: 0.1221 Acc: 0.9556\n",
      "val Loss: 0.0890 Acc: 0.9685\n",
      "\n",
      "Epoch 180/299\n",
      "----------\n",
      "train Loss: 0.1162 Acc: 0.9576\n",
      "val Loss: 0.0828 Acc: 0.9705\n",
      "\n",
      "Epoch 181/299\n",
      "----------\n",
      "train Loss: 0.1191 Acc: 0.9581\n",
      "val Loss: 0.0869 Acc: 0.9685\n",
      "\n",
      "Epoch 182/299\n",
      "----------\n",
      "train Loss: 0.1157 Acc: 0.9576\n",
      "val Loss: 0.0840 Acc: 0.9714\n",
      "\n",
      "Epoch 183/299\n",
      "----------\n",
      "train Loss: 0.1168 Acc: 0.9587\n",
      "val Loss: 0.0899 Acc: 0.9676\n",
      "\n",
      "Epoch 184/299\n",
      "----------\n",
      "train Loss: 0.1191 Acc: 0.9564\n",
      "val Loss: 0.0818 Acc: 0.9726\n",
      "\n",
      "Epoch 185/299\n",
      "----------\n",
      "train Loss: 0.1170 Acc: 0.9570\n",
      "val Loss: 0.0819 Acc: 0.9701\n",
      "\n",
      "Epoch 186/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1133 Acc: 0.9598\n",
      "val Loss: 0.0972 Acc: 0.9643\n",
      "\n",
      "Epoch 187/299\n",
      "----------\n",
      "train Loss: 0.1106 Acc: 0.9587\n",
      "val Loss: 0.0844 Acc: 0.9689\n",
      "\n",
      "Epoch 188/299\n",
      "----------\n",
      "train Loss: 0.1185 Acc: 0.9576\n",
      "val Loss: 0.0833 Acc: 0.9689\n",
      "\n",
      "Epoch 189/299\n",
      "----------\n",
      "train Loss: 0.1133 Acc: 0.9589\n",
      "val Loss: 0.0855 Acc: 0.9685\n",
      "\n",
      "Epoch 190/299\n",
      "----------\n",
      "train Loss: 0.1124 Acc: 0.9591\n",
      "val Loss: 0.0841 Acc: 0.9689\n",
      "\n",
      "Epoch 191/299\n",
      "----------\n",
      "train Loss: 0.1209 Acc: 0.9570\n",
      "val Loss: 0.0862 Acc: 0.9693\n",
      "\n",
      "Epoch 192/299\n",
      "----------\n",
      "train Loss: 0.1168 Acc: 0.9575\n",
      "val Loss: 0.0862 Acc: 0.9672\n",
      "\n",
      "Epoch 193/299\n",
      "----------\n",
      "train Loss: 0.1164 Acc: 0.9559\n",
      "val Loss: 0.0881 Acc: 0.9705\n",
      "\n",
      "Epoch 194/299\n",
      "----------\n",
      "train Loss: 0.1125 Acc: 0.9578\n",
      "val Loss: 0.0860 Acc: 0.9701\n",
      "\n",
      "Epoch 195/299\n",
      "----------\n",
      "train Loss: 0.1149 Acc: 0.9575\n",
      "val Loss: 0.0859 Acc: 0.9693\n",
      "\n",
      "Epoch 196/299\n",
      "----------\n",
      "train Loss: 0.1158 Acc: 0.9577\n",
      "val Loss: 0.0833 Acc: 0.9689\n",
      "\n",
      "Epoch 197/299\n",
      "----------\n",
      "train Loss: 0.1176 Acc: 0.9560\n",
      "val Loss: 0.0847 Acc: 0.9701\n",
      "\n",
      "Epoch 198/299\n",
      "----------\n",
      "train Loss: 0.1155 Acc: 0.9570\n",
      "val Loss: 0.0880 Acc: 0.9693\n",
      "\n",
      "Epoch 199/299\n",
      "----------\n",
      "train Loss: 0.1128 Acc: 0.9593\n",
      "val Loss: 0.0894 Acc: 0.9697\n",
      "\n",
      "Epoch 200/299\n",
      "----------\n",
      "train Loss: 0.1155 Acc: 0.9578\n",
      "val Loss: 0.0869 Acc: 0.9689\n",
      "\n",
      "Epoch 201/299\n",
      "----------\n",
      "train Loss: 0.1160 Acc: 0.9575\n",
      "val Loss: 0.0854 Acc: 0.9689\n",
      "\n",
      "Epoch 202/299\n",
      "----------\n",
      "train Loss: 0.1198 Acc: 0.9564\n",
      "val Loss: 0.0873 Acc: 0.9680\n",
      "\n",
      "Epoch 203/299\n",
      "----------\n",
      "train Loss: 0.1116 Acc: 0.9580\n",
      "val Loss: 0.0872 Acc: 0.9689\n",
      "\n",
      "Epoch 204/299\n",
      "----------\n",
      "train Loss: 0.1205 Acc: 0.9555\n",
      "val Loss: 0.0844 Acc: 0.9709\n",
      "\n",
      "Epoch 205/299\n",
      "----------\n",
      "train Loss: 0.1172 Acc: 0.9578\n",
      "val Loss: 0.0836 Acc: 0.9697\n",
      "\n",
      "Epoch 206/299\n",
      "----------\n",
      "train Loss: 0.1173 Acc: 0.9576\n",
      "val Loss: 0.0833 Acc: 0.9709\n",
      "\n",
      "Epoch 207/299\n",
      "----------\n",
      "train Loss: 0.1124 Acc: 0.9588\n",
      "val Loss: 0.0856 Acc: 0.9689\n",
      "\n",
      "Epoch 208/299\n",
      "----------\n",
      "train Loss: 0.1135 Acc: 0.9583\n",
      "val Loss: 0.0888 Acc: 0.9668\n",
      "\n",
      "Epoch 209/299\n",
      "----------\n",
      "train Loss: 0.1093 Acc: 0.9595\n",
      "val Loss: 0.0818 Acc: 0.9709\n",
      "\n",
      "Epoch 210/299\n",
      "----------\n",
      "train Loss: 0.1176 Acc: 0.9562\n",
      "val Loss: 0.0832 Acc: 0.9718\n",
      "\n",
      "Epoch 211/299\n",
      "----------\n",
      "train Loss: 0.1162 Acc: 0.9569\n",
      "val Loss: 0.0824 Acc: 0.9705\n",
      "\n",
      "Epoch 212/299\n",
      "----------\n",
      "train Loss: 0.1122 Acc: 0.9585\n",
      "val Loss: 0.0895 Acc: 0.9689\n",
      "\n",
      "Epoch 213/299\n",
      "----------\n",
      "train Loss: 0.1164 Acc: 0.9593\n",
      "val Loss: 0.0825 Acc: 0.9709\n",
      "\n",
      "Epoch 214/299\n",
      "----------\n",
      "train Loss: 0.1132 Acc: 0.9569\n",
      "val Loss: 0.0845 Acc: 0.9685\n",
      "\n",
      "Epoch 215/299\n",
      "----------\n",
      "train Loss: 0.1147 Acc: 0.9585\n",
      "val Loss: 0.0828 Acc: 0.9705\n",
      "\n",
      "Epoch 216/299\n",
      "----------\n",
      "train Loss: 0.1149 Acc: 0.9575\n",
      "val Loss: 0.0844 Acc: 0.9705\n",
      "\n",
      "Epoch 217/299\n",
      "----------\n",
      "train Loss: 0.1125 Acc: 0.9591\n",
      "val Loss: 0.0848 Acc: 0.9701\n",
      "\n",
      "Epoch 218/299\n",
      "----------\n",
      "train Loss: 0.1139 Acc: 0.9608\n",
      "val Loss: 0.0839 Acc: 0.9714\n",
      "\n",
      "Epoch 219/299\n",
      "----------\n",
      "train Loss: 0.1075 Acc: 0.9616\n",
      "val Loss: 0.0836 Acc: 0.9714\n",
      "\n",
      "Epoch 220/299\n",
      "----------\n",
      "train Loss: 0.1139 Acc: 0.9609\n",
      "val Loss: 0.0838 Acc: 0.9676\n",
      "\n",
      "Epoch 221/299\n",
      "----------\n",
      "train Loss: 0.1110 Acc: 0.9628\n",
      "val Loss: 0.0843 Acc: 0.9685\n",
      "\n",
      "Epoch 222/299\n",
      "----------\n",
      "train Loss: 0.1173 Acc: 0.9560\n",
      "val Loss: 0.0836 Acc: 0.9709\n",
      "\n",
      "Epoch 223/299\n",
      "----------\n",
      "train Loss: 0.1185 Acc: 0.9563\n",
      "val Loss: 0.0833 Acc: 0.9705\n",
      "\n",
      "Epoch 224/299\n",
      "----------\n",
      "train Loss: 0.1124 Acc: 0.9601\n",
      "val Loss: 0.0838 Acc: 0.9697\n",
      "\n",
      "Epoch 225/299\n",
      "----------\n",
      "train Loss: 0.1156 Acc: 0.9595\n",
      "val Loss: 0.0844 Acc: 0.9705\n",
      "\n",
      "Epoch 226/299\n",
      "----------\n",
      "train Loss: 0.1115 Acc: 0.9596\n",
      "val Loss: 0.0873 Acc: 0.9680\n",
      "\n",
      "Epoch 227/299\n",
      "----------\n",
      "train Loss: 0.1100 Acc: 0.9599\n",
      "val Loss: 0.0813 Acc: 0.9714\n",
      "\n",
      "Epoch 228/299\n",
      "----------\n",
      "train Loss: 0.1093 Acc: 0.9613\n",
      "val Loss: 0.0833 Acc: 0.9697\n",
      "\n",
      "Epoch 229/299\n",
      "----------\n",
      "train Loss: 0.1128 Acc: 0.9600\n",
      "val Loss: 0.0844 Acc: 0.9697\n",
      "\n",
      "Epoch 230/299\n",
      "----------\n",
      "train Loss: 0.1178 Acc: 0.9551\n",
      "val Loss: 0.0848 Acc: 0.9685\n",
      "\n",
      "Epoch 231/299\n",
      "----------\n",
      "train Loss: 0.1122 Acc: 0.9591\n",
      "val Loss: 0.0847 Acc: 0.9697\n",
      "\n",
      "Epoch 232/299\n",
      "----------\n",
      "train Loss: 0.1158 Acc: 0.9560\n",
      "val Loss: 0.0824 Acc: 0.9697\n",
      "\n",
      "Epoch 233/299\n",
      "----------\n",
      "train Loss: 0.1103 Acc: 0.9598\n",
      "val Loss: 0.0819 Acc: 0.9722\n",
      "\n",
      "Epoch 234/299\n",
      "----------\n",
      "train Loss: 0.1117 Acc: 0.9605\n",
      "val Loss: 0.0838 Acc: 0.9685\n",
      "\n",
      "Epoch 235/299\n",
      "----------\n",
      "train Loss: 0.1107 Acc: 0.9573\n",
      "val Loss: 0.0879 Acc: 0.9680\n",
      "\n",
      "Epoch 236/299\n",
      "----------\n",
      "train Loss: 0.1091 Acc: 0.9624\n",
      "val Loss: 0.0819 Acc: 0.9697\n",
      "\n",
      "Epoch 237/299\n",
      "----------\n",
      "train Loss: 0.1163 Acc: 0.9571\n",
      "val Loss: 0.0839 Acc: 0.9697\n",
      "\n",
      "Epoch 238/299\n",
      "----------\n",
      "train Loss: 0.1157 Acc: 0.9568\n",
      "val Loss: 0.0828 Acc: 0.9701\n",
      "\n",
      "Epoch 239/299\n",
      "----------\n",
      "train Loss: 0.1143 Acc: 0.9569\n",
      "val Loss: 0.0825 Acc: 0.9709\n",
      "\n",
      "Epoch 240/299\n",
      "----------\n",
      "train Loss: 0.1051 Acc: 0.9614\n",
      "val Loss: 0.0802 Acc: 0.9718\n",
      "\n",
      "Epoch 241/299\n",
      "----------\n",
      "train Loss: 0.1111 Acc: 0.9596\n",
      "val Loss: 0.0821 Acc: 0.9697\n",
      "\n",
      "Epoch 242/299\n",
      "----------\n",
      "train Loss: 0.1133 Acc: 0.9580\n",
      "val Loss: 0.0839 Acc: 0.9693\n",
      "\n",
      "Epoch 243/299\n",
      "----------\n",
      "train Loss: 0.1148 Acc: 0.9551\n",
      "val Loss: 0.0839 Acc: 0.9701\n",
      "\n",
      "Epoch 244/299\n",
      "----------\n",
      "train Loss: 0.1138 Acc: 0.9568\n",
      "val Loss: 0.0810 Acc: 0.9722\n",
      "\n",
      "Epoch 245/299\n",
      "----------\n",
      "train Loss: 0.1150 Acc: 0.9572\n",
      "val Loss: 0.0815 Acc: 0.9701\n",
      "\n",
      "Epoch 246/299\n",
      "----------\n",
      "train Loss: 0.1115 Acc: 0.9602\n",
      "val Loss: 0.0884 Acc: 0.9689\n",
      "\n",
      "Epoch 247/299\n",
      "----------\n",
      "train Loss: 0.1110 Acc: 0.9592\n",
      "val Loss: 0.0851 Acc: 0.9689\n",
      "\n",
      "Epoch 248/299\n",
      "----------\n",
      "train Loss: 0.1049 Acc: 0.9614\n",
      "val Loss: 0.0826 Acc: 0.9714\n",
      "\n",
      "Epoch 249/299\n",
      "----------\n",
      "train Loss: 0.1131 Acc: 0.9593\n",
      "val Loss: 0.0826 Acc: 0.9689\n",
      "\n",
      "Epoch 250/299\n",
      "----------\n",
      "train Loss: 0.1135 Acc: 0.9576\n",
      "val Loss: 0.0805 Acc: 0.9689\n",
      "\n",
      "Epoch 251/299\n",
      "----------\n",
      "train Loss: 0.1151 Acc: 0.9568\n",
      "val Loss: 0.0861 Acc: 0.9689\n",
      "\n",
      "Epoch 252/299\n",
      "----------\n",
      "train Loss: 0.1087 Acc: 0.9608\n",
      "val Loss: 0.0816 Acc: 0.9726\n",
      "\n",
      "Epoch 253/299\n",
      "----------\n",
      "train Loss: 0.1076 Acc: 0.9602\n",
      "val Loss: 0.0842 Acc: 0.9685\n",
      "\n",
      "Epoch 254/299\n",
      "----------\n",
      "train Loss: 0.1113 Acc: 0.9601\n",
      "val Loss: 0.0840 Acc: 0.9693\n",
      "\n",
      "Epoch 255/299\n",
      "----------\n",
      "train Loss: 0.1179 Acc: 0.9567\n",
      "val Loss: 0.0816 Acc: 0.9705\n",
      "\n",
      "Epoch 256/299\n",
      "----------\n",
      "train Loss: 0.1078 Acc: 0.9607\n",
      "val Loss: 0.0838 Acc: 0.9676\n",
      "\n",
      "Epoch 257/299\n",
      "----------\n",
      "train Loss: 0.1134 Acc: 0.9576\n",
      "val Loss: 0.0827 Acc: 0.9693\n",
      "\n",
      "Epoch 258/299\n",
      "----------\n",
      "train Loss: 0.1177 Acc: 0.9561\n",
      "val Loss: 0.0873 Acc: 0.9668\n",
      "\n",
      "Epoch 259/299\n",
      "----------\n",
      "train Loss: 0.1156 Acc: 0.9568\n",
      "val Loss: 0.0821 Acc: 0.9709\n",
      "\n",
      "Epoch 260/299\n",
      "----------\n",
      "train Loss: 0.1082 Acc: 0.9593\n",
      "val Loss: 0.0849 Acc: 0.9693\n",
      "\n",
      "Epoch 261/299\n",
      "----------\n",
      "train Loss: 0.1133 Acc: 0.9587\n",
      "val Loss: 0.0836 Acc: 0.9701\n",
      "\n",
      "Epoch 262/299\n",
      "----------\n",
      "train Loss: 0.1092 Acc: 0.9600\n",
      "val Loss: 0.0853 Acc: 0.9701\n",
      "\n",
      "Epoch 263/299\n",
      "----------\n",
      "train Loss: 0.1144 Acc: 0.9568\n",
      "val Loss: 0.0803 Acc: 0.9705\n",
      "\n",
      "Epoch 264/299\n",
      "----------\n",
      "train Loss: 0.1124 Acc: 0.9592\n",
      "val Loss: 0.0785 Acc: 0.9709\n",
      "\n",
      "Epoch 265/299\n",
      "----------\n",
      "train Loss: 0.1079 Acc: 0.9600\n",
      "val Loss: 0.0837 Acc: 0.9676\n",
      "\n",
      "Epoch 266/299\n",
      "----------\n",
      "train Loss: 0.1188 Acc: 0.9566\n",
      "val Loss: 0.0836 Acc: 0.9680\n",
      "\n",
      "Epoch 267/299\n",
      "----------\n",
      "train Loss: 0.1096 Acc: 0.9604\n",
      "val Loss: 0.0846 Acc: 0.9714\n",
      "\n",
      "Epoch 268/299\n",
      "----------\n",
      "train Loss: 0.1085 Acc: 0.9618\n",
      "val Loss: 0.0820 Acc: 0.9714\n",
      "\n",
      "Epoch 269/299\n",
      "----------\n",
      "train Loss: 0.1126 Acc: 0.9577\n",
      "val Loss: 0.0819 Acc: 0.9705\n",
      "\n",
      "Epoch 270/299\n",
      "----------\n",
      "train Loss: 0.1113 Acc: 0.9571\n",
      "val Loss: 0.0833 Acc: 0.9685\n",
      "\n",
      "Epoch 271/299\n",
      "----------\n",
      "train Loss: 0.1175 Acc: 0.9571\n",
      "val Loss: 0.0815 Acc: 0.9697\n",
      "\n",
      "Epoch 272/299\n",
      "----------\n",
      "train Loss: 0.1068 Acc: 0.9592\n",
      "val Loss: 0.0854 Acc: 0.9689\n",
      "\n",
      "Epoch 273/299\n",
      "----------\n",
      "train Loss: 0.1094 Acc: 0.9613\n",
      "val Loss: 0.0876 Acc: 0.9689\n",
      "\n",
      "Epoch 274/299\n",
      "----------\n",
      "train Loss: 0.1130 Acc: 0.9593\n",
      "val Loss: 0.0883 Acc: 0.9680\n",
      "\n",
      "Epoch 275/299\n",
      "----------\n",
      "train Loss: 0.1082 Acc: 0.9607\n",
      "val Loss: 0.0832 Acc: 0.9685\n",
      "\n",
      "Epoch 276/299\n",
      "----------\n",
      "train Loss: 0.1126 Acc: 0.9569\n",
      "val Loss: 0.0813 Acc: 0.9701\n",
      "\n",
      "Epoch 277/299\n",
      "----------\n",
      "train Loss: 0.1100 Acc: 0.9611\n",
      "val Loss: 0.0879 Acc: 0.9689\n",
      "\n",
      "Epoch 278/299\n",
      "----------\n",
      "train Loss: 0.1100 Acc: 0.9585\n",
      "val Loss: 0.0803 Acc: 0.9709\n",
      "\n",
      "Epoch 279/299\n",
      "----------\n",
      "train Loss: 0.1148 Acc: 0.9567\n",
      "val Loss: 0.0821 Acc: 0.9697\n",
      "\n",
      "Epoch 280/299\n",
      "----------\n",
      "train Loss: 0.1157 Acc: 0.9582\n",
      "val Loss: 0.0808 Acc: 0.9705\n",
      "\n",
      "Epoch 281/299\n",
      "----------\n",
      "train Loss: 0.1132 Acc: 0.9580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0812 Acc: 0.9701\n",
      "\n",
      "Epoch 282/299\n",
      "----------\n",
      "train Loss: 0.1023 Acc: 0.9642\n",
      "val Loss: 0.0834 Acc: 0.9689\n",
      "\n",
      "Epoch 283/299\n",
      "----------\n",
      "train Loss: 0.1136 Acc: 0.9601\n",
      "val Loss: 0.0806 Acc: 0.9726\n",
      "\n",
      "Epoch 284/299\n",
      "----------\n",
      "train Loss: 0.1109 Acc: 0.9607\n",
      "val Loss: 0.0781 Acc: 0.9718\n",
      "\n",
      "Epoch 285/299\n",
      "----------\n",
      "train Loss: 0.1143 Acc: 0.9575\n",
      "val Loss: 0.0812 Acc: 0.9734\n",
      "\n",
      "Epoch 286/299\n",
      "----------\n",
      "train Loss: 0.1106 Acc: 0.9577\n",
      "val Loss: 0.0824 Acc: 0.9685\n",
      "\n",
      "Epoch 287/299\n",
      "----------\n",
      "train Loss: 0.1114 Acc: 0.9599\n",
      "val Loss: 0.0801 Acc: 0.9722\n",
      "\n",
      "Epoch 288/299\n",
      "----------\n",
      "train Loss: 0.1100 Acc: 0.9570\n",
      "val Loss: 0.0796 Acc: 0.9709\n",
      "\n",
      "Epoch 289/299\n",
      "----------\n",
      "train Loss: 0.1115 Acc: 0.9587\n",
      "val Loss: 0.0814 Acc: 0.9709\n",
      "\n",
      "Epoch 290/299\n",
      "----------\n",
      "train Loss: 0.1141 Acc: 0.9594\n",
      "val Loss: 0.0828 Acc: 0.9685\n",
      "\n",
      "Epoch 291/299\n",
      "----------\n",
      "train Loss: 0.1135 Acc: 0.9585\n",
      "val Loss: 0.0818 Acc: 0.9709\n",
      "\n",
      "Epoch 292/299\n",
      "----------\n",
      "train Loss: 0.1141 Acc: 0.9575\n",
      "val Loss: 0.0794 Acc: 0.9726\n",
      "\n",
      "Epoch 293/299\n",
      "----------\n",
      "train Loss: 0.1074 Acc: 0.9603\n",
      "val Loss: 0.0793 Acc: 0.9709\n",
      "\n",
      "Epoch 294/299\n",
      "----------\n",
      "train Loss: 0.1162 Acc: 0.9561\n",
      "val Loss: 0.0862 Acc: 0.9709\n",
      "\n",
      "Epoch 295/299\n",
      "----------\n",
      "train Loss: 0.1082 Acc: 0.9587\n",
      "val Loss: 0.0824 Acc: 0.9718\n",
      "\n",
      "Epoch 296/299\n",
      "----------\n",
      "train Loss: 0.1118 Acc: 0.9585\n",
      "val Loss: 0.0846 Acc: 0.9689\n",
      "\n",
      "Epoch 297/299\n",
      "----------\n",
      "train Loss: 0.1107 Acc: 0.9567\n",
      "val Loss: 0.0836 Acc: 0.9668\n",
      "\n",
      "Epoch 298/299\n",
      "----------\n",
      "train Loss: 0.1068 Acc: 0.9627\n",
      "val Loss: 0.0875 Acc: 0.9664\n",
      "\n",
      "Epoch 299/299\n",
      "----------\n",
      "train Loss: 0.1099 Acc: 0.9594\n",
      "val Loss: 0.0830 Acc: 0.9697\n",
      "\n",
      "\n",
      "##############################\n",
      "------ Summary ------\n",
      "model -> densenet\n",
      "epochs -> 300\n",
      "lr -> 0.0001\n",
      "batch size -> 8\n",
      "\n",
      "Training complete in 178m 52s\n",
      "Best val Acc: 0.973433\n",
      "##############################\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== END ====\n",
      "[[790   0   1]\n",
      " [  0 768  31]\n",
      " [  0  32 787]]\n",
      "\n",
      "Sensitivity or recall total\n",
      "0.9734329597343296\n",
      "\n",
      "Sensitivity or recall per classes\n",
      "[1.   0.96 0.96]\n",
      "\n",
      "Precision\n",
      "[1.   0.96 0.96]\n",
      "\n",
      "F1 Score\n",
      "[1.   0.96 0.96]\n",
      "Confusion matrix, without normalization\n",
      "\n",
      "============================================================\n",
      "==== INITIALIZING WITH PARAMETERS: ====\n",
      "model -> resnet\n",
      "epochs -> 300\n",
      "lr -> 0.0001\n",
      "batch size -> 8\n",
      "\n",
      "--------------------\n",
      "Params to learn:\n",
      "\t fc.weight\n",
      "\t fc.bias\n",
      "\n",
      "--------------------\n",
      "\n",
      "== Epochs ==\n",
      "Epoch 0/299\n",
      "----------\n",
      "train Loss: 0.6510 Acc: 0.7821\n",
      "val Loss: 0.4019 Acc: 0.9058\n",
      "\n",
      "Epoch 1/299\n",
      "----------\n",
      "train Loss: 0.3823 Acc: 0.8952\n",
      "val Loss: 0.3127 Acc: 0.9070\n",
      "\n",
      "Epoch 2/299\n",
      "----------\n",
      "train Loss: 0.3175 Acc: 0.9038\n",
      "val Loss: 0.2592 Acc: 0.9199\n",
      "\n",
      "Epoch 3/299\n",
      "----------\n",
      "train Loss: 0.2915 Acc: 0.9105\n",
      "val Loss: 0.2629 Acc: 0.9182\n",
      "\n",
      "Epoch 4/299\n",
      "----------\n",
      "train Loss: 0.2698 Acc: 0.9155\n",
      "val Loss: 0.2513 Acc: 0.9116\n",
      "\n",
      "Epoch 5/299\n",
      "----------\n",
      "train Loss: 0.2536 Acc: 0.9196\n",
      "val Loss: 0.1956 Acc: 0.9402\n",
      "\n",
      "Epoch 6/299\n",
      "----------\n",
      "train Loss: 0.2437 Acc: 0.9217\n",
      "val Loss: 0.2554 Acc: 0.9062\n",
      "\n",
      "Epoch 7/299\n",
      "----------\n",
      "train Loss: 0.2288 Acc: 0.9265\n",
      "val Loss: 0.1808 Acc: 0.9448\n",
      "\n",
      "Epoch 8/299\n",
      "----------\n",
      "train Loss: 0.2293 Acc: 0.9237\n",
      "val Loss: 0.1724 Acc: 0.9440\n",
      "\n",
      "Epoch 9/299\n",
      "----------\n",
      "train Loss: 0.2205 Acc: 0.9272\n",
      "val Loss: 0.1992 Acc: 0.9294\n",
      "\n",
      "Epoch 10/299\n",
      "----------\n",
      "train Loss: 0.2223 Acc: 0.9223\n",
      "val Loss: 0.1599 Acc: 0.9506\n",
      "\n",
      "Epoch 11/299\n",
      "----------\n",
      "train Loss: 0.2146 Acc: 0.9291\n",
      "val Loss: 0.1602 Acc: 0.9473\n",
      "\n",
      "Epoch 12/299\n",
      "----------\n",
      "train Loss: 0.2150 Acc: 0.9283\n",
      "val Loss: 0.1603 Acc: 0.9452\n",
      "\n",
      "Epoch 13/299\n",
      "----------\n",
      "train Loss: 0.2052 Acc: 0.9315\n",
      "val Loss: 0.1765 Acc: 0.9361\n",
      "\n",
      "Epoch 14/299\n",
      "----------\n",
      "train Loss: 0.2013 Acc: 0.9328\n",
      "val Loss: 0.1860 Acc: 0.9311\n",
      "\n",
      "Epoch 15/299\n",
      "----------\n",
      "train Loss: 0.1970 Acc: 0.9309\n",
      "val Loss: 0.1570 Acc: 0.9469\n",
      "\n",
      "Epoch 16/299\n",
      "----------\n",
      "train Loss: 0.1932 Acc: 0.9349\n",
      "val Loss: 0.1986 Acc: 0.9257\n",
      "\n",
      "Epoch 17/299\n",
      "----------\n",
      "train Loss: 0.1937 Acc: 0.9357\n",
      "val Loss: 0.1411 Acc: 0.9539\n",
      "\n",
      "Epoch 18/299\n",
      "----------\n",
      "train Loss: 0.1919 Acc: 0.9351\n",
      "val Loss: 0.1434 Acc: 0.9506\n",
      "\n",
      "Epoch 19/299\n",
      "----------\n",
      "train Loss: 0.1859 Acc: 0.9377\n",
      "val Loss: 0.1480 Acc: 0.9481\n",
      "\n",
      "Epoch 20/299\n",
      "----------\n",
      "train Loss: 0.1879 Acc: 0.9358\n",
      "val Loss: 0.1390 Acc: 0.9543\n",
      "\n",
      "Epoch 21/299\n",
      "----------\n",
      "train Loss: 0.1828 Acc: 0.9369\n",
      "val Loss: 0.1590 Acc: 0.9435\n",
      "\n",
      "Epoch 22/299\n",
      "----------\n",
      "train Loss: 0.1846 Acc: 0.9363\n",
      "val Loss: 0.1389 Acc: 0.9527\n",
      "\n",
      "Epoch 23/299\n",
      "----------\n",
      "train Loss: 0.1807 Acc: 0.9379\n",
      "val Loss: 0.1773 Acc: 0.9357\n",
      "\n",
      "Epoch 24/299\n",
      "----------\n",
      "train Loss: 0.1735 Acc: 0.9406\n",
      "val Loss: 0.1903 Acc: 0.9282\n",
      "\n",
      "Epoch 25/299\n",
      "----------\n",
      "train Loss: 0.1829 Acc: 0.9342\n",
      "val Loss: 0.1341 Acc: 0.9535\n",
      "\n",
      "Epoch 26/299\n",
      "----------\n",
      "train Loss: 0.1792 Acc: 0.9377\n",
      "val Loss: 0.1346 Acc: 0.9527\n",
      "\n",
      "Epoch 27/299\n",
      "----------\n",
      "train Loss: 0.1751 Acc: 0.9404\n",
      "val Loss: 0.1292 Acc: 0.9556\n",
      "\n",
      "Epoch 28/299\n",
      "----------\n",
      "train Loss: 0.1733 Acc: 0.9388\n",
      "val Loss: 0.1323 Acc: 0.9535\n",
      "\n",
      "Epoch 29/299\n",
      "----------\n",
      "train Loss: 0.1780 Acc: 0.9388\n",
      "val Loss: 0.1456 Acc: 0.9485\n",
      "\n",
      "Epoch 30/299\n",
      "----------\n",
      "train Loss: 0.1760 Acc: 0.9400\n",
      "val Loss: 0.1462 Acc: 0.9435\n",
      "\n",
      "Epoch 31/299\n",
      "----------\n",
      "train Loss: 0.1715 Acc: 0.9400\n",
      "val Loss: 0.1644 Acc: 0.9398\n",
      "\n",
      "Epoch 32/299\n",
      "----------\n",
      "train Loss: 0.1801 Acc: 0.9346\n",
      "val Loss: 0.1357 Acc: 0.9510\n",
      "\n",
      "Epoch 33/299\n",
      "----------\n",
      "train Loss: 0.1715 Acc: 0.9414\n",
      "val Loss: 0.1255 Acc: 0.9564\n",
      "\n",
      "Epoch 34/299\n",
      "----------\n",
      "train Loss: 0.1700 Acc: 0.9391\n",
      "val Loss: 0.1269 Acc: 0.9539\n",
      "\n",
      "Epoch 35/299\n",
      "----------\n",
      "train Loss: 0.1681 Acc: 0.9404\n",
      "val Loss: 0.1327 Acc: 0.9506\n",
      "\n",
      "Epoch 36/299\n",
      "----------\n",
      "train Loss: 0.1665 Acc: 0.9409\n",
      "val Loss: 0.1484 Acc: 0.9477\n",
      "\n",
      "Epoch 37/299\n",
      "----------\n",
      "train Loss: 0.1691 Acc: 0.9411\n",
      "val Loss: 0.1249 Acc: 0.9585\n",
      "\n",
      "Epoch 38/299\n",
      "----------\n",
      "train Loss: 0.1699 Acc: 0.9395\n",
      "val Loss: 0.1215 Acc: 0.9552\n",
      "\n",
      "Epoch 39/299\n",
      "----------\n",
      "train Loss: 0.1621 Acc: 0.9444\n",
      "val Loss: 0.1225 Acc: 0.9577\n",
      "\n",
      "Epoch 40/299\n",
      "----------\n",
      "train Loss: 0.1701 Acc: 0.9380\n",
      "val Loss: 0.1207 Acc: 0.9589\n",
      "\n",
      "Epoch 41/299\n",
      "----------\n",
      "train Loss: 0.1669 Acc: 0.9395\n",
      "val Loss: 0.1557 Acc: 0.9402\n",
      "\n",
      "Epoch 42/299\n",
      "----------\n",
      "train Loss: 0.1630 Acc: 0.9408\n",
      "val Loss: 0.2052 Acc: 0.9145\n",
      "\n",
      "Epoch 43/299\n",
      "----------\n",
      "train Loss: 0.1646 Acc: 0.9435\n",
      "val Loss: 0.1187 Acc: 0.9564\n",
      "\n",
      "Epoch 44/299\n",
      "----------\n",
      "train Loss: 0.1660 Acc: 0.9430\n",
      "val Loss: 0.1418 Acc: 0.9506\n",
      "\n",
      "Epoch 45/299\n",
      "----------\n",
      "train Loss: 0.1640 Acc: 0.9430\n",
      "val Loss: 0.1851 Acc: 0.9278\n",
      "\n",
      "Epoch 46/299\n",
      "----------\n",
      "train Loss: 0.1634 Acc: 0.9413\n",
      "val Loss: 0.1200 Acc: 0.9556\n",
      "\n",
      "Epoch 47/299\n",
      "----------\n",
      "train Loss: 0.1583 Acc: 0.9459\n",
      "val Loss: 0.1519 Acc: 0.9435\n",
      "\n",
      "Epoch 48/299\n",
      "----------\n",
      "train Loss: 0.1612 Acc: 0.9420\n",
      "val Loss: 0.1208 Acc: 0.9572\n",
      "\n",
      "Epoch 49/299\n",
      "----------\n",
      "train Loss: 0.1591 Acc: 0.9442\n",
      "val Loss: 0.1188 Acc: 0.9543\n",
      "\n",
      "Epoch 50/299\n",
      "----------\n",
      "train Loss: 0.1623 Acc: 0.9428\n",
      "val Loss: 0.1236 Acc: 0.9552\n",
      "\n",
      "Epoch 51/299\n",
      "----------\n",
      "train Loss: 0.1514 Acc: 0.9474\n",
      "val Loss: 0.1424 Acc: 0.9460\n",
      "\n",
      "Epoch 52/299\n",
      "----------\n",
      "train Loss: 0.1596 Acc: 0.9423\n",
      "val Loss: 0.1184 Acc: 0.9581\n",
      "\n",
      "Epoch 53/299\n",
      "----------\n",
      "train Loss: 0.1612 Acc: 0.9429\n",
      "val Loss: 0.1473 Acc: 0.9465\n",
      "\n",
      "Epoch 54/299\n",
      "----------\n",
      "train Loss: 0.1614 Acc: 0.9415\n",
      "val Loss: 0.1217 Acc: 0.9560\n",
      "\n",
      "Epoch 55/299\n",
      "----------\n",
      "train Loss: 0.1548 Acc: 0.9445\n",
      "val Loss: 0.1227 Acc: 0.9548\n",
      "\n",
      "Epoch 56/299\n",
      "----------\n",
      "train Loss: 0.1604 Acc: 0.9440\n",
      "val Loss: 0.1800 Acc: 0.9274\n",
      "\n",
      "Epoch 57/299\n",
      "----------\n",
      "train Loss: 0.1556 Acc: 0.9484\n",
      "val Loss: 0.2345 Acc: 0.9012\n",
      "\n",
      "Epoch 58/299\n",
      "----------\n",
      "train Loss: 0.1548 Acc: 0.9440\n",
      "val Loss: 0.1607 Acc: 0.9373\n",
      "\n",
      "Epoch 59/299\n",
      "----------\n",
      "train Loss: 0.1543 Acc: 0.9460\n",
      "val Loss: 0.1205 Acc: 0.9535\n",
      "\n",
      "Epoch 60/299\n",
      "----------\n",
      "train Loss: 0.1529 Acc: 0.9419\n",
      "val Loss: 0.1231 Acc: 0.9556\n",
      "\n",
      "Epoch 61/299\n",
      "----------\n",
      "train Loss: 0.1576 Acc: 0.9452\n",
      "val Loss: 0.1169 Acc: 0.9556\n",
      "\n",
      "Epoch 62/299\n",
      "----------\n",
      "train Loss: 0.1516 Acc: 0.9442\n",
      "val Loss: 0.1236 Acc: 0.9527\n",
      "\n",
      "Epoch 63/299\n",
      "----------\n",
      "train Loss: 0.1570 Acc: 0.9426\n",
      "val Loss: 0.1181 Acc: 0.9585\n",
      "\n",
      "Epoch 64/299\n",
      "----------\n",
      "train Loss: 0.1531 Acc: 0.9461\n",
      "val Loss: 0.1844 Acc: 0.9249\n",
      "\n",
      "Epoch 65/299\n",
      "----------\n",
      "train Loss: 0.1546 Acc: 0.9436\n",
      "val Loss: 0.1275 Acc: 0.9527\n",
      "\n",
      "Epoch 66/299\n",
      "----------\n",
      "train Loss: 0.1535 Acc: 0.9451\n",
      "val Loss: 0.1190 Acc: 0.9560\n",
      "\n",
      "Epoch 67/299\n",
      "----------\n",
      "train Loss: 0.1539 Acc: 0.9462\n",
      "val Loss: 0.1249 Acc: 0.9514\n",
      "\n",
      "Epoch 68/299\n",
      "----------\n",
      "train Loss: 0.1483 Acc: 0.9467\n",
      "val Loss: 0.1196 Acc: 0.9556\n",
      "\n",
      "Epoch 69/299\n",
      "----------\n",
      "train Loss: 0.1535 Acc: 0.9442\n",
      "val Loss: 0.1113 Acc: 0.9597\n",
      "\n",
      "Epoch 70/299\n",
      "----------\n",
      "train Loss: 0.1504 Acc: 0.9468\n",
      "val Loss: 0.1201 Acc: 0.9543\n",
      "\n",
      "Epoch 71/299\n",
      "----------\n",
      "train Loss: 0.1587 Acc: 0.9431\n",
      "val Loss: 0.1234 Acc: 0.9548\n",
      "\n",
      "Epoch 72/299\n",
      "----------\n",
      "train Loss: 0.1529 Acc: 0.9445\n",
      "val Loss: 0.1362 Acc: 0.9510\n",
      "\n",
      "Epoch 73/299\n",
      "----------\n",
      "train Loss: 0.1519 Acc: 0.9431\n",
      "val Loss: 0.1169 Acc: 0.9552\n",
      "\n",
      "Epoch 74/299\n",
      "----------\n",
      "train Loss: 0.1510 Acc: 0.9447\n",
      "val Loss: 0.1536 Acc: 0.9406\n",
      "\n",
      "Epoch 75/299\n",
      "----------\n",
      "train Loss: 0.1513 Acc: 0.9458\n",
      "val Loss: 0.1253 Acc: 0.9523\n",
      "\n",
      "Epoch 76/299\n",
      "----------\n",
      "train Loss: 0.1515 Acc: 0.9442\n",
      "val Loss: 0.1113 Acc: 0.9585\n",
      "\n",
      "Epoch 77/299\n",
      "----------\n",
      "train Loss: 0.1532 Acc: 0.9436\n",
      "val Loss: 0.1201 Acc: 0.9548\n",
      "\n",
      "Epoch 78/299\n",
      "----------\n",
      "train Loss: 0.1564 Acc: 0.9448\n",
      "val Loss: 0.1128 Acc: 0.9593\n",
      "\n",
      "Epoch 79/299\n",
      "----------\n",
      "train Loss: 0.1467 Acc: 0.9471\n",
      "val Loss: 0.1264 Acc: 0.9523\n",
      "\n",
      "Epoch 80/299\n",
      "----------\n",
      "train Loss: 0.1459 Acc: 0.9459\n",
      "val Loss: 0.1137 Acc: 0.9581\n",
      "\n",
      "Epoch 81/299\n",
      "----------\n",
      "train Loss: 0.1476 Acc: 0.9472\n",
      "val Loss: 0.1567 Acc: 0.9386\n",
      "\n",
      "Epoch 82/299\n",
      "----------\n",
      "train Loss: 0.1499 Acc: 0.9486\n",
      "val Loss: 0.1889 Acc: 0.9286\n",
      "\n",
      "Epoch 83/299\n",
      "----------\n",
      "train Loss: 0.1529 Acc: 0.9456\n",
      "val Loss: 0.1399 Acc: 0.9473\n",
      "\n",
      "Epoch 84/299\n",
      "----------\n",
      "train Loss: 0.1476 Acc: 0.9468\n",
      "val Loss: 0.1121 Acc: 0.9589\n",
      "\n",
      "Epoch 85/299\n",
      "----------\n",
      "train Loss: 0.1463 Acc: 0.9485\n",
      "val Loss: 0.1117 Acc: 0.9614\n",
      "\n",
      "Epoch 86/299\n",
      "----------\n",
      "train Loss: 0.1474 Acc: 0.9480\n",
      "val Loss: 0.1149 Acc: 0.9543\n",
      "\n",
      "Epoch 87/299\n",
      "----------\n",
      "train Loss: 0.1478 Acc: 0.9447\n",
      "val Loss: 0.1108 Acc: 0.9589\n",
      "\n",
      "Epoch 88/299\n",
      "----------\n",
      "train Loss: 0.1439 Acc: 0.9473\n",
      "val Loss: 0.1082 Acc: 0.9597\n",
      "\n",
      "Epoch 89/299\n",
      "----------\n",
      "train Loss: 0.1541 Acc: 0.9431\n",
      "val Loss: 0.1122 Acc: 0.9568\n",
      "\n",
      "Epoch 90/299\n",
      "----------\n",
      "train Loss: 0.1456 Acc: 0.9452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1563 Acc: 0.9398\n",
      "\n",
      "Epoch 91/299\n",
      "----------\n",
      "train Loss: 0.1474 Acc: 0.9467\n",
      "val Loss: 0.1105 Acc: 0.9572\n",
      "\n",
      "Epoch 92/299\n",
      "----------\n",
      "train Loss: 0.1464 Acc: 0.9460\n",
      "val Loss: 0.1253 Acc: 0.9539\n",
      "\n",
      "Epoch 93/299\n",
      "----------\n",
      "train Loss: 0.1431 Acc: 0.9459\n",
      "val Loss: 0.1188 Acc: 0.9527\n",
      "\n",
      "Epoch 94/299\n",
      "----------\n",
      "train Loss: 0.1514 Acc: 0.9451\n",
      "val Loss: 0.1117 Acc: 0.9572\n",
      "\n",
      "Epoch 95/299\n",
      "----------\n",
      "train Loss: 0.1521 Acc: 0.9414\n",
      "val Loss: 0.1177 Acc: 0.9539\n",
      "\n",
      "Epoch 96/299\n",
      "----------\n",
      "train Loss: 0.1451 Acc: 0.9465\n",
      "val Loss: 0.1131 Acc: 0.9581\n",
      "\n",
      "Epoch 97/299\n",
      "----------\n",
      "train Loss: 0.1459 Acc: 0.9454\n",
      "val Loss: 0.1141 Acc: 0.9593\n",
      "\n",
      "Epoch 98/299\n",
      "----------\n",
      "train Loss: 0.1402 Acc: 0.9487\n",
      "val Loss: 0.1087 Acc: 0.9593\n",
      "\n",
      "Epoch 99/299\n",
      "----------\n",
      "train Loss: 0.1521 Acc: 0.9429\n",
      "val Loss: 0.1083 Acc: 0.9601\n",
      "\n",
      "Epoch 100/299\n",
      "----------\n",
      "train Loss: 0.1460 Acc: 0.9458\n",
      "val Loss: 0.1628 Acc: 0.9361\n",
      "\n",
      "Epoch 101/299\n",
      "----------\n",
      "train Loss: 0.1421 Acc: 0.9487\n",
      "val Loss: 0.1556 Acc: 0.9386\n",
      "\n",
      "Epoch 102/299\n",
      "----------\n",
      "train Loss: 0.1409 Acc: 0.9477\n",
      "val Loss: 0.1301 Acc: 0.9498\n",
      "\n",
      "Epoch 103/299\n",
      "----------\n",
      "train Loss: 0.1483 Acc: 0.9456\n",
      "val Loss: 0.1200 Acc: 0.9552\n",
      "\n",
      "Epoch 104/299\n",
      "----------\n",
      "train Loss: 0.1437 Acc: 0.9493\n",
      "val Loss: 0.1125 Acc: 0.9597\n",
      "\n",
      "Epoch 105/299\n",
      "----------\n",
      "train Loss: 0.1429 Acc: 0.9472\n",
      "val Loss: 0.1114 Acc: 0.9593\n",
      "\n",
      "Epoch 106/299\n",
      "----------\n",
      "train Loss: 0.1412 Acc: 0.9478\n",
      "val Loss: 0.1943 Acc: 0.9215\n",
      "\n",
      "Epoch 107/299\n",
      "----------\n",
      "train Loss: 0.1428 Acc: 0.9495\n",
      "val Loss: 0.1572 Acc: 0.9386\n",
      "\n",
      "Epoch 108/299\n",
      "----------\n",
      "train Loss: 0.1411 Acc: 0.9471\n",
      "val Loss: 0.1264 Acc: 0.9531\n",
      "\n",
      "Epoch 109/299\n",
      "----------\n",
      "train Loss: 0.1397 Acc: 0.9504\n",
      "val Loss: 0.1345 Acc: 0.9469\n",
      "\n",
      "Epoch 110/299\n",
      "----------\n",
      "train Loss: 0.1425 Acc: 0.9486\n",
      "val Loss: 0.1573 Acc: 0.9369\n",
      "\n",
      "Epoch 111/299\n",
      "----------\n",
      "train Loss: 0.1474 Acc: 0.9464\n",
      "val Loss: 0.1090 Acc: 0.9589\n",
      "\n",
      "Epoch 112/299\n",
      "----------\n",
      "train Loss: 0.1432 Acc: 0.9471\n",
      "val Loss: 0.1472 Acc: 0.9435\n",
      "\n",
      "Epoch 113/299\n",
      "----------\n",
      "train Loss: 0.1428 Acc: 0.9483\n",
      "val Loss: 0.1088 Acc: 0.9606\n",
      "\n",
      "Epoch 114/299\n",
      "----------\n",
      "train Loss: 0.1453 Acc: 0.9455\n",
      "val Loss: 0.1477 Acc: 0.9398\n",
      "\n",
      "Epoch 115/299\n",
      "----------\n",
      "train Loss: 0.1458 Acc: 0.9476\n",
      "val Loss: 0.1134 Acc: 0.9548\n",
      "\n",
      "Epoch 116/299\n",
      "----------\n",
      "train Loss: 0.1379 Acc: 0.9512\n",
      "val Loss: 0.1057 Acc: 0.9597\n",
      "\n",
      "Epoch 117/299\n",
      "----------\n",
      "train Loss: 0.1452 Acc: 0.9451\n",
      "val Loss: 0.1109 Acc: 0.9556\n",
      "\n",
      "Epoch 118/299\n",
      "----------\n",
      "train Loss: 0.1390 Acc: 0.9483\n",
      "val Loss: 0.2442 Acc: 0.8995\n",
      "\n",
      "Epoch 119/299\n",
      "----------\n",
      "train Loss: 0.1387 Acc: 0.9482\n",
      "val Loss: 0.1840 Acc: 0.9269\n",
      "\n",
      "Epoch 120/299\n",
      "----------\n",
      "train Loss: 0.1443 Acc: 0.9482\n",
      "val Loss: 0.1070 Acc: 0.9610\n",
      "\n",
      "Epoch 121/299\n",
      "----------\n",
      "train Loss: 0.1456 Acc: 0.9467\n",
      "val Loss: 0.1458 Acc: 0.9440\n",
      "\n",
      "Epoch 122/299\n",
      "----------\n",
      "train Loss: 0.1366 Acc: 0.9508\n",
      "val Loss: 0.1548 Acc: 0.9381\n",
      "\n",
      "Epoch 123/299\n",
      "----------\n",
      "train Loss: 0.1423 Acc: 0.9506\n",
      "val Loss: 0.1656 Acc: 0.9365\n",
      "\n",
      "Epoch 124/299\n",
      "----------\n",
      "train Loss: 0.1394 Acc: 0.9498\n",
      "val Loss: 0.1162 Acc: 0.9556\n",
      "\n",
      "Epoch 125/299\n",
      "----------\n",
      "train Loss: 0.1393 Acc: 0.9491\n",
      "val Loss: 0.1331 Acc: 0.9485\n",
      "\n",
      "Epoch 126/299\n",
      "----------\n",
      "train Loss: 0.1407 Acc: 0.9481\n",
      "val Loss: 0.1105 Acc: 0.9564\n",
      "\n",
      "Epoch 127/299\n",
      "----------\n",
      "train Loss: 0.1386 Acc: 0.9512\n",
      "val Loss: 0.1090 Acc: 0.9593\n",
      "\n",
      "Epoch 128/299\n",
      "----------\n",
      "train Loss: 0.1333 Acc: 0.9519\n",
      "val Loss: 0.1501 Acc: 0.9406\n",
      "\n",
      "Epoch 129/299\n",
      "----------\n",
      "train Loss: 0.1406 Acc: 0.9487\n",
      "val Loss: 0.1096 Acc: 0.9581\n",
      "\n",
      "Epoch 130/299\n",
      "----------\n",
      "train Loss: 0.1372 Acc: 0.9485\n",
      "val Loss: 0.1248 Acc: 0.9510\n",
      "\n",
      "Epoch 131/299\n",
      "----------\n",
      "train Loss: 0.1383 Acc: 0.9498\n",
      "val Loss: 0.1071 Acc: 0.9631\n",
      "\n",
      "Epoch 132/299\n",
      "----------\n",
      "train Loss: 0.1424 Acc: 0.9481\n",
      "val Loss: 0.1078 Acc: 0.9597\n",
      "\n",
      "Epoch 133/299\n",
      "----------\n",
      "train Loss: 0.1360 Acc: 0.9520\n",
      "val Loss: 0.1146 Acc: 0.9552\n",
      "\n",
      "Epoch 134/299\n",
      "----------\n",
      "train Loss: 0.1360 Acc: 0.9504\n",
      "val Loss: 0.1357 Acc: 0.9469\n",
      "\n",
      "Epoch 135/299\n",
      "----------\n",
      "train Loss: 0.1326 Acc: 0.9507\n",
      "val Loss: 0.1048 Acc: 0.9597\n",
      "\n",
      "Epoch 136/299\n",
      "----------\n",
      "train Loss: 0.1393 Acc: 0.9512\n",
      "val Loss: 0.1370 Acc: 0.9456\n",
      "\n",
      "Epoch 137/299\n",
      "----------\n",
      "train Loss: 0.1376 Acc: 0.9528\n",
      "val Loss: 0.1069 Acc: 0.9610\n",
      "\n",
      "Epoch 138/299\n",
      "----------\n",
      "train Loss: 0.1356 Acc: 0.9502\n",
      "val Loss: 0.1549 Acc: 0.9398\n",
      "\n",
      "Epoch 139/299\n",
      "----------\n",
      "train Loss: 0.1394 Acc: 0.9482\n",
      "val Loss: 0.1544 Acc: 0.9415\n",
      "\n",
      "Epoch 140/299\n",
      "----------\n",
      "train Loss: 0.1405 Acc: 0.9490\n",
      "val Loss: 0.1138 Acc: 0.9572\n",
      "\n",
      "Epoch 141/299\n",
      "----------\n",
      "train Loss: 0.1380 Acc: 0.9481\n",
      "val Loss: 0.1137 Acc: 0.9572\n",
      "\n",
      "Epoch 142/299\n",
      "----------\n",
      "train Loss: 0.1437 Acc: 0.9465\n",
      "val Loss: 0.1222 Acc: 0.9535\n",
      "\n",
      "Epoch 143/299\n",
      "----------\n",
      "train Loss: 0.1407 Acc: 0.9484\n",
      "val Loss: 0.1120 Acc: 0.9572\n",
      "\n",
      "Epoch 144/299\n",
      "----------\n",
      "train Loss: 0.1409 Acc: 0.9482\n",
      "val Loss: 0.1212 Acc: 0.9543\n",
      "\n",
      "Epoch 145/299\n",
      "----------\n",
      "train Loss: 0.1362 Acc: 0.9510\n",
      "val Loss: 0.1494 Acc: 0.9406\n",
      "\n",
      "Epoch 146/299\n",
      "----------\n",
      "train Loss: 0.1361 Acc: 0.9496\n",
      "val Loss: 0.1093 Acc: 0.9581\n",
      "\n",
      "Epoch 147/299\n",
      "----------\n",
      "train Loss: 0.1391 Acc: 0.9500\n",
      "val Loss: 0.1227 Acc: 0.9527\n",
      "\n",
      "Epoch 148/299\n",
      "----------\n",
      "train Loss: 0.1343 Acc: 0.9515\n",
      "val Loss: 0.1186 Acc: 0.9527\n",
      "\n",
      "Epoch 149/299\n",
      "----------\n",
      "train Loss: 0.1377 Acc: 0.9475\n",
      "val Loss: 0.1068 Acc: 0.9589\n",
      "\n",
      "Epoch 150/299\n",
      "----------\n",
      "train Loss: 0.1383 Acc: 0.9496\n",
      "val Loss: 0.1087 Acc: 0.9597\n",
      "\n",
      "Epoch 151/299\n",
      "----------\n",
      "train Loss: 0.1365 Acc: 0.9508\n",
      "val Loss: 0.1061 Acc: 0.9610\n",
      "\n",
      "Epoch 152/299\n",
      "----------\n",
      "train Loss: 0.1313 Acc: 0.9534\n",
      "val Loss: 0.1036 Acc: 0.9610\n",
      "\n",
      "Epoch 153/299\n",
      "----------\n",
      "train Loss: 0.1402 Acc: 0.9502\n",
      "val Loss: 0.1193 Acc: 0.9535\n",
      "\n",
      "Epoch 154/299\n",
      "----------\n",
      "train Loss: 0.1380 Acc: 0.9503\n",
      "val Loss: 0.1212 Acc: 0.9543\n",
      "\n",
      "Epoch 155/299\n",
      "----------\n",
      "train Loss: 0.1382 Acc: 0.9489\n",
      "val Loss: 0.1087 Acc: 0.9577\n",
      "\n",
      "Epoch 156/299\n",
      "----------\n",
      "train Loss: 0.1362 Acc: 0.9508\n",
      "val Loss: 0.1298 Acc: 0.9498\n",
      "\n",
      "Epoch 157/299\n",
      "----------\n",
      "train Loss: 0.1346 Acc: 0.9498\n",
      "val Loss: 0.1353 Acc: 0.9452\n",
      "\n",
      "Epoch 158/299\n",
      "----------\n",
      "train Loss: 0.1327 Acc: 0.9510\n",
      "val Loss: 0.1062 Acc: 0.9593\n",
      "\n",
      "Epoch 159/299\n",
      "----------\n",
      "train Loss: 0.1334 Acc: 0.9513\n",
      "val Loss: 0.1064 Acc: 0.9564\n",
      "\n",
      "Epoch 160/299\n",
      "----------\n",
      "train Loss: 0.1339 Acc: 0.9484\n",
      "val Loss: 0.1052 Acc: 0.9597\n",
      "\n",
      "Epoch 161/299\n",
      "----------\n",
      "train Loss: 0.1369 Acc: 0.9485\n",
      "val Loss: 0.1602 Acc: 0.9369\n",
      "\n",
      "Epoch 162/299\n",
      "----------\n",
      "train Loss: 0.1346 Acc: 0.9505\n",
      "val Loss: 0.1408 Acc: 0.9469\n",
      "\n",
      "Epoch 163/299\n",
      "----------\n",
      "train Loss: 0.1328 Acc: 0.9525\n",
      "val Loss: 0.1896 Acc: 0.9286\n",
      "\n",
      "Epoch 164/299\n",
      "----------\n",
      "train Loss: 0.1390 Acc: 0.9481\n",
      "val Loss: 0.1426 Acc: 0.9440\n",
      "\n",
      "Epoch 165/299\n",
      "----------\n",
      "train Loss: 0.1345 Acc: 0.9521\n",
      "val Loss: 0.1331 Acc: 0.9477\n",
      "\n",
      "Epoch 166/299\n",
      "----------\n",
      "train Loss: 0.1324 Acc: 0.9502\n",
      "val Loss: 0.1220 Acc: 0.9535\n",
      "\n",
      "Epoch 167/299\n",
      "----------\n",
      "train Loss: 0.1414 Acc: 0.9459\n",
      "val Loss: 0.1306 Acc: 0.9469\n",
      "\n",
      "Epoch 168/299\n",
      "----------\n",
      "train Loss: 0.1334 Acc: 0.9506\n",
      "val Loss: 0.1749 Acc: 0.9315\n",
      "\n",
      "Epoch 169/299\n",
      "----------\n",
      "train Loss: 0.1367 Acc: 0.9506\n",
      "val Loss: 0.1065 Acc: 0.9589\n",
      "\n",
      "Epoch 170/299\n",
      "----------\n",
      "train Loss: 0.1366 Acc: 0.9472\n",
      "val Loss: 0.1139 Acc: 0.9556\n",
      "\n",
      "Epoch 171/299\n",
      "----------\n",
      "train Loss: 0.1340 Acc: 0.9509\n",
      "val Loss: 0.1093 Acc: 0.9568\n",
      "\n",
      "Epoch 172/299\n",
      "----------\n",
      "train Loss: 0.1340 Acc: 0.9505\n",
      "val Loss: 0.2437 Acc: 0.9029\n",
      "\n",
      "Epoch 173/299\n",
      "----------\n",
      "train Loss: 0.1378 Acc: 0.9495\n",
      "val Loss: 0.1345 Acc: 0.9465\n",
      "\n",
      "Epoch 174/299\n",
      "----------\n",
      "train Loss: 0.1362 Acc: 0.9494\n",
      "val Loss: 0.1837 Acc: 0.9282\n",
      "\n",
      "Epoch 175/299\n",
      "----------\n",
      "train Loss: 0.1328 Acc: 0.9531\n",
      "val Loss: 0.1062 Acc: 0.9597\n",
      "\n",
      "Epoch 176/299\n",
      "----------\n",
      "train Loss: 0.1326 Acc: 0.9537\n",
      "val Loss: 0.1486 Acc: 0.9415\n",
      "\n",
      "Epoch 177/299\n",
      "----------\n",
      "train Loss: 0.1358 Acc: 0.9491\n",
      "val Loss: 0.1038 Acc: 0.9618\n",
      "\n",
      "Epoch 178/299\n",
      "----------\n",
      "train Loss: 0.1341 Acc: 0.9517\n",
      "val Loss: 0.1223 Acc: 0.9518\n",
      "\n",
      "Epoch 179/299\n",
      "----------\n",
      "train Loss: 0.1340 Acc: 0.9498\n",
      "val Loss: 0.1035 Acc: 0.9631\n",
      "\n",
      "Epoch 180/299\n",
      "----------\n",
      "train Loss: 0.1292 Acc: 0.9512\n",
      "val Loss: 0.1220 Acc: 0.9527\n",
      "\n",
      "Epoch 181/299\n",
      "----------\n",
      "train Loss: 0.1349 Acc: 0.9520\n",
      "val Loss: 0.1036 Acc: 0.9601\n",
      "\n",
      "Epoch 182/299\n",
      "----------\n",
      "train Loss: 0.1332 Acc: 0.9502\n",
      "val Loss: 0.1508 Acc: 0.9427\n",
      "\n",
      "Epoch 183/299\n",
      "----------\n",
      "train Loss: 0.1253 Acc: 0.9559\n",
      "val Loss: 0.1415 Acc: 0.9448\n",
      "\n",
      "Epoch 184/299\n",
      "----------\n",
      "train Loss: 0.1341 Acc: 0.9504\n",
      "val Loss: 0.1118 Acc: 0.9560\n",
      "\n",
      "Epoch 185/299\n",
      "----------\n",
      "train Loss: 0.1322 Acc: 0.9516\n",
      "val Loss: 0.1359 Acc: 0.9489\n",
      "\n",
      "Epoch 186/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1336 Acc: 0.9486\n",
      "val Loss: 0.1168 Acc: 0.9543\n",
      "\n",
      "Epoch 187/299\n",
      "----------\n",
      "train Loss: 0.1332 Acc: 0.9516\n",
      "val Loss: 0.1106 Acc: 0.9572\n",
      "\n",
      "Epoch 188/299\n",
      "----------\n",
      "train Loss: 0.1323 Acc: 0.9504\n",
      "val Loss: 0.1149 Acc: 0.9527\n",
      "\n",
      "Epoch 189/299\n",
      "----------\n",
      "train Loss: 0.1363 Acc: 0.9496\n",
      "val Loss: 0.1106 Acc: 0.9581\n",
      "\n",
      "Epoch 190/299\n",
      "----------\n",
      "train Loss: 0.1378 Acc: 0.9493\n",
      "val Loss: 0.1517 Acc: 0.9402\n",
      "\n",
      "Epoch 191/299\n",
      "----------\n",
      "train Loss: 0.1320 Acc: 0.9511\n",
      "val Loss: 0.1085 Acc: 0.9556\n",
      "\n",
      "Epoch 192/299\n",
      "----------\n",
      "train Loss: 0.1339 Acc: 0.9494\n",
      "val Loss: 0.1023 Acc: 0.9610\n",
      "\n",
      "Epoch 193/299\n",
      "----------\n",
      "train Loss: 0.1359 Acc: 0.9494\n",
      "val Loss: 0.1071 Acc: 0.9564\n",
      "\n",
      "Epoch 194/299\n",
      "----------\n",
      "train Loss: 0.1309 Acc: 0.9530\n",
      "val Loss: 0.1047 Acc: 0.9589\n",
      "\n",
      "Epoch 195/299\n",
      "----------\n",
      "train Loss: 0.1359 Acc: 0.9502\n",
      "val Loss: 0.1167 Acc: 0.9527\n",
      "\n",
      "Epoch 196/299\n",
      "----------\n",
      "train Loss: 0.1301 Acc: 0.9523\n",
      "val Loss: 0.1895 Acc: 0.9220\n",
      "\n",
      "Epoch 197/299\n",
      "----------\n",
      "train Loss: 0.1342 Acc: 0.9509\n",
      "val Loss: 0.1201 Acc: 0.9489\n",
      "\n",
      "Epoch 198/299\n",
      "----------\n",
      "train Loss: 0.1328 Acc: 0.9515\n",
      "val Loss: 0.1286 Acc: 0.9489\n",
      "\n",
      "Epoch 199/299\n",
      "----------\n",
      "train Loss: 0.1327 Acc: 0.9507\n",
      "val Loss: 0.1048 Acc: 0.9572\n",
      "\n",
      "Epoch 200/299\n",
      "----------\n",
      "train Loss: 0.1329 Acc: 0.9504\n",
      "val Loss: 0.1038 Acc: 0.9589\n",
      "\n",
      "Epoch 201/299\n",
      "----------\n",
      "train Loss: 0.1317 Acc: 0.9521\n",
      "val Loss: 0.1141 Acc: 0.9543\n",
      "\n",
      "Epoch 202/299\n",
      "----------\n",
      "train Loss: 0.1285 Acc: 0.9529\n",
      "val Loss: 0.1568 Acc: 0.9386\n",
      "\n",
      "Epoch 203/299\n",
      "----------\n",
      "train Loss: 0.1258 Acc: 0.9523\n",
      "val Loss: 0.1060 Acc: 0.9572\n",
      "\n",
      "Epoch 204/299\n",
      "----------\n",
      "train Loss: 0.1342 Acc: 0.9515\n",
      "val Loss: 0.1489 Acc: 0.9435\n",
      "\n",
      "Epoch 205/299\n",
      "----------\n",
      "train Loss: 0.1302 Acc: 0.9522\n",
      "val Loss: 0.1586 Acc: 0.9390\n",
      "\n",
      "Epoch 206/299\n",
      "----------\n",
      "train Loss: 0.1304 Acc: 0.9528\n",
      "val Loss: 0.1017 Acc: 0.9614\n",
      "\n",
      "Epoch 207/299\n",
      "----------\n",
      "train Loss: 0.1322 Acc: 0.9487\n",
      "val Loss: 0.1139 Acc: 0.9548\n",
      "\n",
      "Epoch 208/299\n",
      "----------\n",
      "train Loss: 0.1311 Acc: 0.9530\n",
      "val Loss: 0.1066 Acc: 0.9597\n",
      "\n",
      "Epoch 209/299\n",
      "----------\n",
      "train Loss: 0.1360 Acc: 0.9491\n",
      "val Loss: 0.1840 Acc: 0.9257\n",
      "\n",
      "Epoch 210/299\n",
      "----------\n",
      "train Loss: 0.1292 Acc: 0.9534\n",
      "val Loss: 0.1538 Acc: 0.9411\n",
      "\n",
      "Epoch 211/299\n",
      "----------\n",
      "train Loss: 0.1303 Acc: 0.9504\n",
      "val Loss: 0.1151 Acc: 0.9543\n",
      "\n",
      "Epoch 212/299\n",
      "----------\n",
      "train Loss: 0.1345 Acc: 0.9514\n",
      "val Loss: 0.1038 Acc: 0.9610\n",
      "\n",
      "Epoch 213/299\n",
      "----------\n",
      "train Loss: 0.1332 Acc: 0.9526\n",
      "val Loss: 0.1158 Acc: 0.9556\n",
      "\n",
      "Epoch 214/299\n",
      "----------\n",
      "train Loss: 0.1314 Acc: 0.9526\n",
      "val Loss: 0.1036 Acc: 0.9585\n",
      "\n",
      "Epoch 215/299\n",
      "----------\n",
      "train Loss: 0.1339 Acc: 0.9511\n",
      "val Loss: 0.1150 Acc: 0.9577\n",
      "\n",
      "Epoch 216/299\n",
      "----------\n",
      "train Loss: 0.1339 Acc: 0.9486\n",
      "val Loss: 0.1313 Acc: 0.9477\n",
      "\n",
      "Epoch 217/299\n",
      "----------\n",
      "train Loss: 0.1368 Acc: 0.9490\n",
      "val Loss: 0.1124 Acc: 0.9564\n",
      "\n",
      "Epoch 218/299\n",
      "----------\n",
      "train Loss: 0.1270 Acc: 0.9558\n",
      "val Loss: 0.1042 Acc: 0.9568\n",
      "\n",
      "Epoch 219/299\n",
      "----------\n",
      "train Loss: 0.1281 Acc: 0.9506\n",
      "val Loss: 0.1082 Acc: 0.9585\n",
      "\n",
      "Epoch 220/299\n",
      "----------\n",
      "train Loss: 0.1277 Acc: 0.9522\n",
      "val Loss: 0.1185 Acc: 0.9564\n",
      "\n",
      "Epoch 221/299\n",
      "----------\n",
      "train Loss: 0.1325 Acc: 0.9522\n",
      "val Loss: 0.1276 Acc: 0.9502\n",
      "\n",
      "Epoch 222/299\n",
      "----------\n",
      "train Loss: 0.1345 Acc: 0.9520\n",
      "val Loss: 0.1567 Acc: 0.9369\n",
      "\n",
      "Epoch 223/299\n",
      "----------\n",
      "train Loss: 0.1279 Acc: 0.9510\n",
      "val Loss: 0.1131 Acc: 0.9556\n",
      "\n",
      "Epoch 224/299\n",
      "----------\n",
      "train Loss: 0.1281 Acc: 0.9547\n",
      "val Loss: 0.1066 Acc: 0.9572\n",
      "\n",
      "Epoch 225/299\n",
      "----------\n",
      "train Loss: 0.1292 Acc: 0.9511\n",
      "val Loss: 0.1751 Acc: 0.9303\n",
      "\n",
      "Epoch 226/299\n",
      "----------\n",
      "train Loss: 0.1328 Acc: 0.9505\n",
      "val Loss: 0.1206 Acc: 0.9518\n",
      "\n",
      "Epoch 227/299\n",
      "----------\n",
      "train Loss: 0.1310 Acc: 0.9507\n",
      "val Loss: 0.1149 Acc: 0.9548\n",
      "\n",
      "Epoch 228/299\n",
      "----------\n",
      "train Loss: 0.1270 Acc: 0.9521\n",
      "val Loss: 0.1074 Acc: 0.9585\n",
      "\n",
      "Epoch 229/299\n",
      "----------\n",
      "train Loss: 0.1272 Acc: 0.9550\n",
      "val Loss: 0.1353 Acc: 0.9465\n",
      "\n",
      "Epoch 230/299\n",
      "----------\n",
      "train Loss: 0.1273 Acc: 0.9535\n",
      "val Loss: 0.1040 Acc: 0.9593\n",
      "\n",
      "Epoch 231/299\n",
      "----------\n",
      "train Loss: 0.1286 Acc: 0.9529\n",
      "val Loss: 0.1143 Acc: 0.9548\n",
      "\n",
      "Epoch 232/299\n",
      "----------\n",
      "train Loss: 0.1309 Acc: 0.9527\n",
      "val Loss: 0.1195 Acc: 0.9548\n",
      "\n",
      "Epoch 233/299\n",
      "----------\n",
      "train Loss: 0.1355 Acc: 0.9490\n",
      "val Loss: 0.1153 Acc: 0.9564\n",
      "\n",
      "Epoch 234/299\n",
      "----------\n",
      "train Loss: 0.1263 Acc: 0.9531\n",
      "val Loss: 0.1408 Acc: 0.9423\n",
      "\n",
      "Epoch 235/299\n",
      "----------\n",
      "train Loss: 0.1311 Acc: 0.9503\n",
      "val Loss: 0.1130 Acc: 0.9539\n",
      "\n",
      "Epoch 236/299\n",
      "----------\n",
      "train Loss: 0.1258 Acc: 0.9535\n",
      "val Loss: 0.1152 Acc: 0.9564\n",
      "\n",
      "Epoch 237/299\n",
      "----------\n",
      "train Loss: 0.1320 Acc: 0.9503\n",
      "val Loss: 0.1740 Acc: 0.9290\n",
      "\n",
      "Epoch 238/299\n",
      "----------\n",
      "train Loss: 0.1254 Acc: 0.9533\n",
      "val Loss: 0.1056 Acc: 0.9577\n",
      "\n",
      "Epoch 239/299\n",
      "----------\n",
      "train Loss: 0.1269 Acc: 0.9541\n",
      "val Loss: 0.1030 Acc: 0.9589\n",
      "\n",
      "Epoch 240/299\n",
      "----------\n",
      "train Loss: 0.1269 Acc: 0.9551\n",
      "val Loss: 0.0985 Acc: 0.9601\n",
      "\n",
      "Epoch 241/299\n",
      "----------\n",
      "train Loss: 0.1304 Acc: 0.9509\n",
      "val Loss: 0.1065 Acc: 0.9572\n",
      "\n",
      "Epoch 242/299\n",
      "----------\n",
      "train Loss: 0.1330 Acc: 0.9494\n",
      "val Loss: 0.1183 Acc: 0.9564\n",
      "\n",
      "Epoch 243/299\n",
      "----------\n",
      "train Loss: 0.1319 Acc: 0.9513\n",
      "val Loss: 0.1143 Acc: 0.9564\n",
      "\n",
      "Epoch 244/299\n",
      "----------\n",
      "train Loss: 0.1275 Acc: 0.9511\n",
      "val Loss: 0.1734 Acc: 0.9294\n",
      "\n",
      "Epoch 245/299\n",
      "----------\n",
      "train Loss: 0.1289 Acc: 0.9517\n",
      "val Loss: 0.1034 Acc: 0.9601\n",
      "\n",
      "Epoch 246/299\n",
      "----------\n",
      "train Loss: 0.1275 Acc: 0.9527\n",
      "val Loss: 0.1335 Acc: 0.9477\n",
      "\n",
      "Epoch 247/299\n",
      "----------\n",
      "train Loss: 0.1280 Acc: 0.9525\n",
      "val Loss: 0.1142 Acc: 0.9531\n",
      "\n",
      "Epoch 248/299\n",
      "----------\n",
      "train Loss: 0.1305 Acc: 0.9500\n",
      "val Loss: 0.1377 Acc: 0.9465\n",
      "\n",
      "Epoch 249/299\n",
      "----------\n",
      "train Loss: 0.1358 Acc: 0.9498\n",
      "val Loss: 0.1062 Acc: 0.9577\n",
      "\n",
      "Epoch 250/299\n",
      "----------\n",
      "train Loss: 0.1219 Acc: 0.9549\n",
      "val Loss: 0.1271 Acc: 0.9514\n",
      "\n",
      "Epoch 251/299\n",
      "----------\n",
      "train Loss: 0.1278 Acc: 0.9522\n",
      "val Loss: 0.1131 Acc: 0.9531\n",
      "\n",
      "Epoch 252/299\n",
      "----------\n",
      "train Loss: 0.1303 Acc: 0.9515\n",
      "val Loss: 0.1337 Acc: 0.9473\n",
      "\n",
      "Epoch 253/299\n",
      "----------\n",
      "train Loss: 0.1287 Acc: 0.9544\n",
      "val Loss: 0.1132 Acc: 0.9556\n",
      "\n",
      "Epoch 254/299\n",
      "----------\n",
      "train Loss: 0.1277 Acc: 0.9529\n",
      "val Loss: 0.1473 Acc: 0.9419\n",
      "\n",
      "Epoch 255/299\n",
      "----------\n",
      "train Loss: 0.1230 Acc: 0.9555\n",
      "val Loss: 0.1347 Acc: 0.9465\n",
      "\n",
      "Epoch 256/299\n",
      "----------\n",
      "train Loss: 0.1317 Acc: 0.9528\n",
      "val Loss: 0.1037 Acc: 0.9597\n",
      "\n",
      "Epoch 257/299\n",
      "----------\n",
      "train Loss: 0.1290 Acc: 0.9518\n",
      "val Loss: 0.1375 Acc: 0.9456\n",
      "\n",
      "Epoch 258/299\n",
      "----------\n",
      "train Loss: 0.1326 Acc: 0.9528\n",
      "val Loss: 0.1316 Acc: 0.9498\n",
      "\n",
      "Epoch 259/299\n",
      "----------\n",
      "train Loss: 0.1375 Acc: 0.9496\n",
      "val Loss: 0.1160 Acc: 0.9560\n",
      "\n",
      "Epoch 260/299\n",
      "----------\n",
      "train Loss: 0.1308 Acc: 0.9505\n",
      "val Loss: 0.1401 Acc: 0.9465\n",
      "\n",
      "Epoch 261/299\n",
      "----------\n",
      "train Loss: 0.1261 Acc: 0.9528\n",
      "val Loss: 0.1027 Acc: 0.9597\n",
      "\n",
      "Epoch 262/299\n",
      "----------\n",
      "train Loss: 0.1277 Acc: 0.9545\n",
      "val Loss: 0.1193 Acc: 0.9556\n",
      "\n",
      "Epoch 263/299\n",
      "----------\n",
      "train Loss: 0.1290 Acc: 0.9514\n",
      "val Loss: 0.1090 Acc: 0.9581\n",
      "\n",
      "Epoch 264/299\n",
      "----------\n",
      "train Loss: 0.1264 Acc: 0.9537\n",
      "val Loss: 0.1021 Acc: 0.9601\n",
      "\n",
      "Epoch 265/299\n",
      "----------\n",
      "train Loss: 0.1283 Acc: 0.9517\n",
      "val Loss: 0.1040 Acc: 0.9577\n",
      "\n",
      "Epoch 266/299\n",
      "----------\n",
      "train Loss: 0.1294 Acc: 0.9527\n",
      "val Loss: 0.1034 Acc: 0.9601\n",
      "\n",
      "Epoch 267/299\n",
      "----------\n",
      "train Loss: 0.1302 Acc: 0.9526\n",
      "val Loss: 0.1852 Acc: 0.9244\n",
      "\n",
      "Epoch 268/299\n",
      "----------\n",
      "train Loss: 0.1246 Acc: 0.9543\n",
      "val Loss: 0.1053 Acc: 0.9606\n",
      "\n",
      "Epoch 269/299\n",
      "----------\n",
      "train Loss: 0.1294 Acc: 0.9514\n",
      "val Loss: 0.1322 Acc: 0.9473\n",
      "\n",
      "Epoch 270/299\n",
      "----------\n",
      "train Loss: 0.1259 Acc: 0.9521\n",
      "val Loss: 0.1061 Acc: 0.9572\n",
      "\n",
      "Epoch 271/299\n",
      "----------\n",
      "train Loss: 0.1285 Acc: 0.9521\n",
      "val Loss: 0.1096 Acc: 0.9568\n",
      "\n",
      "Epoch 272/299\n",
      "----------\n",
      "train Loss: 0.1216 Acc: 0.9534\n",
      "val Loss: 0.1118 Acc: 0.9552\n",
      "\n",
      "Epoch 273/299\n",
      "----------\n",
      "train Loss: 0.1277 Acc: 0.9516\n",
      "val Loss: 0.1312 Acc: 0.9481\n",
      "\n",
      "Epoch 274/299\n",
      "----------\n",
      "train Loss: 0.1318 Acc: 0.9516\n",
      "val Loss: 0.1747 Acc: 0.9315\n",
      "\n",
      "Epoch 275/299\n",
      "----------\n",
      "train Loss: 0.1299 Acc: 0.9506\n",
      "val Loss: 0.1059 Acc: 0.9564\n",
      "\n",
      "Epoch 276/299\n",
      "----------\n",
      "train Loss: 0.1263 Acc: 0.9532\n",
      "val Loss: 0.1396 Acc: 0.9465\n",
      "\n",
      "Epoch 277/299\n",
      "----------\n",
      "train Loss: 0.1285 Acc: 0.9512\n",
      "val Loss: 0.1085 Acc: 0.9564\n",
      "\n",
      "Epoch 278/299\n",
      "----------\n",
      "train Loss: 0.1241 Acc: 0.9536\n",
      "val Loss: 0.1008 Acc: 0.9610\n",
      "\n",
      "Epoch 279/299\n",
      "----------\n",
      "train Loss: 0.1207 Acc: 0.9550\n",
      "val Loss: 0.0993 Acc: 0.9622\n",
      "\n",
      "Epoch 280/299\n",
      "----------\n",
      "train Loss: 0.1267 Acc: 0.9535\n",
      "val Loss: 0.1382 Acc: 0.9485\n",
      "\n",
      "Epoch 281/299\n",
      "----------\n",
      "train Loss: 0.1273 Acc: 0.9533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1029 Acc: 0.9618\n",
      "\n",
      "Epoch 282/299\n",
      "----------\n",
      "train Loss: 0.1262 Acc: 0.9505\n",
      "val Loss: 0.1043 Acc: 0.9585\n",
      "\n",
      "Epoch 283/299\n",
      "----------\n",
      "train Loss: 0.1180 Acc: 0.9582\n",
      "val Loss: 0.1074 Acc: 0.9552\n",
      "\n",
      "Epoch 284/299\n",
      "----------\n",
      "train Loss: 0.1192 Acc: 0.9550\n",
      "val Loss: 0.1060 Acc: 0.9606\n",
      "\n",
      "Epoch 285/299\n",
      "----------\n",
      "train Loss: 0.1280 Acc: 0.9523\n",
      "val Loss: 0.1161 Acc: 0.9518\n",
      "\n",
      "Epoch 286/299\n",
      "----------\n",
      "train Loss: 0.1269 Acc: 0.9529\n",
      "val Loss: 0.1394 Acc: 0.9448\n",
      "\n",
      "Epoch 287/299\n",
      "----------\n",
      "train Loss: 0.1255 Acc: 0.9527\n",
      "val Loss: 0.1825 Acc: 0.9261\n",
      "\n",
      "Epoch 288/299\n",
      "----------\n",
      "train Loss: 0.1290 Acc: 0.9540\n",
      "val Loss: 0.1088 Acc: 0.9552\n",
      "\n",
      "Epoch 289/299\n",
      "----------\n",
      "train Loss: 0.1261 Acc: 0.9534\n",
      "val Loss: 0.1683 Acc: 0.9332\n",
      "\n",
      "Epoch 290/299\n",
      "----------\n",
      "train Loss: 0.1321 Acc: 0.9521\n",
      "val Loss: 0.1342 Acc: 0.9465\n",
      "\n",
      "Epoch 291/299\n",
      "----------\n",
      "train Loss: 0.1240 Acc: 0.9520\n",
      "val Loss: 0.1095 Acc: 0.9556\n",
      "\n",
      "Epoch 292/299\n",
      "----------\n",
      "train Loss: 0.1305 Acc: 0.9521\n",
      "val Loss: 0.1042 Acc: 0.9601\n",
      "\n",
      "Epoch 293/299\n",
      "----------\n",
      "train Loss: 0.1282 Acc: 0.9532\n",
      "val Loss: 0.1386 Acc: 0.9440\n",
      "\n",
      "Epoch 294/299\n",
      "----------\n",
      "train Loss: 0.1286 Acc: 0.9547\n",
      "val Loss: 0.1355 Acc: 0.9435\n",
      "\n",
      "Epoch 295/299\n",
      "----------\n",
      "train Loss: 0.1247 Acc: 0.9550\n",
      "val Loss: 0.1025 Acc: 0.9589\n",
      "\n",
      "Epoch 296/299\n",
      "----------\n",
      "train Loss: 0.1267 Acc: 0.9517\n",
      "val Loss: 0.1050 Acc: 0.9556\n",
      "\n",
      "Epoch 297/299\n",
      "----------\n",
      "train Loss: 0.1247 Acc: 0.9538\n",
      "val Loss: 0.1355 Acc: 0.9456\n",
      "\n",
      "Epoch 298/299\n",
      "----------\n",
      "train Loss: 0.1290 Acc: 0.9501\n",
      "val Loss: 0.1199 Acc: 0.9523\n",
      "\n",
      "Epoch 299/299\n",
      "----------\n",
      "train Loss: 0.1261 Acc: 0.9516\n",
      "val Loss: 0.1449 Acc: 0.9435\n",
      "\n",
      "\n",
      "##############################\n",
      "------ Summary ------\n",
      "model -> resnet\n",
      "epochs -> 300\n",
      "lr -> 0.0001\n",
      "batch size -> 8\n",
      "\n",
      "Training complete in 67m 42s\n",
      "Best val Acc: 0.963055\n",
      "##############################\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== END ====\n",
      "[[790   0   1]\n",
      " [  1 775  23]\n",
      " [  4  60 755]]\n",
      "\n",
      "Sensitivity or recall total\n",
      "0.9630552096305521\n",
      "\n",
      "Sensitivity or recall per classes\n",
      "[1.   0.97 0.92]\n",
      "\n",
      "Precision\n",
      "[0.99 0.93 0.97]\n",
      "\n",
      "F1 Score\n",
      "[1.   0.95 0.94]\n",
      "Confusion matrix, without normalization\n",
      "\n",
      "============================================================\n",
      "==== INITIALIZING WITH PARAMETERS: ====\n",
      "model -> alexnet\n",
      "epochs -> 300\n",
      "lr -> 0.0001\n",
      "batch size -> 8\n",
      "\n",
      "--------------------\n",
      "Params to learn:\n",
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n",
      "\n",
      "--------------------\n",
      "\n",
      "== Epochs ==\n",
      "Epoch 0/299\n",
      "----------\n",
      "train Loss: 0.4494 Acc: 0.8309\n",
      "val Loss: 0.2855 Acc: 0.9045\n",
      "\n",
      "Epoch 1/299\n",
      "----------\n",
      "train Loss: 0.3393 Acc: 0.8718\n",
      "val Loss: 0.2370 Acc: 0.9215\n",
      "\n",
      "Epoch 2/299\n",
      "----------\n",
      "train Loss: 0.3070 Acc: 0.8850\n",
      "val Loss: 0.2455 Acc: 0.9137\n",
      "\n",
      "Epoch 3/299\n",
      "----------\n",
      "train Loss: 0.2995 Acc: 0.8850\n",
      "val Loss: 0.2242 Acc: 0.9203\n",
      "\n",
      "Epoch 4/299\n",
      "----------\n",
      "train Loss: 0.2916 Acc: 0.8906\n",
      "val Loss: 0.2128 Acc: 0.9282\n",
      "\n",
      "Epoch 5/299\n",
      "----------\n",
      "train Loss: 0.2803 Acc: 0.8921\n",
      "val Loss: 0.2155 Acc: 0.9207\n",
      "\n",
      "Epoch 6/299\n",
      "----------\n",
      "train Loss: 0.2780 Acc: 0.8987\n",
      "val Loss: 0.2103 Acc: 0.9203\n",
      "\n",
      "Epoch 7/299\n",
      "----------\n",
      "train Loss: 0.2689 Acc: 0.9009\n",
      "val Loss: 0.1940 Acc: 0.9328\n",
      "\n",
      "Epoch 8/299\n",
      "----------\n",
      "train Loss: 0.2724 Acc: 0.8987\n",
      "val Loss: 0.2139 Acc: 0.9207\n",
      "\n",
      "Epoch 9/299\n",
      "----------\n",
      "train Loss: 0.2749 Acc: 0.8944\n",
      "val Loss: 0.1859 Acc: 0.9377\n",
      "\n",
      "Epoch 10/299\n",
      "----------\n",
      "train Loss: 0.2664 Acc: 0.9003\n",
      "val Loss: 0.1966 Acc: 0.9282\n",
      "\n",
      "Epoch 11/299\n",
      "----------\n",
      "train Loss: 0.2600 Acc: 0.8996\n",
      "val Loss: 0.1830 Acc: 0.9340\n",
      "\n",
      "Epoch 12/299\n",
      "----------\n",
      "train Loss: 0.2621 Acc: 0.9015\n",
      "val Loss: 0.1888 Acc: 0.9286\n",
      "\n",
      "Epoch 13/299\n",
      "----------\n",
      "train Loss: 0.2562 Acc: 0.9016\n",
      "val Loss: 0.2003 Acc: 0.9298\n",
      "\n",
      "Epoch 14/299\n",
      "----------\n",
      "train Loss: 0.2582 Acc: 0.9029\n",
      "val Loss: 0.1899 Acc: 0.9340\n",
      "\n",
      "Epoch 15/299\n",
      "----------\n",
      "train Loss: 0.2529 Acc: 0.9056\n",
      "val Loss: 0.2151 Acc: 0.9224\n",
      "\n",
      "Epoch 16/299\n",
      "----------\n",
      "train Loss: 0.2586 Acc: 0.8990\n",
      "val Loss: 0.1888 Acc: 0.9336\n",
      "\n",
      "Epoch 17/299\n",
      "----------\n",
      "train Loss: 0.2568 Acc: 0.9025\n",
      "val Loss: 0.2013 Acc: 0.9294\n",
      "\n",
      "Epoch 18/299\n",
      "----------\n",
      "train Loss: 0.2516 Acc: 0.9082\n",
      "val Loss: 0.1820 Acc: 0.9365\n",
      "\n",
      "Epoch 19/299\n",
      "----------\n",
      "train Loss: 0.2537 Acc: 0.9027\n",
      "val Loss: 0.1803 Acc: 0.9352\n",
      "\n",
      "Epoch 20/299\n",
      "----------\n",
      "train Loss: 0.2492 Acc: 0.9065\n",
      "val Loss: 0.1790 Acc: 0.9344\n",
      "\n",
      "Epoch 21/299\n",
      "----------\n",
      "train Loss: 0.2513 Acc: 0.9067\n",
      "val Loss: 0.1869 Acc: 0.9328\n",
      "\n",
      "Epoch 22/299\n",
      "----------\n",
      "train Loss: 0.2431 Acc: 0.9083\n",
      "val Loss: 0.1742 Acc: 0.9369\n",
      "\n",
      "Epoch 23/299\n",
      "----------\n",
      "train Loss: 0.2570 Acc: 0.8987\n",
      "val Loss: 0.1705 Acc: 0.9411\n",
      "\n",
      "Epoch 24/299\n",
      "----------\n",
      "train Loss: 0.2432 Acc: 0.9081\n",
      "val Loss: 0.1714 Acc: 0.9402\n",
      "\n",
      "Epoch 25/299\n",
      "----------\n",
      "train Loss: 0.2434 Acc: 0.9077\n",
      "val Loss: 0.1758 Acc: 0.9361\n",
      "\n",
      "Epoch 26/299\n",
      "----------\n",
      "train Loss: 0.2489 Acc: 0.9056\n",
      "val Loss: 0.1688 Acc: 0.9390\n",
      "\n",
      "Epoch 27/299\n",
      "----------\n",
      "train Loss: 0.2417 Acc: 0.9065\n",
      "val Loss: 0.1793 Acc: 0.9336\n",
      "\n",
      "Epoch 28/299\n",
      "----------\n",
      "train Loss: 0.2481 Acc: 0.9057\n",
      "val Loss: 0.1806 Acc: 0.9344\n",
      "\n",
      "Epoch 29/299\n",
      "----------\n",
      "train Loss: 0.2470 Acc: 0.9058\n",
      "val Loss: 0.1698 Acc: 0.9394\n",
      "\n",
      "Epoch 30/299\n",
      "----------\n",
      "train Loss: 0.2450 Acc: 0.9052\n",
      "val Loss: 0.1609 Acc: 0.9444\n",
      "\n",
      "Epoch 31/299\n",
      "----------\n",
      "train Loss: 0.2461 Acc: 0.9040\n",
      "val Loss: 0.1744 Acc: 0.9365\n",
      "\n",
      "Epoch 32/299\n",
      "----------\n",
      "train Loss: 0.2439 Acc: 0.9068\n",
      "val Loss: 0.1716 Acc: 0.9394\n",
      "\n",
      "Epoch 33/299\n",
      "----------\n",
      "train Loss: 0.2475 Acc: 0.9060\n",
      "val Loss: 0.1770 Acc: 0.9357\n",
      "\n",
      "Epoch 34/299\n",
      "----------\n",
      "train Loss: 0.2414 Acc: 0.9077\n",
      "val Loss: 0.1716 Acc: 0.9390\n",
      "\n",
      "Epoch 35/299\n",
      "----------\n",
      "train Loss: 0.2355 Acc: 0.9089\n",
      "val Loss: 0.1650 Acc: 0.9415\n",
      "\n",
      "Epoch 36/299\n",
      "----------\n",
      "train Loss: 0.2416 Acc: 0.9114\n",
      "val Loss: 0.2015 Acc: 0.9244\n",
      "\n",
      "Epoch 37/299\n",
      "----------\n",
      "train Loss: 0.2381 Acc: 0.9097\n",
      "val Loss: 0.1767 Acc: 0.9332\n",
      "\n",
      "Epoch 38/299\n",
      "----------\n",
      "train Loss: 0.2401 Acc: 0.9081\n",
      "val Loss: 0.1613 Acc: 0.9431\n",
      "\n",
      "Epoch 39/299\n",
      "----------\n",
      "train Loss: 0.2440 Acc: 0.9037\n",
      "val Loss: 0.1659 Acc: 0.9427\n",
      "\n",
      "Epoch 40/299\n",
      "----------\n",
      "train Loss: 0.2375 Acc: 0.9129\n",
      "val Loss: 0.1589 Acc: 0.9423\n",
      "\n",
      "Epoch 41/299\n",
      "----------\n",
      "train Loss: 0.2377 Acc: 0.9101\n",
      "val Loss: 0.1913 Acc: 0.9269\n",
      "\n",
      "Epoch 42/299\n",
      "----------\n",
      "train Loss: 0.2391 Acc: 0.9066\n",
      "val Loss: 0.1961 Acc: 0.9274\n",
      "\n",
      "Epoch 43/299\n",
      "----------\n",
      "train Loss: 0.2370 Acc: 0.9137\n",
      "val Loss: 0.1659 Acc: 0.9406\n",
      "\n",
      "Epoch 44/299\n",
      "----------\n",
      "train Loss: 0.2409 Acc: 0.9103\n",
      "val Loss: 0.1895 Acc: 0.9286\n",
      "\n",
      "Epoch 45/299\n",
      "----------\n",
      "train Loss: 0.2393 Acc: 0.9101\n",
      "val Loss: 0.1544 Acc: 0.9460\n",
      "\n",
      "Epoch 46/299\n",
      "----------\n",
      "train Loss: 0.2325 Acc: 0.9133\n",
      "val Loss: 0.1679 Acc: 0.9411\n",
      "\n",
      "Epoch 47/299\n",
      "----------\n",
      "train Loss: 0.2365 Acc: 0.9103\n",
      "val Loss: 0.1457 Acc: 0.9514\n",
      "\n",
      "Epoch 48/299\n",
      "----------\n",
      "train Loss: 0.2392 Acc: 0.9106\n",
      "val Loss: 0.1744 Acc: 0.9369\n",
      "\n",
      "Epoch 49/299\n",
      "----------\n",
      "train Loss: 0.2419 Acc: 0.9085\n",
      "val Loss: 0.1693 Acc: 0.9381\n",
      "\n",
      "Epoch 50/299\n",
      "----------\n",
      "train Loss: 0.2412 Acc: 0.9107\n",
      "val Loss: 0.1674 Acc: 0.9398\n",
      "\n",
      "Epoch 51/299\n",
      "----------\n",
      "train Loss: 0.2338 Acc: 0.9124\n",
      "val Loss: 0.1833 Acc: 0.9332\n",
      "\n",
      "Epoch 52/299\n",
      "----------\n",
      "train Loss: 0.2324 Acc: 0.9116\n",
      "val Loss: 0.1633 Acc: 0.9411\n",
      "\n",
      "Epoch 53/299\n",
      "----------\n",
      "train Loss: 0.2361 Acc: 0.9128\n",
      "val Loss: 0.1849 Acc: 0.9315\n",
      "\n",
      "Epoch 54/299\n",
      "----------\n",
      "train Loss: 0.2320 Acc: 0.9116\n",
      "val Loss: 0.1684 Acc: 0.9398\n",
      "\n",
      "Epoch 55/299\n",
      "----------\n",
      "train Loss: 0.2306 Acc: 0.9148\n",
      "val Loss: 0.1833 Acc: 0.9303\n",
      "\n",
      "Epoch 56/299\n",
      "----------\n",
      "train Loss: 0.2323 Acc: 0.9110\n",
      "val Loss: 0.1861 Acc: 0.9336\n",
      "\n",
      "Epoch 57/299\n",
      "----------\n",
      "train Loss: 0.2298 Acc: 0.9134\n",
      "val Loss: 0.1602 Acc: 0.9456\n",
      "\n",
      "Epoch 58/299\n",
      "----------\n",
      "train Loss: 0.2362 Acc: 0.9134\n",
      "val Loss: 0.1696 Acc: 0.9381\n",
      "\n",
      "Epoch 59/299\n",
      "----------\n",
      "train Loss: 0.2327 Acc: 0.9143\n",
      "val Loss: 0.1868 Acc: 0.9315\n",
      "\n",
      "Epoch 60/299\n",
      "----------\n",
      "train Loss: 0.2333 Acc: 0.9106\n",
      "val Loss: 0.1511 Acc: 0.9465\n",
      "\n",
      "Epoch 61/299\n",
      "----------\n",
      "train Loss: 0.2405 Acc: 0.9082\n",
      "val Loss: 0.1702 Acc: 0.9381\n",
      "\n",
      "Epoch 62/299\n",
      "----------\n",
      "train Loss: 0.2295 Acc: 0.9139\n",
      "val Loss: 0.1549 Acc: 0.9448\n",
      "\n",
      "Epoch 63/299\n",
      "----------\n",
      "train Loss: 0.2352 Acc: 0.9133\n",
      "val Loss: 0.1532 Acc: 0.9431\n",
      "\n",
      "Epoch 64/299\n",
      "----------\n",
      "train Loss: 0.2396 Acc: 0.9091\n",
      "val Loss: 0.1697 Acc: 0.9386\n",
      "\n",
      "Epoch 65/299\n",
      "----------\n",
      "train Loss: 0.2346 Acc: 0.9115\n",
      "val Loss: 0.1610 Acc: 0.9411\n",
      "\n",
      "Epoch 66/299\n",
      "----------\n",
      "train Loss: 0.2196 Acc: 0.9170\n",
      "val Loss: 0.1766 Acc: 0.9344\n",
      "\n",
      "Epoch 67/299\n",
      "----------\n",
      "train Loss: 0.2260 Acc: 0.9160\n",
      "val Loss: 0.1648 Acc: 0.9398\n",
      "\n",
      "Epoch 68/299\n",
      "----------\n",
      "train Loss: 0.2361 Acc: 0.9101\n",
      "val Loss: 0.1701 Acc: 0.9357\n",
      "\n",
      "Epoch 69/299\n",
      "----------\n",
      "train Loss: 0.2278 Acc: 0.9140\n",
      "val Loss: 0.1636 Acc: 0.9377\n",
      "\n",
      "Epoch 70/299\n",
      "----------\n",
      "train Loss: 0.2293 Acc: 0.9152\n",
      "val Loss: 0.1601 Acc: 0.9448\n",
      "\n",
      "Epoch 71/299\n",
      "----------\n",
      "train Loss: 0.2192 Acc: 0.9195\n",
      "val Loss: 0.1592 Acc: 0.9406\n",
      "\n",
      "Epoch 72/299\n",
      "----------\n",
      "train Loss: 0.2296 Acc: 0.9153\n",
      "val Loss: 0.1552 Acc: 0.9448\n",
      "\n",
      "Epoch 73/299\n",
      "----------\n",
      "train Loss: 0.2269 Acc: 0.9142\n",
      "val Loss: 0.1473 Acc: 0.9469\n",
      "\n",
      "Epoch 74/299\n",
      "----------\n",
      "train Loss: 0.2365 Acc: 0.9121\n",
      "val Loss: 0.1628 Acc: 0.9415\n",
      "\n",
      "Epoch 75/299\n",
      "----------\n",
      "train Loss: 0.2280 Acc: 0.9157\n",
      "val Loss: 0.1747 Acc: 0.9340\n",
      "\n",
      "Epoch 76/299\n",
      "----------\n",
      "train Loss: 0.2277 Acc: 0.9137\n",
      "val Loss: 0.1812 Acc: 0.9315\n",
      "\n",
      "Epoch 77/299\n",
      "----------\n",
      "train Loss: 0.2325 Acc: 0.9114\n",
      "val Loss: 0.1617 Acc: 0.9398\n",
      "\n",
      "Epoch 78/299\n",
      "----------\n",
      "train Loss: 0.2294 Acc: 0.9138\n",
      "val Loss: 0.1620 Acc: 0.9411\n",
      "\n",
      "Epoch 79/299\n",
      "----------\n",
      "train Loss: 0.2281 Acc: 0.9118\n",
      "val Loss: 0.1860 Acc: 0.9315\n",
      "\n",
      "Epoch 80/299\n",
      "----------\n",
      "train Loss: 0.2242 Acc: 0.9151\n",
      "val Loss: 0.1612 Acc: 0.9411\n",
      "\n",
      "Epoch 81/299\n",
      "----------\n",
      "train Loss: 0.2264 Acc: 0.9147\n",
      "val Loss: 0.1558 Acc: 0.9444\n",
      "\n",
      "Epoch 82/299\n",
      "----------\n",
      "train Loss: 0.2274 Acc: 0.9116\n",
      "val Loss: 0.1800 Acc: 0.9315\n",
      "\n",
      "Epoch 83/299\n",
      "----------\n",
      "train Loss: 0.2231 Acc: 0.9141\n",
      "val Loss: 0.1584 Acc: 0.9452\n",
      "\n",
      "Epoch 84/299\n",
      "----------\n",
      "train Loss: 0.2329 Acc: 0.9130\n",
      "val Loss: 0.1549 Acc: 0.9440\n",
      "\n",
      "Epoch 85/299\n",
      "----------\n",
      "train Loss: 0.2274 Acc: 0.9155\n",
      "val Loss: 0.1621 Acc: 0.9423\n",
      "\n",
      "Epoch 86/299\n",
      "----------\n",
      "train Loss: 0.2310 Acc: 0.9140\n",
      "val Loss: 0.1585 Acc: 0.9419\n",
      "\n",
      "Epoch 87/299\n",
      "----------\n",
      "train Loss: 0.2303 Acc: 0.9133\n",
      "val Loss: 0.1529 Acc: 0.9460\n",
      "\n",
      "Epoch 88/299\n",
      "----------\n",
      "train Loss: 0.2403 Acc: 0.9093\n",
      "val Loss: 0.1667 Acc: 0.9411\n",
      "\n",
      "Epoch 89/299\n",
      "----------\n",
      "train Loss: 0.2263 Acc: 0.9134\n",
      "val Loss: 0.1675 Acc: 0.9402\n",
      "\n",
      "Epoch 90/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2276 Acc: 0.9139\n",
      "val Loss: 0.1727 Acc: 0.9365\n",
      "\n",
      "Epoch 91/299\n",
      "----------\n",
      "train Loss: 0.2262 Acc: 0.9125\n",
      "val Loss: 0.1631 Acc: 0.9411\n",
      "\n",
      "Epoch 92/299\n",
      "----------\n",
      "train Loss: 0.2205 Acc: 0.9168\n",
      "val Loss: 0.1596 Acc: 0.9440\n",
      "\n",
      "Epoch 93/299\n",
      "----------\n",
      "train Loss: 0.2305 Acc: 0.9101\n",
      "val Loss: 0.1638 Acc: 0.9394\n",
      "\n",
      "Epoch 94/299\n",
      "----------\n",
      "train Loss: 0.2246 Acc: 0.9150\n",
      "val Loss: 0.1527 Acc: 0.9473\n",
      "\n",
      "Epoch 95/299\n",
      "----------\n",
      "train Loss: 0.2278 Acc: 0.9155\n",
      "val Loss: 0.1777 Acc: 0.9340\n",
      "\n",
      "Epoch 96/299\n",
      "----------\n",
      "train Loss: 0.2284 Acc: 0.9163\n",
      "val Loss: 0.1581 Acc: 0.9440\n",
      "\n",
      "Epoch 97/299\n",
      "----------\n",
      "train Loss: 0.2239 Acc: 0.9172\n",
      "val Loss: 0.1453 Acc: 0.9473\n",
      "\n",
      "Epoch 98/299\n",
      "----------\n",
      "train Loss: 0.2324 Acc: 0.9103\n",
      "val Loss: 0.1657 Acc: 0.9381\n",
      "\n",
      "Epoch 99/299\n",
      "----------\n",
      "train Loss: 0.2289 Acc: 0.9129\n",
      "val Loss: 0.1532 Acc: 0.9440\n",
      "\n",
      "Epoch 100/299\n",
      "----------\n",
      "train Loss: 0.2173 Acc: 0.9193\n",
      "val Loss: 0.1766 Acc: 0.9352\n",
      "\n",
      "Epoch 101/299\n",
      "----------\n",
      "train Loss: 0.2255 Acc: 0.9156\n",
      "val Loss: 0.1906 Acc: 0.9265\n",
      "\n",
      "Epoch 102/299\n",
      "----------\n",
      "train Loss: 0.2255 Acc: 0.9142\n",
      "val Loss: 0.1566 Acc: 0.9448\n",
      "\n",
      "Epoch 103/299\n",
      "----------\n",
      "train Loss: 0.2264 Acc: 0.9155\n",
      "val Loss: 0.1609 Acc: 0.9419\n",
      "\n",
      "Epoch 104/299\n",
      "----------\n",
      "train Loss: 0.2287 Acc: 0.9141\n",
      "val Loss: 0.1693 Acc: 0.9373\n",
      "\n",
      "Epoch 105/299\n",
      "----------\n",
      "train Loss: 0.2246 Acc: 0.9142\n",
      "val Loss: 0.1492 Acc: 0.9456\n",
      "\n",
      "Epoch 106/299\n",
      "----------\n",
      "train Loss: 0.2245 Acc: 0.9126\n",
      "val Loss: 0.1494 Acc: 0.9431\n",
      "\n",
      "Epoch 107/299\n",
      "----------\n",
      "train Loss: 0.2267 Acc: 0.9161\n",
      "val Loss: 0.1565 Acc: 0.9423\n",
      "\n",
      "Epoch 108/299\n",
      "----------\n",
      "train Loss: 0.2283 Acc: 0.9127\n",
      "val Loss: 0.1566 Acc: 0.9423\n",
      "\n",
      "Epoch 109/299\n",
      "----------\n",
      "train Loss: 0.2249 Acc: 0.9165\n",
      "val Loss: 0.1554 Acc: 0.9452\n",
      "\n",
      "Epoch 110/299\n",
      "----------\n",
      "train Loss: 0.2296 Acc: 0.9149\n",
      "val Loss: 0.1566 Acc: 0.9415\n",
      "\n",
      "Epoch 111/299\n",
      "----------\n",
      "train Loss: 0.2283 Acc: 0.9128\n",
      "val Loss: 0.1538 Acc: 0.9435\n",
      "\n",
      "Epoch 112/299\n",
      "----------\n",
      "train Loss: 0.2248 Acc: 0.9169\n",
      "val Loss: 0.1416 Acc: 0.9510\n",
      "\n",
      "Epoch 113/299\n",
      "----------\n",
      "train Loss: 0.2224 Acc: 0.9183\n",
      "val Loss: 0.1557 Acc: 0.9444\n",
      "\n",
      "Epoch 114/299\n",
      "----------\n",
      "train Loss: 0.2270 Acc: 0.9122\n",
      "val Loss: 0.1706 Acc: 0.9361\n",
      "\n",
      "Epoch 115/299\n",
      "----------\n",
      "train Loss: 0.2248 Acc: 0.9158\n",
      "val Loss: 0.1659 Acc: 0.9398\n",
      "\n",
      "Epoch 116/299\n",
      "----------\n",
      "train Loss: 0.2228 Acc: 0.9179\n",
      "val Loss: 0.1413 Acc: 0.9506\n",
      "\n",
      "Epoch 117/299\n",
      "----------\n",
      "train Loss: 0.2155 Acc: 0.9179\n",
      "val Loss: 0.1653 Acc: 0.9373\n",
      "\n",
      "Epoch 118/299\n",
      "----------\n",
      "train Loss: 0.2273 Acc: 0.9137\n",
      "val Loss: 0.1780 Acc: 0.9328\n",
      "\n",
      "Epoch 119/299\n",
      "----------\n",
      "train Loss: 0.2275 Acc: 0.9119\n",
      "val Loss: 0.1588 Acc: 0.9415\n",
      "\n",
      "Epoch 120/299\n",
      "----------\n",
      "train Loss: 0.2200 Acc: 0.9145\n",
      "val Loss: 0.1777 Acc: 0.9332\n",
      "\n",
      "Epoch 121/299\n",
      "----------\n",
      "train Loss: 0.2214 Acc: 0.9165\n",
      "val Loss: 0.1531 Acc: 0.9440\n",
      "\n",
      "Epoch 122/299\n",
      "----------\n",
      "train Loss: 0.2253 Acc: 0.9173\n",
      "val Loss: 0.1597 Acc: 0.9411\n",
      "\n",
      "Epoch 123/299\n",
      "----------\n",
      "train Loss: 0.2239 Acc: 0.9163\n",
      "val Loss: 0.1577 Acc: 0.9402\n",
      "\n",
      "Epoch 124/299\n",
      "----------\n",
      "train Loss: 0.2213 Acc: 0.9186\n",
      "val Loss: 0.1462 Acc: 0.9489\n",
      "\n",
      "Epoch 125/299\n",
      "----------\n",
      "train Loss: 0.2279 Acc: 0.9153\n",
      "val Loss: 0.1652 Acc: 0.9386\n",
      "\n",
      "Epoch 126/299\n",
      "----------\n",
      "train Loss: 0.2193 Acc: 0.9182\n",
      "val Loss: 0.1524 Acc: 0.9444\n",
      "\n",
      "Epoch 127/299\n",
      "----------\n",
      "train Loss: 0.2240 Acc: 0.9132\n",
      "val Loss: 0.1528 Acc: 0.9440\n",
      "\n",
      "Epoch 128/299\n",
      "----------\n",
      "train Loss: 0.2225 Acc: 0.9174\n",
      "val Loss: 0.1521 Acc: 0.9452\n",
      "\n",
      "Epoch 129/299\n",
      "----------\n",
      "train Loss: 0.2179 Acc: 0.9193\n",
      "val Loss: 0.1762 Acc: 0.9352\n",
      "\n",
      "Epoch 130/299\n",
      "----------\n",
      "train Loss: 0.2249 Acc: 0.9166\n",
      "val Loss: 0.1786 Acc: 0.9332\n",
      "\n",
      "Epoch 131/299\n",
      "----------\n",
      "train Loss: 0.2338 Acc: 0.9105\n",
      "val Loss: 0.1495 Acc: 0.9473\n",
      "\n",
      "Epoch 132/299\n",
      "----------\n",
      "train Loss: 0.2207 Acc: 0.9163\n",
      "val Loss: 0.1509 Acc: 0.9435\n",
      "\n",
      "Epoch 133/299\n",
      "----------\n",
      "train Loss: 0.2207 Acc: 0.9152\n",
      "val Loss: 0.2033 Acc: 0.9228\n",
      "\n",
      "Epoch 134/299\n",
      "----------\n",
      "train Loss: 0.2271 Acc: 0.9127\n",
      "val Loss: 0.1605 Acc: 0.9411\n",
      "\n",
      "Epoch 135/299\n",
      "----------\n",
      "train Loss: 0.2214 Acc: 0.9164\n",
      "val Loss: 0.1606 Acc: 0.9423\n",
      "\n",
      "Epoch 136/299\n",
      "----------\n",
      "train Loss: 0.2179 Acc: 0.9151\n",
      "val Loss: 0.1637 Acc: 0.9373\n",
      "\n",
      "Epoch 137/299\n",
      "----------\n",
      "train Loss: 0.2217 Acc: 0.9154\n",
      "val Loss: 0.1492 Acc: 0.9444\n",
      "\n",
      "Epoch 138/299\n",
      "----------\n",
      "train Loss: 0.2272 Acc: 0.9136\n",
      "val Loss: 0.1536 Acc: 0.9440\n",
      "\n",
      "Epoch 139/299\n",
      "----------\n",
      "train Loss: 0.2214 Acc: 0.9167\n",
      "val Loss: 0.1564 Acc: 0.9435\n",
      "\n",
      "Epoch 140/299\n",
      "----------\n",
      "train Loss: 0.2252 Acc: 0.9129\n",
      "val Loss: 0.1669 Acc: 0.9373\n",
      "\n",
      "Epoch 141/299\n",
      "----------\n",
      "train Loss: 0.2333 Acc: 0.9088\n",
      "val Loss: 0.1805 Acc: 0.9298\n",
      "\n",
      "Epoch 142/299\n",
      "----------\n",
      "train Loss: 0.2183 Acc: 0.9160\n",
      "val Loss: 0.1500 Acc: 0.9456\n",
      "\n",
      "Epoch 143/299\n",
      "----------\n",
      "train Loss: 0.2226 Acc: 0.9148\n",
      "val Loss: 0.1539 Acc: 0.9452\n",
      "\n",
      "Epoch 144/299\n",
      "----------\n",
      "train Loss: 0.2184 Acc: 0.9184\n",
      "val Loss: 0.1653 Acc: 0.9377\n",
      "\n",
      "Epoch 145/299\n",
      "----------\n",
      "train Loss: 0.2261 Acc: 0.9128\n",
      "val Loss: 0.1533 Acc: 0.9444\n",
      "\n",
      "Epoch 146/299\n",
      "----------\n",
      "train Loss: 0.2230 Acc: 0.9181\n",
      "val Loss: 0.1719 Acc: 0.9340\n",
      "\n",
      "Epoch 147/299\n",
      "----------\n",
      "train Loss: 0.2230 Acc: 0.9129\n",
      "val Loss: 0.1567 Acc: 0.9423\n",
      "\n",
      "Epoch 148/299\n",
      "----------\n",
      "train Loss: 0.2177 Acc: 0.9186\n",
      "val Loss: 0.1428 Acc: 0.9456\n",
      "\n",
      "Epoch 149/299\n",
      "----------\n",
      "train Loss: 0.2240 Acc: 0.9162\n",
      "val Loss: 0.1637 Acc: 0.9381\n",
      "\n",
      "Epoch 150/299\n",
      "----------\n",
      "train Loss: 0.2183 Acc: 0.9175\n",
      "val Loss: 0.1603 Acc: 0.9398\n",
      "\n",
      "Epoch 151/299\n",
      "----------\n",
      "train Loss: 0.2217 Acc: 0.9169\n",
      "val Loss: 0.1667 Acc: 0.9340\n",
      "\n",
      "Epoch 152/299\n",
      "----------\n",
      "train Loss: 0.2244 Acc: 0.9157\n",
      "val Loss: 0.1632 Acc: 0.9419\n",
      "\n",
      "Epoch 153/299\n",
      "----------\n",
      "train Loss: 0.2191 Acc: 0.9183\n",
      "val Loss: 0.1489 Acc: 0.9452\n",
      "\n",
      "Epoch 154/299\n",
      "----------\n",
      "train Loss: 0.2268 Acc: 0.9141\n",
      "val Loss: 0.1813 Acc: 0.9286\n",
      "\n",
      "Epoch 155/299\n",
      "----------\n",
      "train Loss: 0.2232 Acc: 0.9175\n",
      "val Loss: 0.1585 Acc: 0.9402\n",
      "\n",
      "Epoch 156/299\n",
      "----------\n",
      "train Loss: 0.2212 Acc: 0.9162\n",
      "val Loss: 0.1514 Acc: 0.9427\n",
      "\n",
      "Epoch 157/299\n",
      "----------\n",
      "train Loss: 0.2193 Acc: 0.9180\n",
      "val Loss: 0.1442 Acc: 0.9498\n",
      "\n",
      "Epoch 158/299\n",
      "----------\n",
      "train Loss: 0.2224 Acc: 0.9160\n",
      "val Loss: 0.1817 Acc: 0.9319\n",
      "\n",
      "Epoch 159/299\n",
      "----------\n",
      "train Loss: 0.2251 Acc: 0.9141\n",
      "val Loss: 0.1663 Acc: 0.9377\n",
      "\n",
      "Epoch 160/299\n",
      "----------\n",
      "train Loss: 0.2185 Acc: 0.9169\n",
      "val Loss: 0.1644 Acc: 0.9381\n",
      "\n",
      "Epoch 161/299\n",
      "----------\n",
      "train Loss: 0.2247 Acc: 0.9160\n",
      "val Loss: 0.1718 Acc: 0.9348\n",
      "\n",
      "Epoch 162/299\n",
      "----------\n",
      "train Loss: 0.2266 Acc: 0.9175\n",
      "val Loss: 0.1796 Acc: 0.9311\n",
      "\n",
      "Epoch 163/299\n",
      "----------\n",
      "train Loss: 0.2152 Acc: 0.9189\n",
      "val Loss: 0.1431 Acc: 0.9506\n",
      "\n",
      "Epoch 164/299\n",
      "----------\n",
      "train Loss: 0.2228 Acc: 0.9164\n",
      "val Loss: 0.1517 Acc: 0.9460\n",
      "\n",
      "Epoch 165/299\n",
      "----------\n",
      "train Loss: 0.2271 Acc: 0.9120\n",
      "val Loss: 0.1583 Acc: 0.9406\n",
      "\n",
      "Epoch 166/299\n",
      "----------\n",
      "train Loss: 0.2203 Acc: 0.9159\n",
      "val Loss: 0.1567 Acc: 0.9419\n",
      "\n",
      "Epoch 167/299\n",
      "----------\n",
      "train Loss: 0.2170 Acc: 0.9175\n",
      "val Loss: 0.1613 Acc: 0.9423\n",
      "\n",
      "Epoch 168/299\n",
      "----------\n",
      "train Loss: 0.2178 Acc: 0.9137\n",
      "val Loss: 0.1362 Acc: 0.9506\n",
      "\n",
      "Epoch 169/299\n",
      "----------\n",
      "train Loss: 0.2229 Acc: 0.9163\n",
      "val Loss: 0.1468 Acc: 0.9481\n",
      "\n",
      "Epoch 170/299\n",
      "----------\n",
      "train Loss: 0.2213 Acc: 0.9164\n",
      "val Loss: 0.1587 Acc: 0.9398\n",
      "\n",
      "Epoch 171/299\n",
      "----------\n",
      "train Loss: 0.2189 Acc: 0.9153\n",
      "val Loss: 0.1501 Acc: 0.9456\n",
      "\n",
      "Epoch 172/299\n",
      "----------\n",
      "train Loss: 0.2183 Acc: 0.9164\n",
      "val Loss: 0.1411 Acc: 0.9440\n",
      "\n",
      "Epoch 173/299\n",
      "----------\n",
      "train Loss: 0.2198 Acc: 0.9164\n",
      "val Loss: 0.1627 Acc: 0.9402\n",
      "\n",
      "Epoch 174/299\n",
      "----------\n",
      "train Loss: 0.2223 Acc: 0.9160\n",
      "val Loss: 0.1581 Acc: 0.9398\n",
      "\n",
      "Epoch 175/299\n",
      "----------\n",
      "train Loss: 0.2188 Acc: 0.9178\n",
      "val Loss: 0.1560 Acc: 0.9427\n",
      "\n",
      "Epoch 176/299\n",
      "----------\n",
      "train Loss: 0.2242 Acc: 0.9137\n",
      "val Loss: 0.1537 Acc: 0.9460\n",
      "\n",
      "Epoch 177/299\n",
      "----------\n",
      "train Loss: 0.2249 Acc: 0.9125\n",
      "val Loss: 0.1527 Acc: 0.9473\n",
      "\n",
      "Epoch 178/299\n",
      "----------\n",
      "train Loss: 0.2222 Acc: 0.9159\n",
      "val Loss: 0.1411 Acc: 0.9502\n",
      "\n",
      "Epoch 179/299\n",
      "----------\n",
      "train Loss: 0.2217 Acc: 0.9162\n",
      "val Loss: 0.1762 Acc: 0.9315\n",
      "\n",
      "Epoch 180/299\n",
      "----------\n",
      "train Loss: 0.2195 Acc: 0.9175\n",
      "val Loss: 0.1481 Acc: 0.9469\n",
      "\n",
      "Epoch 181/299\n",
      "----------\n",
      "train Loss: 0.2166 Acc: 0.9161\n",
      "val Loss: 0.1516 Acc: 0.9440\n",
      "\n",
      "Epoch 182/299\n",
      "----------\n",
      "train Loss: 0.2111 Acc: 0.9231\n",
      "val Loss: 0.1902 Acc: 0.9282\n",
      "\n",
      "Epoch 183/299\n",
      "----------\n",
      "train Loss: 0.2179 Acc: 0.9196\n",
      "val Loss: 0.1845 Acc: 0.9307\n",
      "\n",
      "Epoch 184/299\n",
      "----------\n",
      "train Loss: 0.2198 Acc: 0.9142\n",
      "val Loss: 0.1526 Acc: 0.9431\n",
      "\n",
      "Epoch 185/299\n",
      "----------\n",
      "train Loss: 0.2226 Acc: 0.9155\n",
      "val Loss: 0.1552 Acc: 0.9415\n",
      "\n",
      "Epoch 186/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2136 Acc: 0.9189\n",
      "val Loss: 0.1437 Acc: 0.9477\n",
      "\n",
      "Epoch 187/299\n",
      "----------\n",
      "train Loss: 0.2252 Acc: 0.9128\n",
      "val Loss: 0.1418 Acc: 0.9481\n",
      "\n",
      "Epoch 188/299\n",
      "----------\n",
      "train Loss: 0.2127 Acc: 0.9188\n",
      "val Loss: 0.1586 Acc: 0.9427\n",
      "\n",
      "Epoch 189/299\n",
      "----------\n",
      "train Loss: 0.2255 Acc: 0.9143\n",
      "val Loss: 0.1450 Acc: 0.9514\n",
      "\n",
      "Epoch 190/299\n",
      "----------\n",
      "train Loss: 0.2203 Acc: 0.9164\n",
      "val Loss: 0.1712 Acc: 0.9344\n",
      "\n",
      "Epoch 191/299\n",
      "----------\n",
      "train Loss: 0.2176 Acc: 0.9167\n",
      "val Loss: 0.1580 Acc: 0.9419\n",
      "\n",
      "Epoch 192/299\n",
      "----------\n",
      "train Loss: 0.2198 Acc: 0.9157\n",
      "val Loss: 0.1453 Acc: 0.9489\n",
      "\n",
      "Epoch 193/299\n",
      "----------\n",
      "train Loss: 0.2130 Acc: 0.9199\n",
      "val Loss: 0.1454 Acc: 0.9494\n",
      "\n",
      "Epoch 194/299\n",
      "----------\n",
      "train Loss: 0.2209 Acc: 0.9154\n",
      "val Loss: 0.1547 Acc: 0.9431\n",
      "\n",
      "Epoch 195/299\n",
      "----------\n",
      "train Loss: 0.2223 Acc: 0.9152\n",
      "val Loss: 0.1450 Acc: 0.9473\n",
      "\n",
      "Epoch 196/299\n",
      "----------\n",
      "train Loss: 0.2256 Acc: 0.9163\n",
      "val Loss: 0.1570 Acc: 0.9444\n",
      "\n",
      "Epoch 197/299\n",
      "----------\n",
      "train Loss: 0.2265 Acc: 0.9148\n",
      "val Loss: 0.1557 Acc: 0.9435\n",
      "\n",
      "Epoch 198/299\n",
      "----------\n",
      "train Loss: 0.2215 Acc: 0.9167\n",
      "val Loss: 0.1748 Acc: 0.9328\n",
      "\n",
      "Epoch 199/299\n",
      "----------\n",
      "train Loss: 0.2152 Acc: 0.9185\n",
      "val Loss: 0.1431 Acc: 0.9469\n",
      "\n",
      "Epoch 200/299\n",
      "----------\n",
      "train Loss: 0.2194 Acc: 0.9188\n",
      "val Loss: 0.1482 Acc: 0.9465\n",
      "\n",
      "Epoch 201/299\n",
      "----------\n",
      "train Loss: 0.2206 Acc: 0.9183\n",
      "val Loss: 0.1664 Acc: 0.9373\n",
      "\n",
      "Epoch 202/299\n",
      "----------\n",
      "train Loss: 0.2164 Acc: 0.9178\n",
      "val Loss: 0.1583 Acc: 0.9386\n",
      "\n",
      "Epoch 203/299\n",
      "----------\n",
      "train Loss: 0.2155 Acc: 0.9180\n",
      "val Loss: 0.1475 Acc: 0.9477\n",
      "\n",
      "Epoch 204/299\n",
      "----------\n",
      "train Loss: 0.2206 Acc: 0.9146\n",
      "val Loss: 0.1609 Acc: 0.9394\n",
      "\n",
      "Epoch 205/299\n",
      "----------\n",
      "train Loss: 0.2205 Acc: 0.9148\n",
      "val Loss: 0.1456 Acc: 0.9448\n",
      "\n",
      "Epoch 206/299\n",
      "----------\n",
      "train Loss: 0.2219 Acc: 0.9173\n",
      "val Loss: 0.1493 Acc: 0.9456\n",
      "\n",
      "Epoch 207/299\n",
      "----------\n",
      "train Loss: 0.2212 Acc: 0.9140\n",
      "val Loss: 0.1968 Acc: 0.9253\n",
      "\n",
      "Epoch 208/299\n",
      "----------\n",
      "train Loss: 0.2113 Acc: 0.9227\n",
      "val Loss: 0.1502 Acc: 0.9452\n",
      "\n",
      "Epoch 209/299\n",
      "----------\n",
      "train Loss: 0.2189 Acc: 0.9158\n",
      "val Loss: 0.1393 Acc: 0.9531\n",
      "\n",
      "Epoch 210/299\n",
      "----------\n",
      "train Loss: 0.2221 Acc: 0.9130\n",
      "val Loss: 0.1447 Acc: 0.9498\n",
      "\n",
      "Epoch 211/299\n",
      "----------\n",
      "train Loss: 0.2174 Acc: 0.9155\n",
      "val Loss: 0.1664 Acc: 0.9377\n",
      "\n",
      "Epoch 212/299\n",
      "----------\n",
      "train Loss: 0.2201 Acc: 0.9165\n",
      "val Loss: 0.1437 Acc: 0.9473\n",
      "\n",
      "Epoch 213/299\n",
      "----------\n",
      "train Loss: 0.2178 Acc: 0.9140\n",
      "val Loss: 0.1727 Acc: 0.9336\n",
      "\n",
      "Epoch 214/299\n",
      "----------\n",
      "train Loss: 0.2146 Acc: 0.9200\n",
      "val Loss: 0.1576 Acc: 0.9419\n",
      "\n",
      "Epoch 215/299\n",
      "----------\n",
      "train Loss: 0.2161 Acc: 0.9199\n",
      "val Loss: 0.1595 Acc: 0.9411\n",
      "\n",
      "Epoch 216/299\n",
      "----------\n",
      "train Loss: 0.2137 Acc: 0.9166\n",
      "val Loss: 0.1796 Acc: 0.9328\n",
      "\n",
      "Epoch 217/299\n",
      "----------\n",
      "train Loss: 0.2193 Acc: 0.9166\n",
      "val Loss: 0.1462 Acc: 0.9477\n",
      "\n",
      "Epoch 218/299\n",
      "----------\n",
      "train Loss: 0.2147 Acc: 0.9174\n",
      "val Loss: 0.1522 Acc: 0.9423\n",
      "\n",
      "Epoch 219/299\n",
      "----------\n",
      "train Loss: 0.2096 Acc: 0.9190\n",
      "val Loss: 0.1447 Acc: 0.9477\n",
      "\n",
      "Epoch 220/299\n",
      "----------\n",
      "train Loss: 0.2191 Acc: 0.9160\n",
      "val Loss: 0.1608 Acc: 0.9398\n",
      "\n",
      "Epoch 221/299\n",
      "----------\n",
      "train Loss: 0.2257 Acc: 0.9148\n",
      "val Loss: 0.1481 Acc: 0.9448\n",
      "\n",
      "Epoch 222/299\n",
      "----------\n",
      "train Loss: 0.2142 Acc: 0.9194\n",
      "val Loss: 0.1399 Acc: 0.9489\n",
      "\n",
      "Epoch 223/299\n",
      "----------\n",
      "train Loss: 0.2191 Acc: 0.9183\n",
      "val Loss: 0.1611 Acc: 0.9377\n",
      "\n",
      "Epoch 224/299\n",
      "----------\n",
      "train Loss: 0.2186 Acc: 0.9149\n",
      "val Loss: 0.1492 Acc: 0.9456\n",
      "\n",
      "Epoch 225/299\n",
      "----------\n",
      "train Loss: 0.2203 Acc: 0.9178\n",
      "val Loss: 0.1727 Acc: 0.9332\n",
      "\n",
      "Epoch 226/299\n",
      "----------\n",
      "train Loss: 0.2261 Acc: 0.9158\n",
      "val Loss: 0.1483 Acc: 0.9452\n",
      "\n",
      "Epoch 227/299\n",
      "----------\n",
      "train Loss: 0.2207 Acc: 0.9139\n",
      "val Loss: 0.1562 Acc: 0.9415\n",
      "\n",
      "Epoch 228/299\n",
      "----------\n",
      "train Loss: 0.2253 Acc: 0.9140\n",
      "val Loss: 0.1505 Acc: 0.9423\n",
      "\n",
      "Epoch 229/299\n",
      "----------\n",
      "train Loss: 0.2248 Acc: 0.9136\n",
      "val Loss: 0.1517 Acc: 0.9448\n",
      "\n",
      "Epoch 230/299\n",
      "----------\n",
      "train Loss: 0.2212 Acc: 0.9152\n",
      "val Loss: 0.1386 Acc: 0.9510\n",
      "\n",
      "Epoch 231/299\n",
      "----------\n",
      "train Loss: 0.2150 Acc: 0.9180\n",
      "val Loss: 0.1577 Acc: 0.9398\n",
      "\n",
      "Epoch 232/299\n",
      "----------\n",
      "train Loss: 0.2162 Acc: 0.9187\n",
      "val Loss: 0.1713 Acc: 0.9319\n",
      "\n",
      "Epoch 233/299\n",
      "----------\n",
      "train Loss: 0.2174 Acc: 0.9178\n",
      "val Loss: 0.1771 Acc: 0.9323\n",
      "\n",
      "Epoch 234/299\n",
      "----------\n",
      "train Loss: 0.2159 Acc: 0.9159\n",
      "val Loss: 0.1537 Acc: 0.9431\n",
      "\n",
      "Epoch 235/299\n",
      "----------\n",
      "train Loss: 0.2259 Acc: 0.9124\n",
      "val Loss: 0.1684 Acc: 0.9377\n",
      "\n",
      "Epoch 236/299\n",
      "----------\n",
      "train Loss: 0.2157 Acc: 0.9192\n",
      "val Loss: 0.1566 Acc: 0.9427\n",
      "\n",
      "Epoch 237/299\n",
      "----------\n",
      "train Loss: 0.2203 Acc: 0.9186\n",
      "val Loss: 0.1484 Acc: 0.9444\n",
      "\n",
      "Epoch 238/299\n",
      "----------\n",
      "train Loss: 0.2248 Acc: 0.9141\n",
      "val Loss: 0.1807 Acc: 0.9303\n",
      "\n",
      "Epoch 239/299\n",
      "----------\n",
      "train Loss: 0.2246 Acc: 0.9152\n",
      "val Loss: 0.1414 Acc: 0.9477\n",
      "\n",
      "Epoch 240/299\n",
      "----------\n",
      "train Loss: 0.2141 Acc: 0.9190\n",
      "val Loss: 0.1534 Acc: 0.9390\n",
      "\n",
      "Epoch 241/299\n",
      "----------\n",
      "train Loss: 0.2190 Acc: 0.9166\n",
      "val Loss: 0.1453 Acc: 0.9440\n",
      "\n",
      "Epoch 242/299\n",
      "----------\n",
      "train Loss: 0.2154 Acc: 0.9206\n",
      "val Loss: 0.1576 Acc: 0.9411\n",
      "\n",
      "Epoch 243/299\n",
      "----------\n",
      "train Loss: 0.2126 Acc: 0.9195\n",
      "val Loss: 0.1836 Acc: 0.9303\n",
      "\n",
      "Epoch 244/299\n",
      "----------\n",
      "train Loss: 0.2183 Acc: 0.9174\n",
      "val Loss: 0.1549 Acc: 0.9394\n",
      "\n",
      "Epoch 245/299\n",
      "----------\n",
      "train Loss: 0.2220 Acc: 0.9140\n",
      "val Loss: 0.1568 Acc: 0.9402\n",
      "\n",
      "Epoch 246/299\n",
      "----------\n",
      "train Loss: 0.2207 Acc: 0.9164\n",
      "val Loss: 0.1693 Acc: 0.9357\n",
      "\n",
      "Epoch 247/299\n",
      "----------\n",
      "train Loss: 0.2224 Acc: 0.9167\n",
      "val Loss: 0.1826 Acc: 0.9274\n",
      "\n",
      "Epoch 248/299\n",
      "----------\n",
      "train Loss: 0.2147 Acc: 0.9193\n",
      "val Loss: 0.1637 Acc: 0.9377\n",
      "\n",
      "Epoch 249/299\n",
      "----------\n",
      "train Loss: 0.2247 Acc: 0.9138\n",
      "val Loss: 0.1528 Acc: 0.9448\n",
      "\n",
      "Epoch 250/299\n",
      "----------\n",
      "train Loss: 0.2130 Acc: 0.9215\n",
      "val Loss: 0.1410 Acc: 0.9481\n",
      "\n",
      "Epoch 251/299\n",
      "----------\n",
      "train Loss: 0.2176 Acc: 0.9204\n",
      "val Loss: 0.1386 Acc: 0.9510\n",
      "\n",
      "Epoch 252/299\n",
      "----------\n",
      "train Loss: 0.2184 Acc: 0.9147\n",
      "val Loss: 0.1738 Acc: 0.9336\n",
      "\n",
      "Epoch 253/299\n",
      "----------\n",
      "train Loss: 0.2130 Acc: 0.9197\n",
      "val Loss: 0.1550 Acc: 0.9423\n",
      "\n",
      "Epoch 254/299\n",
      "----------\n",
      "train Loss: 0.2186 Acc: 0.9147\n",
      "val Loss: 0.1376 Acc: 0.9535\n",
      "\n",
      "Epoch 255/299\n",
      "----------\n",
      "train Loss: 0.2217 Acc: 0.9168\n",
      "val Loss: 0.1690 Acc: 0.9361\n",
      "\n",
      "Epoch 256/299\n",
      "----------\n",
      "train Loss: 0.2144 Acc: 0.9189\n",
      "val Loss: 0.1477 Acc: 0.9448\n",
      "\n",
      "Epoch 257/299\n",
      "----------\n",
      "train Loss: 0.2159 Acc: 0.9174\n",
      "val Loss: 0.1499 Acc: 0.9427\n",
      "\n",
      "Epoch 258/299\n",
      "----------\n",
      "train Loss: 0.2193 Acc: 0.9184\n",
      "val Loss: 0.1439 Acc: 0.9448\n",
      "\n",
      "Epoch 259/299\n",
      "----------\n",
      "train Loss: 0.2117 Acc: 0.9212\n",
      "val Loss: 0.1605 Acc: 0.9377\n",
      "\n",
      "Epoch 260/299\n",
      "----------\n",
      "train Loss: 0.2156 Acc: 0.9182\n",
      "val Loss: 0.1519 Acc: 0.9444\n",
      "\n",
      "Epoch 261/299\n",
      "----------\n",
      "train Loss: 0.2135 Acc: 0.9169\n",
      "val Loss: 0.1447 Acc: 0.9469\n",
      "\n",
      "Epoch 262/299\n",
      "----------\n",
      "train Loss: 0.2166 Acc: 0.9210\n",
      "val Loss: 0.1481 Acc: 0.9448\n",
      "\n",
      "Epoch 263/299\n",
      "----------\n",
      "train Loss: 0.2134 Acc: 0.9180\n",
      "val Loss: 0.1765 Acc: 0.9315\n",
      "\n",
      "Epoch 264/299\n",
      "----------\n",
      "train Loss: 0.2132 Acc: 0.9204\n",
      "val Loss: 0.1548 Acc: 0.9435\n",
      "\n",
      "Epoch 265/299\n",
      "----------\n",
      "train Loss: 0.2187 Acc: 0.9193\n",
      "val Loss: 0.1484 Acc: 0.9456\n",
      "\n",
      "Epoch 266/299\n",
      "----------\n",
      "train Loss: 0.2209 Acc: 0.9141\n",
      "val Loss: 0.1686 Acc: 0.9352\n",
      "\n",
      "Epoch 267/299\n",
      "----------\n",
      "train Loss: 0.2177 Acc: 0.9191\n",
      "val Loss: 0.1487 Acc: 0.9452\n",
      "\n",
      "Epoch 268/299\n",
      "----------\n",
      "train Loss: 0.2230 Acc: 0.9143\n",
      "val Loss: 0.1588 Acc: 0.9423\n",
      "\n",
      "Epoch 269/299\n",
      "----------\n",
      "train Loss: 0.2158 Acc: 0.9183\n",
      "val Loss: 0.1609 Acc: 0.9357\n",
      "\n",
      "Epoch 270/299\n",
      "----------\n",
      "train Loss: 0.2148 Acc: 0.9180\n",
      "val Loss: 0.1438 Acc: 0.9477\n",
      "\n",
      "Epoch 271/299\n",
      "----------\n",
      "train Loss: 0.2137 Acc: 0.9195\n",
      "val Loss: 0.1443 Acc: 0.9481\n",
      "\n",
      "Epoch 272/299\n",
      "----------\n",
      "train Loss: 0.2168 Acc: 0.9191\n",
      "val Loss: 0.1465 Acc: 0.9440\n",
      "\n",
      "Epoch 273/299\n",
      "----------\n",
      "train Loss: 0.2083 Acc: 0.9224\n",
      "val Loss: 0.1500 Acc: 0.9431\n",
      "\n",
      "Epoch 274/299\n",
      "----------\n",
      "train Loss: 0.2154 Acc: 0.9158\n",
      "val Loss: 0.1568 Acc: 0.9373\n",
      "\n",
      "Epoch 275/299\n",
      "----------\n",
      "train Loss: 0.2122 Acc: 0.9212\n",
      "val Loss: 0.1541 Acc: 0.9411\n",
      "\n",
      "Epoch 276/299\n",
      "----------\n",
      "train Loss: 0.2222 Acc: 0.9140\n",
      "val Loss: 0.1505 Acc: 0.9440\n",
      "\n",
      "Epoch 277/299\n",
      "----------\n",
      "train Loss: 0.2224 Acc: 0.9169\n",
      "val Loss: 0.1496 Acc: 0.9419\n",
      "\n",
      "Epoch 278/299\n",
      "----------\n",
      "train Loss: 0.2092 Acc: 0.9202\n",
      "val Loss: 0.1547 Acc: 0.9402\n",
      "\n",
      "Epoch 279/299\n",
      "----------\n",
      "train Loss: 0.2171 Acc: 0.9184\n",
      "val Loss: 0.1431 Acc: 0.9477\n",
      "\n",
      "Epoch 280/299\n",
      "----------\n",
      "train Loss: 0.2131 Acc: 0.9166\n",
      "val Loss: 0.1622 Acc: 0.9373\n",
      "\n",
      "Epoch 281/299\n",
      "----------\n",
      "train Loss: 0.2255 Acc: 0.9139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1395 Acc: 0.9477\n",
      "\n",
      "Epoch 282/299\n",
      "----------\n",
      "train Loss: 0.2124 Acc: 0.9213\n",
      "val Loss: 0.1615 Acc: 0.9386\n",
      "\n",
      "Epoch 283/299\n",
      "----------\n",
      "train Loss: 0.2183 Acc: 0.9191\n",
      "val Loss: 0.1632 Acc: 0.9394\n",
      "\n",
      "Epoch 284/299\n",
      "----------\n",
      "train Loss: 0.2101 Acc: 0.9225\n",
      "val Loss: 0.1431 Acc: 0.9460\n",
      "\n",
      "Epoch 285/299\n",
      "----------\n",
      "train Loss: 0.2200 Acc: 0.9149\n",
      "val Loss: 0.1532 Acc: 0.9419\n",
      "\n",
      "Epoch 286/299\n",
      "----------\n",
      "train Loss: 0.2165 Acc: 0.9185\n",
      "val Loss: 0.1446 Acc: 0.9452\n",
      "\n",
      "Epoch 287/299\n",
      "----------\n",
      "train Loss: 0.2194 Acc: 0.9162\n",
      "val Loss: 0.1558 Acc: 0.9406\n",
      "\n",
      "Epoch 288/299\n",
      "----------\n",
      "train Loss: 0.2213 Acc: 0.9162\n",
      "val Loss: 0.1553 Acc: 0.9406\n",
      "\n",
      "Epoch 289/299\n",
      "----------\n",
      "train Loss: 0.2142 Acc: 0.9169\n",
      "val Loss: 0.1459 Acc: 0.9460\n",
      "\n",
      "Epoch 290/299\n",
      "----------\n",
      "train Loss: 0.2175 Acc: 0.9154\n",
      "val Loss: 0.1436 Acc: 0.9465\n",
      "\n",
      "Epoch 291/299\n",
      "----------\n",
      "train Loss: 0.2133 Acc: 0.9203\n",
      "val Loss: 0.1533 Acc: 0.9435\n",
      "\n",
      "Epoch 292/299\n",
      "----------\n",
      "train Loss: 0.2181 Acc: 0.9161\n",
      "val Loss: 0.1414 Acc: 0.9489\n",
      "\n",
      "Epoch 293/299\n",
      "----------\n",
      "train Loss: 0.2159 Acc: 0.9199\n",
      "val Loss: 0.1545 Acc: 0.9423\n",
      "\n",
      "Epoch 294/299\n",
      "----------\n",
      "train Loss: 0.2188 Acc: 0.9176\n",
      "val Loss: 0.1710 Acc: 0.9348\n",
      "\n",
      "Epoch 295/299\n",
      "----------\n",
      "train Loss: 0.2127 Acc: 0.9186\n",
      "val Loss: 0.1473 Acc: 0.9452\n",
      "\n",
      "Epoch 296/299\n",
      "----------\n",
      "train Loss: 0.2184 Acc: 0.9174\n",
      "val Loss: 0.1738 Acc: 0.9323\n",
      "\n",
      "Epoch 297/299\n",
      "----------\n",
      "train Loss: 0.2143 Acc: 0.9191\n",
      "val Loss: 0.1558 Acc: 0.9427\n",
      "\n",
      "Epoch 298/299\n",
      "----------\n",
      "train Loss: 0.2188 Acc: 0.9168\n",
      "val Loss: 0.1392 Acc: 0.9485\n",
      "\n",
      "Epoch 299/299\n",
      "----------\n",
      "train Loss: 0.2190 Acc: 0.9152\n",
      "val Loss: 0.1460 Acc: 0.9465\n",
      "\n",
      "\n",
      "##############################\n",
      "------ Summary ------\n",
      "model -> alexnet\n",
      "epochs -> 300\n",
      "lr -> 0.0001\n",
      "batch size -> 8\n",
      "\n",
      "Training complete in 46m 38s\n",
      "Best val Acc: 0.953508\n",
      "##############################\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== END ====\n",
      "[[778   6   7]\n",
      " [  5 781  13]\n",
      " [  7  74 738]]\n",
      "\n",
      "Sensitivity or recall total\n",
      "0.9535076795350768\n",
      "\n",
      "Sensitivity or recall per classes\n",
      "[0.98 0.98 0.9 ]\n",
      "\n",
      "Precision\n",
      "[0.98 0.91 0.97]\n",
      "\n",
      "F1 Score\n",
      "[0.98 0.94 0.94]\n",
      "Confusion matrix, without normalization\n",
      "\n",
      "============================================================\n",
      "==== INITIALIZING WITH PARAMETERS: ====\n",
      "model -> vgg\n",
      "epochs -> 300\n",
      "lr -> 0.0001\n",
      "batch size -> 8\n",
      "\n",
      "--------------------\n",
      "Params to learn:\n",
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n",
      "\n",
      "--------------------\n",
      "\n",
      "== Epochs ==\n",
      "Epoch 0/299\n",
      "----------\n",
      "train Loss: 0.5465 Acc: 0.8394\n",
      "val Loss: 0.3144 Acc: 0.9315\n",
      "\n",
      "Epoch 1/299\n",
      "----------\n",
      "train Loss: 0.3280 Acc: 0.9087\n",
      "val Loss: 0.2464 Acc: 0.9369\n",
      "\n",
      "Epoch 2/299\n",
      "----------\n",
      "train Loss: 0.2771 Acc: 0.9191\n",
      "val Loss: 0.2156 Acc: 0.9411\n",
      "\n",
      "Epoch 3/299\n",
      "----------\n",
      "train Loss: 0.2557 Acc: 0.9170\n",
      "val Loss: 0.1973 Acc: 0.9431\n",
      "\n",
      "Epoch 4/299\n",
      "----------\n",
      "train Loss: 0.2392 Acc: 0.9229\n",
      "val Loss: 0.1798 Acc: 0.9448\n",
      "\n",
      "Epoch 5/299\n",
      "----------\n",
      "train Loss: 0.2299 Acc: 0.9234\n",
      "val Loss: 0.1818 Acc: 0.9460\n",
      "\n",
      "Epoch 6/299\n",
      "----------\n",
      "train Loss: 0.2249 Acc: 0.9260\n",
      "val Loss: 0.1693 Acc: 0.9489\n",
      "\n",
      "Epoch 7/299\n",
      "----------\n",
      "train Loss: 0.2176 Acc: 0.9258\n",
      "val Loss: 0.1685 Acc: 0.9469\n",
      "\n",
      "Epoch 8/299\n",
      "----------\n",
      "train Loss: 0.2138 Acc: 0.9275\n",
      "val Loss: 0.1634 Acc: 0.9489\n",
      "\n",
      "Epoch 9/299\n",
      "----------\n",
      "train Loss: 0.2103 Acc: 0.9273\n",
      "val Loss: 0.1616 Acc: 0.9535\n",
      "\n",
      "Epoch 10/299\n",
      "----------\n",
      "train Loss: 0.2051 Acc: 0.9297\n",
      "val Loss: 0.1548 Acc: 0.9506\n",
      "\n",
      "Epoch 11/299\n",
      "----------\n",
      "train Loss: 0.2059 Acc: 0.9320\n",
      "val Loss: 0.1559 Acc: 0.9489\n",
      "\n",
      "Epoch 12/299\n",
      "----------\n",
      "train Loss: 0.1959 Acc: 0.9310\n",
      "val Loss: 0.1540 Acc: 0.9543\n",
      "\n",
      "Epoch 13/299\n",
      "----------\n",
      "train Loss: 0.1906 Acc: 0.9355\n",
      "val Loss: 0.1581 Acc: 0.9514\n",
      "\n",
      "Epoch 14/299\n",
      "----------\n",
      "train Loss: 0.1948 Acc: 0.9300\n",
      "val Loss: 0.1423 Acc: 0.9543\n",
      "\n",
      "Epoch 15/299\n",
      "----------\n",
      "train Loss: 0.1910 Acc: 0.9333\n",
      "val Loss: 0.1431 Acc: 0.9523\n",
      "\n",
      "Epoch 16/299\n",
      "----------\n",
      "train Loss: 0.1896 Acc: 0.9325\n",
      "val Loss: 0.1378 Acc: 0.9527\n",
      "\n",
      "Epoch 17/299\n",
      "----------\n",
      "train Loss: 0.1934 Acc: 0.9304\n",
      "val Loss: 0.1382 Acc: 0.9531\n",
      "\n",
      "Epoch 18/299\n",
      "----------\n",
      "train Loss: 0.1838 Acc: 0.9358\n",
      "val Loss: 0.1414 Acc: 0.9568\n",
      "\n",
      "Epoch 19/299\n",
      "----------\n",
      "train Loss: 0.1873 Acc: 0.9333\n",
      "val Loss: 0.1349 Acc: 0.9531\n",
      "\n",
      "Epoch 20/299\n",
      "----------\n",
      "train Loss: 0.1811 Acc: 0.9354\n",
      "val Loss: 0.1349 Acc: 0.9527\n",
      "\n",
      "Epoch 21/299\n",
      "----------\n",
      "train Loss: 0.1799 Acc: 0.9355\n",
      "val Loss: 0.1294 Acc: 0.9568\n",
      "\n",
      "Epoch 22/299\n",
      "----------\n",
      "train Loss: 0.1831 Acc: 0.9349\n",
      "val Loss: 0.1324 Acc: 0.9539\n",
      "\n",
      "Epoch 23/299\n",
      "----------\n",
      "train Loss: 0.1833 Acc: 0.9353\n",
      "val Loss: 0.1377 Acc: 0.9552\n",
      "\n",
      "Epoch 24/299\n",
      "----------\n",
      "train Loss: 0.1815 Acc: 0.9352\n",
      "val Loss: 0.1289 Acc: 0.9577\n",
      "\n",
      "Epoch 25/299\n",
      "----------\n",
      "train Loss: 0.1783 Acc: 0.9350\n",
      "val Loss: 0.1277 Acc: 0.9593\n",
      "\n",
      "Epoch 26/299\n",
      "----------\n",
      "train Loss: 0.1739 Acc: 0.9368\n",
      "val Loss: 0.1230 Acc: 0.9572\n",
      "\n",
      "Epoch 27/299\n",
      "----------\n",
      "train Loss: 0.1779 Acc: 0.9387\n",
      "val Loss: 0.1225 Acc: 0.9606\n",
      "\n",
      "Epoch 28/299\n",
      "----------\n",
      "train Loss: 0.1723 Acc: 0.9376\n",
      "val Loss: 0.1311 Acc: 0.9564\n",
      "\n",
      "Epoch 29/299\n",
      "----------\n",
      "train Loss: 0.1775 Acc: 0.9356\n",
      "val Loss: 0.1244 Acc: 0.9593\n",
      "\n",
      "Epoch 30/299\n",
      "----------\n",
      "train Loss: 0.1740 Acc: 0.9424\n",
      "val Loss: 0.1229 Acc: 0.9581\n",
      "\n",
      "Epoch 31/299\n",
      "----------\n",
      "train Loss: 0.1748 Acc: 0.9371\n",
      "val Loss: 0.1263 Acc: 0.9589\n",
      "\n",
      "Epoch 32/299\n",
      "----------\n",
      "train Loss: 0.1773 Acc: 0.9366\n",
      "val Loss: 0.1232 Acc: 0.9556\n",
      "\n",
      "Epoch 33/299\n",
      "----------\n",
      "train Loss: 0.1699 Acc: 0.9388\n",
      "val Loss: 0.1244 Acc: 0.9606\n",
      "\n",
      "Epoch 34/299\n",
      "----------\n",
      "train Loss: 0.1687 Acc: 0.9393\n",
      "val Loss: 0.1205 Acc: 0.9581\n",
      "\n",
      "Epoch 35/299\n",
      "----------\n",
      "train Loss: 0.1707 Acc: 0.9373\n",
      "val Loss: 0.1201 Acc: 0.9614\n",
      "\n",
      "Epoch 36/299\n",
      "----------\n",
      "train Loss: 0.1729 Acc: 0.9377\n",
      "val Loss: 0.1164 Acc: 0.9585\n",
      "\n",
      "Epoch 37/299\n",
      "----------\n",
      "train Loss: 0.1685 Acc: 0.9366\n",
      "val Loss: 0.1170 Acc: 0.9626\n",
      "\n",
      "Epoch 38/299\n",
      "----------\n",
      "train Loss: 0.1644 Acc: 0.9413\n",
      "val Loss: 0.1244 Acc: 0.9572\n",
      "\n",
      "Epoch 39/299\n",
      "----------\n",
      "train Loss: 0.1718 Acc: 0.9379\n",
      "val Loss: 0.1223 Acc: 0.9572\n",
      "\n",
      "Epoch 40/299\n",
      "----------\n",
      "train Loss: 0.1652 Acc: 0.9398\n",
      "val Loss: 0.1175 Acc: 0.9597\n",
      "\n",
      "Epoch 41/299\n",
      "----------\n",
      "train Loss: 0.1679 Acc: 0.9387\n",
      "val Loss: 0.1136 Acc: 0.9589\n",
      "\n",
      "Epoch 42/299\n",
      "----------\n",
      "train Loss: 0.1681 Acc: 0.9398\n",
      "val Loss: 0.1193 Acc: 0.9572\n",
      "\n",
      "Epoch 43/299\n",
      "----------\n",
      "train Loss: 0.1657 Acc: 0.9402\n",
      "val Loss: 0.1171 Acc: 0.9606\n",
      "\n",
      "Epoch 44/299\n",
      "----------\n",
      "train Loss: 0.1679 Acc: 0.9376\n",
      "val Loss: 0.1192 Acc: 0.9614\n",
      "\n",
      "Epoch 45/299\n",
      "----------\n",
      "train Loss: 0.1677 Acc: 0.9371\n",
      "val Loss: 0.1152 Acc: 0.9606\n",
      "\n",
      "Epoch 46/299\n",
      "----------\n",
      "train Loss: 0.1685 Acc: 0.9373\n",
      "val Loss: 0.1138 Acc: 0.9589\n",
      "\n",
      "Epoch 47/299\n",
      "----------\n",
      "train Loss: 0.1662 Acc: 0.9399\n",
      "val Loss: 0.1146 Acc: 0.9597\n",
      "\n",
      "Epoch 48/299\n",
      "----------\n",
      "train Loss: 0.1698 Acc: 0.9376\n",
      "val Loss: 0.1187 Acc: 0.9597\n",
      "\n",
      "Epoch 49/299\n",
      "----------\n",
      "train Loss: 0.1640 Acc: 0.9402\n",
      "val Loss: 0.1106 Acc: 0.9610\n",
      "\n",
      "Epoch 50/299\n",
      "----------\n",
      "train Loss: 0.1643 Acc: 0.9386\n",
      "val Loss: 0.1104 Acc: 0.9635\n",
      "\n",
      "Epoch 51/299\n",
      "----------\n",
      "train Loss: 0.1652 Acc: 0.9396\n",
      "val Loss: 0.1140 Acc: 0.9597\n",
      "\n",
      "Epoch 52/299\n",
      "----------\n",
      "train Loss: 0.1653 Acc: 0.9415\n",
      "val Loss: 0.1120 Acc: 0.9606\n",
      "\n",
      "Epoch 53/299\n",
      "----------\n",
      "train Loss: 0.1603 Acc: 0.9431\n",
      "val Loss: 0.1142 Acc: 0.9631\n",
      "\n",
      "Epoch 54/299\n",
      "----------\n",
      "train Loss: 0.1584 Acc: 0.9436\n",
      "val Loss: 0.1220 Acc: 0.9572\n",
      "\n",
      "Epoch 55/299\n",
      "----------\n",
      "train Loss: 0.1592 Acc: 0.9413\n",
      "val Loss: 0.1131 Acc: 0.9631\n",
      "\n",
      "Epoch 56/299\n",
      "----------\n",
      "train Loss: 0.1579 Acc: 0.9433\n",
      "val Loss: 0.1151 Acc: 0.9585\n",
      "\n",
      "Epoch 57/299\n",
      "----------\n",
      "train Loss: 0.1588 Acc: 0.9422\n",
      "val Loss: 0.1085 Acc: 0.9626\n",
      "\n",
      "Epoch 58/299\n",
      "----------\n",
      "train Loss: 0.1644 Acc: 0.9394\n",
      "val Loss: 0.1134 Acc: 0.9606\n",
      "\n",
      "Epoch 59/299\n",
      "----------\n",
      "train Loss: 0.1630 Acc: 0.9400\n",
      "val Loss: 0.1108 Acc: 0.9622\n",
      "\n",
      "Epoch 60/299\n",
      "----------\n",
      "train Loss: 0.1600 Acc: 0.9412\n",
      "val Loss: 0.1135 Acc: 0.9635\n",
      "\n",
      "Epoch 61/299\n",
      "----------\n",
      "train Loss: 0.1579 Acc: 0.9418\n",
      "val Loss: 0.1122 Acc: 0.9631\n",
      "\n",
      "Epoch 62/299\n",
      "----------\n",
      "train Loss: 0.1612 Acc: 0.9404\n",
      "val Loss: 0.1065 Acc: 0.9606\n",
      "\n",
      "Epoch 63/299\n",
      "----------\n",
      "train Loss: 0.1576 Acc: 0.9425\n",
      "val Loss: 0.1083 Acc: 0.9643\n",
      "\n",
      "Epoch 64/299\n",
      "----------\n",
      "train Loss: 0.1605 Acc: 0.9408\n",
      "val Loss: 0.1057 Acc: 0.9626\n",
      "\n",
      "Epoch 65/299\n",
      "----------\n",
      "train Loss: 0.1575 Acc: 0.9425\n",
      "val Loss: 0.1082 Acc: 0.9622\n",
      "\n",
      "Epoch 66/299\n",
      "----------\n",
      "train Loss: 0.1641 Acc: 0.9403\n",
      "val Loss: 0.1091 Acc: 0.9639\n",
      "\n",
      "Epoch 67/299\n",
      "----------\n",
      "train Loss: 0.1621 Acc: 0.9404\n",
      "val Loss: 0.1081 Acc: 0.9618\n",
      "\n",
      "Epoch 68/299\n",
      "----------\n",
      "train Loss: 0.1636 Acc: 0.9383\n",
      "val Loss: 0.1081 Acc: 0.9610\n",
      "\n",
      "Epoch 69/299\n",
      "----------\n",
      "train Loss: 0.1557 Acc: 0.9415\n",
      "val Loss: 0.1082 Acc: 0.9610\n",
      "\n",
      "Epoch 70/299\n",
      "----------\n",
      "train Loss: 0.1634 Acc: 0.9414\n",
      "val Loss: 0.1103 Acc: 0.9643\n",
      "\n",
      "Epoch 71/299\n",
      "----------\n",
      "train Loss: 0.1603 Acc: 0.9418\n",
      "val Loss: 0.1113 Acc: 0.9626\n",
      "\n",
      "Epoch 72/299\n",
      "----------\n",
      "train Loss: 0.1592 Acc: 0.9424\n",
      "val Loss: 0.1044 Acc: 0.9622\n",
      "\n",
      "Epoch 73/299\n",
      "----------\n",
      "train Loss: 0.1577 Acc: 0.9408\n",
      "val Loss: 0.1056 Acc: 0.9614\n",
      "\n",
      "Epoch 74/299\n",
      "----------\n",
      "train Loss: 0.1539 Acc: 0.9448\n",
      "val Loss: 0.1054 Acc: 0.9618\n",
      "\n",
      "Epoch 75/299\n",
      "----------\n",
      "train Loss: 0.1549 Acc: 0.9442\n",
      "val Loss: 0.1064 Acc: 0.9631\n",
      "\n",
      "Epoch 76/299\n",
      "----------\n",
      "train Loss: 0.1559 Acc: 0.9446\n",
      "val Loss: 0.1048 Acc: 0.9651\n",
      "\n",
      "Epoch 77/299\n",
      "----------\n",
      "train Loss: 0.1523 Acc: 0.9444\n",
      "val Loss: 0.1066 Acc: 0.9639\n",
      "\n",
      "Epoch 78/299\n",
      "----------\n",
      "train Loss: 0.1579 Acc: 0.9422\n",
      "val Loss: 0.1046 Acc: 0.9622\n",
      "\n",
      "Epoch 79/299\n",
      "----------\n",
      "train Loss: 0.1615 Acc: 0.9409\n",
      "val Loss: 0.1048 Acc: 0.9664\n",
      "\n",
      "Epoch 80/299\n",
      "----------\n",
      "train Loss: 0.1590 Acc: 0.9415\n",
      "val Loss: 0.1113 Acc: 0.9614\n",
      "\n",
      "Epoch 81/299\n",
      "----------\n",
      "train Loss: 0.1661 Acc: 0.9374\n",
      "val Loss: 0.1090 Acc: 0.9639\n",
      "\n",
      "Epoch 82/299\n",
      "----------\n",
      "train Loss: 0.1551 Acc: 0.9444\n",
      "val Loss: 0.1044 Acc: 0.9651\n",
      "\n",
      "Epoch 83/299\n",
      "----------\n",
      "train Loss: 0.1611 Acc: 0.9407\n",
      "val Loss: 0.1051 Acc: 0.9668\n",
      "\n",
      "Epoch 84/299\n",
      "----------\n",
      "train Loss: 0.1629 Acc: 0.9404\n",
      "val Loss: 0.1063 Acc: 0.9622\n",
      "\n",
      "Epoch 85/299\n",
      "----------\n",
      "train Loss: 0.1546 Acc: 0.9433\n",
      "val Loss: 0.1119 Acc: 0.9635\n",
      "\n",
      "Epoch 86/299\n",
      "----------\n",
      "train Loss: 0.1567 Acc: 0.9414\n",
      "val Loss: 0.1032 Acc: 0.9651\n",
      "\n",
      "Epoch 87/299\n",
      "----------\n",
      "train Loss: 0.1532 Acc: 0.9413\n",
      "val Loss: 0.1030 Acc: 0.9643\n",
      "\n",
      "Epoch 88/299\n",
      "----------\n",
      "train Loss: 0.1493 Acc: 0.9460\n",
      "val Loss: 0.1010 Acc: 0.9643\n",
      "\n",
      "Epoch 89/299\n",
      "----------\n",
      "train Loss: 0.1536 Acc: 0.9427\n",
      "val Loss: 0.1058 Acc: 0.9631\n",
      "\n",
      "Epoch 90/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1557 Acc: 0.9433\n",
      "val Loss: 0.1043 Acc: 0.9622\n",
      "\n",
      "Epoch 91/299\n",
      "----------\n",
      "train Loss: 0.1567 Acc: 0.9410\n",
      "val Loss: 0.1025 Acc: 0.9643\n",
      "\n",
      "Epoch 92/299\n",
      "----------\n",
      "train Loss: 0.1528 Acc: 0.9439\n",
      "val Loss: 0.1046 Acc: 0.9635\n",
      "\n",
      "Epoch 93/299\n",
      "----------\n",
      "train Loss: 0.1525 Acc: 0.9445\n",
      "val Loss: 0.0998 Acc: 0.9660\n",
      "\n",
      "Epoch 94/299\n",
      "----------\n",
      "train Loss: 0.1521 Acc: 0.9456\n",
      "val Loss: 0.1033 Acc: 0.9647\n",
      "\n",
      "Epoch 95/299\n",
      "----------\n",
      "train Loss: 0.1547 Acc: 0.9419\n",
      "val Loss: 0.0970 Acc: 0.9660\n",
      "\n",
      "Epoch 96/299\n",
      "----------\n",
      "train Loss: 0.1563 Acc: 0.9425\n",
      "val Loss: 0.1013 Acc: 0.9647\n",
      "\n",
      "Epoch 97/299\n",
      "----------\n",
      "train Loss: 0.1563 Acc: 0.9411\n",
      "val Loss: 0.1044 Acc: 0.9643\n",
      "\n",
      "Epoch 98/299\n",
      "----------\n",
      "train Loss: 0.1549 Acc: 0.9435\n",
      "val Loss: 0.1057 Acc: 0.9672\n",
      "\n",
      "Epoch 99/299\n",
      "----------\n",
      "train Loss: 0.1542 Acc: 0.9430\n",
      "val Loss: 0.0987 Acc: 0.9664\n",
      "\n",
      "Epoch 100/299\n",
      "----------\n",
      "train Loss: 0.1556 Acc: 0.9451\n",
      "val Loss: 0.1050 Acc: 0.9631\n",
      "\n",
      "Epoch 101/299\n",
      "----------\n",
      "train Loss: 0.1569 Acc: 0.9400\n",
      "val Loss: 0.1090 Acc: 0.9601\n",
      "\n",
      "Epoch 102/299\n",
      "----------\n",
      "train Loss: 0.1507 Acc: 0.9452\n",
      "val Loss: 0.1035 Acc: 0.9635\n",
      "\n",
      "Epoch 103/299\n",
      "----------\n",
      "train Loss: 0.1518 Acc: 0.9441\n",
      "val Loss: 0.0972 Acc: 0.9668\n",
      "\n",
      "Epoch 104/299\n",
      "----------\n",
      "train Loss: 0.1477 Acc: 0.9472\n",
      "val Loss: 0.0972 Acc: 0.9672\n",
      "\n",
      "Epoch 105/299\n",
      "----------\n",
      "train Loss: 0.1579 Acc: 0.9374\n",
      "val Loss: 0.0978 Acc: 0.9668\n",
      "\n",
      "Epoch 106/299\n",
      "----------\n",
      "train Loss: 0.1544 Acc: 0.9438\n",
      "val Loss: 0.1016 Acc: 0.9651\n",
      "\n",
      "Epoch 107/299\n",
      "----------\n",
      "train Loss: 0.1532 Acc: 0.9442\n",
      "val Loss: 0.1049 Acc: 0.9651\n",
      "\n",
      "Epoch 108/299\n",
      "----------\n",
      "train Loss: 0.1535 Acc: 0.9422\n",
      "val Loss: 0.1034 Acc: 0.9664\n",
      "\n",
      "Epoch 109/299\n",
      "----------\n",
      "train Loss: 0.1516 Acc: 0.9441\n",
      "val Loss: 0.0981 Acc: 0.9680\n",
      "\n",
      "Epoch 110/299\n",
      "----------\n",
      "train Loss: 0.1485 Acc: 0.9459\n",
      "val Loss: 0.1048 Acc: 0.9647\n",
      "\n",
      "Epoch 111/299\n",
      "----------\n",
      "train Loss: 0.1467 Acc: 0.9451\n",
      "val Loss: 0.0978 Acc: 0.9655\n",
      "\n",
      "Epoch 112/299\n",
      "----------\n",
      "train Loss: 0.1560 Acc: 0.9460\n",
      "val Loss: 0.1034 Acc: 0.9660\n",
      "\n",
      "Epoch 113/299\n",
      "----------\n",
      "train Loss: 0.1497 Acc: 0.9446\n",
      "val Loss: 0.1015 Acc: 0.9680\n",
      "\n",
      "Epoch 114/299\n",
      "----------\n",
      "train Loss: 0.1513 Acc: 0.9446\n",
      "val Loss: 0.1000 Acc: 0.9668\n",
      "\n",
      "Epoch 115/299\n",
      "----------\n",
      "train Loss: 0.1525 Acc: 0.9454\n",
      "val Loss: 0.1044 Acc: 0.9660\n",
      "\n",
      "Epoch 116/299\n",
      "----------\n",
      "train Loss: 0.1549 Acc: 0.9388\n",
      "val Loss: 0.1040 Acc: 0.9635\n",
      "\n",
      "Epoch 117/299\n",
      "----------\n",
      "train Loss: 0.1538 Acc: 0.9430\n",
      "val Loss: 0.1011 Acc: 0.9676\n",
      "\n",
      "Epoch 118/299\n",
      "----------\n",
      "train Loss: 0.1499 Acc: 0.9432\n",
      "val Loss: 0.0971 Acc: 0.9664\n",
      "\n",
      "Epoch 119/299\n",
      "----------\n",
      "train Loss: 0.1533 Acc: 0.9448\n",
      "val Loss: 0.0967 Acc: 0.9680\n",
      "\n",
      "Epoch 120/299\n",
      "----------\n",
      "train Loss: 0.1524 Acc: 0.9420\n",
      "val Loss: 0.0984 Acc: 0.9680\n",
      "\n",
      "Epoch 121/299\n",
      "----------\n",
      "train Loss: 0.1451 Acc: 0.9473\n",
      "val Loss: 0.0985 Acc: 0.9660\n",
      "\n",
      "Epoch 122/299\n",
      "----------\n",
      "train Loss: 0.1562 Acc: 0.9426\n",
      "val Loss: 0.0959 Acc: 0.9664\n",
      "\n",
      "Epoch 123/299\n",
      "----------\n",
      "train Loss: 0.1492 Acc: 0.9456\n",
      "val Loss: 0.0983 Acc: 0.9639\n",
      "\n",
      "Epoch 124/299\n",
      "----------\n",
      "train Loss: 0.1469 Acc: 0.9460\n",
      "val Loss: 0.0972 Acc: 0.9664\n",
      "\n",
      "Epoch 125/299\n",
      "----------\n",
      "train Loss: 0.1450 Acc: 0.9442\n",
      "val Loss: 0.1028 Acc: 0.9651\n",
      "\n",
      "Epoch 126/299\n",
      "----------\n",
      "train Loss: 0.1518 Acc: 0.9430\n",
      "val Loss: 0.0993 Acc: 0.9664\n",
      "\n",
      "Epoch 127/299\n",
      "----------\n",
      "train Loss: 0.1435 Acc: 0.9486\n",
      "val Loss: 0.0997 Acc: 0.9668\n",
      "\n",
      "Epoch 128/299\n",
      "----------\n",
      "train Loss: 0.1456 Acc: 0.9477\n",
      "val Loss: 0.0969 Acc: 0.9676\n",
      "\n",
      "Epoch 129/299\n",
      "----------\n",
      "train Loss: 0.1425 Acc: 0.9467\n",
      "val Loss: 0.1027 Acc: 0.9668\n",
      "\n",
      "Epoch 130/299\n",
      "----------\n",
      "train Loss: 0.1479 Acc: 0.9460\n",
      "val Loss: 0.1026 Acc: 0.9635\n",
      "\n",
      "Epoch 131/299\n",
      "----------\n",
      "train Loss: 0.1470 Acc: 0.9438\n",
      "val Loss: 0.0975 Acc: 0.9664\n",
      "\n",
      "Epoch 132/299\n",
      "----------\n",
      "train Loss: 0.1503 Acc: 0.9459\n",
      "val Loss: 0.1019 Acc: 0.9664\n",
      "\n",
      "Epoch 133/299\n",
      "----------\n",
      "train Loss: 0.1516 Acc: 0.9439\n",
      "val Loss: 0.1005 Acc: 0.9664\n",
      "\n",
      "Epoch 134/299\n",
      "----------\n",
      "train Loss: 0.1491 Acc: 0.9452\n",
      "val Loss: 0.1063 Acc: 0.9639\n",
      "\n",
      "Epoch 135/299\n",
      "----------\n",
      "train Loss: 0.1504 Acc: 0.9435\n",
      "val Loss: 0.0977 Acc: 0.9676\n",
      "\n",
      "Epoch 136/299\n",
      "----------\n",
      "train Loss: 0.1492 Acc: 0.9437\n",
      "val Loss: 0.0960 Acc: 0.9664\n",
      "\n",
      "Epoch 137/299\n",
      "----------\n",
      "train Loss: 0.1518 Acc: 0.9426\n",
      "val Loss: 0.0972 Acc: 0.9647\n",
      "\n",
      "Epoch 138/299\n",
      "----------\n",
      "train Loss: 0.1451 Acc: 0.9466\n",
      "val Loss: 0.1065 Acc: 0.9655\n",
      "\n",
      "Epoch 139/299\n",
      "----------\n",
      "train Loss: 0.1458 Acc: 0.9450\n",
      "val Loss: 0.0971 Acc: 0.9668\n",
      "\n",
      "Epoch 140/299\n",
      "----------\n",
      "train Loss: 0.1441 Acc: 0.9473\n",
      "val Loss: 0.0948 Acc: 0.9668\n",
      "\n",
      "Epoch 141/299\n",
      "----------\n",
      "train Loss: 0.1451 Acc: 0.9471\n",
      "val Loss: 0.0969 Acc: 0.9664\n",
      "\n",
      "Epoch 142/299\n",
      "----------\n",
      "train Loss: 0.1463 Acc: 0.9444\n",
      "val Loss: 0.0977 Acc: 0.9664\n",
      "\n",
      "Epoch 143/299\n",
      "----------\n",
      "train Loss: 0.1479 Acc: 0.9452\n",
      "val Loss: 0.0983 Acc: 0.9655\n",
      "\n",
      "Epoch 144/299\n",
      "----------\n",
      "train Loss: 0.1429 Acc: 0.9467\n",
      "val Loss: 0.0975 Acc: 0.9685\n",
      "\n",
      "Epoch 145/299\n",
      "----------\n",
      "train Loss: 0.1475 Acc: 0.9429\n",
      "val Loss: 0.0976 Acc: 0.9672\n",
      "\n",
      "Epoch 146/299\n",
      "----------\n",
      "train Loss: 0.1485 Acc: 0.9427\n",
      "val Loss: 0.1012 Acc: 0.9651\n",
      "\n",
      "Epoch 147/299\n",
      "----------\n",
      "train Loss: 0.1434 Acc: 0.9483\n",
      "val Loss: 0.0972 Acc: 0.9651\n",
      "\n",
      "Epoch 148/299\n",
      "----------\n",
      "train Loss: 0.1476 Acc: 0.9440\n",
      "val Loss: 0.0984 Acc: 0.9655\n",
      "\n",
      "Epoch 149/299\n",
      "----------\n",
      "train Loss: 0.1450 Acc: 0.9477\n",
      "val Loss: 0.0950 Acc: 0.9672\n",
      "\n",
      "Epoch 150/299\n",
      "----------\n",
      "train Loss: 0.1425 Acc: 0.9473\n",
      "val Loss: 0.0975 Acc: 0.9639\n",
      "\n",
      "Epoch 151/299\n",
      "----------\n",
      "train Loss: 0.1481 Acc: 0.9451\n",
      "val Loss: 0.1012 Acc: 0.9664\n",
      "\n",
      "Epoch 152/299\n",
      "----------\n",
      "train Loss: 0.1480 Acc: 0.9447\n",
      "val Loss: 0.0965 Acc: 0.9668\n",
      "\n",
      "Epoch 153/299\n",
      "----------\n",
      "train Loss: 0.1439 Acc: 0.9476\n",
      "val Loss: 0.0997 Acc: 0.9664\n",
      "\n",
      "Epoch 154/299\n",
      "----------\n",
      "train Loss: 0.1505 Acc: 0.9434\n",
      "val Loss: 0.0973 Acc: 0.9651\n",
      "\n",
      "Epoch 155/299\n",
      "----------\n",
      "train Loss: 0.1429 Acc: 0.9452\n",
      "val Loss: 0.0949 Acc: 0.9668\n",
      "\n",
      "Epoch 156/299\n",
      "----------\n",
      "train Loss: 0.1416 Acc: 0.9472\n",
      "val Loss: 0.0945 Acc: 0.9676\n",
      "\n",
      "Epoch 157/299\n",
      "----------\n",
      "train Loss: 0.1418 Acc: 0.9466\n",
      "val Loss: 0.0921 Acc: 0.9685\n",
      "\n",
      "Epoch 158/299\n",
      "----------\n",
      "train Loss: 0.1472 Acc: 0.9471\n",
      "val Loss: 0.0959 Acc: 0.9676\n",
      "\n",
      "Epoch 159/299\n",
      "----------\n",
      "train Loss: 0.1459 Acc: 0.9474\n",
      "val Loss: 0.1012 Acc: 0.9660\n",
      "\n",
      "Epoch 160/299\n",
      "----------\n",
      "train Loss: 0.1500 Acc: 0.9451\n",
      "val Loss: 0.0988 Acc: 0.9664\n",
      "\n",
      "Epoch 161/299\n",
      "----------\n",
      "train Loss: 0.1469 Acc: 0.9435\n",
      "val Loss: 0.0998 Acc: 0.9668\n",
      "\n",
      "Epoch 162/299\n",
      "----------\n",
      "train Loss: 0.1483 Acc: 0.9444\n",
      "val Loss: 0.1017 Acc: 0.9635\n",
      "\n",
      "Epoch 163/299\n",
      "----------\n",
      "train Loss: 0.1510 Acc: 0.9465\n",
      "val Loss: 0.0955 Acc: 0.9664\n",
      "\n",
      "Epoch 164/299\n",
      "----------\n",
      "train Loss: 0.1451 Acc: 0.9455\n",
      "val Loss: 0.0939 Acc: 0.9680\n",
      "\n",
      "Epoch 165/299\n",
      "----------\n",
      "train Loss: 0.1455 Acc: 0.9457\n",
      "val Loss: 0.0986 Acc: 0.9655\n",
      "\n",
      "Epoch 166/299\n",
      "----------\n",
      "train Loss: 0.1508 Acc: 0.9450\n",
      "val Loss: 0.0949 Acc: 0.9672\n",
      "\n",
      "Epoch 167/299\n",
      "----------\n",
      "train Loss: 0.1489 Acc: 0.9454\n",
      "val Loss: 0.0957 Acc: 0.9672\n",
      "\n",
      "Epoch 168/299\n",
      "----------\n",
      "train Loss: 0.1444 Acc: 0.9451\n",
      "val Loss: 0.0939 Acc: 0.9672\n",
      "\n",
      "Epoch 169/299\n",
      "----------\n",
      "train Loss: 0.1470 Acc: 0.9415\n",
      "val Loss: 0.0988 Acc: 0.9655\n",
      "\n",
      "Epoch 170/299\n",
      "----------\n",
      "train Loss: 0.1465 Acc: 0.9442\n",
      "val Loss: 0.0941 Acc: 0.9689\n",
      "\n",
      "Epoch 171/299\n",
      "----------\n",
      "train Loss: 0.1442 Acc: 0.9460\n",
      "val Loss: 0.0939 Acc: 0.9676\n",
      "\n",
      "Epoch 172/299\n",
      "----------\n",
      "train Loss: 0.1472 Acc: 0.9468\n",
      "val Loss: 0.0968 Acc: 0.9668\n",
      "\n",
      "Epoch 173/299\n",
      "----------\n",
      "train Loss: 0.1457 Acc: 0.9455\n",
      "val Loss: 0.0919 Acc: 0.9685\n",
      "\n",
      "Epoch 174/299\n",
      "----------\n",
      "train Loss: 0.1516 Acc: 0.9438\n",
      "val Loss: 0.0941 Acc: 0.9672\n",
      "\n",
      "Epoch 175/299\n",
      "----------\n",
      "train Loss: 0.1483 Acc: 0.9454\n",
      "val Loss: 0.0943 Acc: 0.9672\n",
      "\n",
      "Epoch 176/299\n",
      "----------\n",
      "train Loss: 0.1443 Acc: 0.9464\n",
      "val Loss: 0.0970 Acc: 0.9676\n",
      "\n",
      "Epoch 177/299\n",
      "----------\n",
      "train Loss: 0.1373 Acc: 0.9475\n",
      "val Loss: 0.0932 Acc: 0.9676\n",
      "\n",
      "Epoch 178/299\n",
      "----------\n",
      "train Loss: 0.1422 Acc: 0.9474\n",
      "val Loss: 0.0942 Acc: 0.9685\n",
      "\n",
      "Epoch 179/299\n",
      "----------\n",
      "train Loss: 0.1483 Acc: 0.9466\n",
      "val Loss: 0.0979 Acc: 0.9651\n",
      "\n",
      "Epoch 180/299\n",
      "----------\n",
      "train Loss: 0.1500 Acc: 0.9447\n",
      "val Loss: 0.0989 Acc: 0.9672\n",
      "\n",
      "Epoch 181/299\n",
      "----------\n",
      "train Loss: 0.1448 Acc: 0.9447\n",
      "val Loss: 0.0953 Acc: 0.9680\n",
      "\n",
      "Epoch 182/299\n",
      "----------\n",
      "train Loss: 0.1431 Acc: 0.9487\n",
      "val Loss: 0.0964 Acc: 0.9664\n",
      "\n",
      "Epoch 183/299\n",
      "----------\n",
      "train Loss: 0.1482 Acc: 0.9429\n",
      "val Loss: 0.0916 Acc: 0.9693\n",
      "\n",
      "Epoch 184/299\n",
      "----------\n",
      "train Loss: 0.1433 Acc: 0.9482\n",
      "val Loss: 0.0957 Acc: 0.9680\n",
      "\n",
      "Epoch 185/299\n",
      "----------\n",
      "train Loss: 0.1488 Acc: 0.9431\n",
      "val Loss: 0.0917 Acc: 0.9672\n",
      "\n",
      "Epoch 186/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1431 Acc: 0.9461\n",
      "val Loss: 0.0928 Acc: 0.9668\n",
      "\n",
      "Epoch 187/299\n",
      "----------\n",
      "train Loss: 0.1447 Acc: 0.9459\n",
      "val Loss: 0.0938 Acc: 0.9689\n",
      "\n",
      "Epoch 188/299\n",
      "----------\n",
      "train Loss: 0.1465 Acc: 0.9460\n",
      "val Loss: 0.0969 Acc: 0.9672\n",
      "\n",
      "Epoch 189/299\n",
      "----------\n",
      "train Loss: 0.1443 Acc: 0.9466\n",
      "val Loss: 0.0905 Acc: 0.9689\n",
      "\n",
      "Epoch 190/299\n",
      "----------\n",
      "train Loss: 0.1401 Acc: 0.9490\n",
      "val Loss: 0.0988 Acc: 0.9643\n",
      "\n",
      "Epoch 191/299\n",
      "----------\n",
      "train Loss: 0.1479 Acc: 0.9465\n",
      "val Loss: 0.0951 Acc: 0.9680\n",
      "\n",
      "Epoch 192/299\n",
      "----------\n",
      "train Loss: 0.1421 Acc: 0.9465\n",
      "val Loss: 0.0985 Acc: 0.9647\n",
      "\n",
      "Epoch 193/299\n",
      "----------\n",
      "train Loss: 0.1442 Acc: 0.9466\n",
      "val Loss: 0.0907 Acc: 0.9697\n",
      "\n",
      "Epoch 194/299\n",
      "----------\n",
      "train Loss: 0.1457 Acc: 0.9468\n",
      "val Loss: 0.0943 Acc: 0.9701\n",
      "\n",
      "Epoch 195/299\n",
      "----------\n",
      "train Loss: 0.1437 Acc: 0.9481\n",
      "val Loss: 0.0905 Acc: 0.9685\n",
      "\n",
      "Epoch 196/299\n",
      "----------\n",
      "train Loss: 0.1420 Acc: 0.9472\n",
      "val Loss: 0.0957 Acc: 0.9668\n",
      "\n",
      "Epoch 197/299\n",
      "----------\n",
      "train Loss: 0.1435 Acc: 0.9454\n",
      "val Loss: 0.0921 Acc: 0.9676\n",
      "\n",
      "Epoch 198/299\n",
      "----------\n",
      "train Loss: 0.1460 Acc: 0.9444\n",
      "val Loss: 0.0931 Acc: 0.9680\n",
      "\n",
      "Epoch 199/299\n",
      "----------\n",
      "train Loss: 0.1424 Acc: 0.9461\n",
      "val Loss: 0.0966 Acc: 0.9676\n",
      "\n",
      "Epoch 200/299\n",
      "----------\n",
      "train Loss: 0.1477 Acc: 0.9438\n",
      "val Loss: 0.0958 Acc: 0.9672\n",
      "\n",
      "Epoch 201/299\n",
      "----------\n",
      "train Loss: 0.1398 Acc: 0.9473\n",
      "val Loss: 0.0915 Acc: 0.9672\n",
      "\n",
      "Epoch 202/299\n",
      "----------\n",
      "train Loss: 0.1406 Acc: 0.9478\n",
      "val Loss: 0.0908 Acc: 0.9680\n",
      "\n",
      "Epoch 203/299\n",
      "----------\n",
      "train Loss: 0.1441 Acc: 0.9440\n",
      "val Loss: 0.0952 Acc: 0.9680\n",
      "\n",
      "Epoch 204/299\n",
      "----------\n",
      "train Loss: 0.1435 Acc: 0.9457\n",
      "val Loss: 0.0919 Acc: 0.9689\n",
      "\n",
      "Epoch 205/299\n",
      "----------\n",
      "train Loss: 0.1436 Acc: 0.9423\n",
      "val Loss: 0.0906 Acc: 0.9676\n",
      "\n",
      "Epoch 206/299\n",
      "----------\n",
      "train Loss: 0.1403 Acc: 0.9479\n",
      "val Loss: 0.0893 Acc: 0.9705\n",
      "\n",
      "Epoch 207/299\n",
      "----------\n",
      "train Loss: 0.1505 Acc: 0.9435\n",
      "val Loss: 0.0934 Acc: 0.9676\n",
      "\n",
      "Epoch 208/299\n",
      "----------\n",
      "train Loss: 0.1438 Acc: 0.9438\n",
      "val Loss: 0.0919 Acc: 0.9697\n",
      "\n",
      "Epoch 209/299\n",
      "----------\n",
      "train Loss: 0.1420 Acc: 0.9477\n",
      "val Loss: 0.0951 Acc: 0.9701\n",
      "\n",
      "Epoch 210/299\n",
      "----------\n",
      "train Loss: 0.1452 Acc: 0.9445\n",
      "val Loss: 0.0911 Acc: 0.9701\n",
      "\n",
      "Epoch 211/299\n",
      "----------\n",
      "train Loss: 0.1497 Acc: 0.9421\n",
      "val Loss: 0.0919 Acc: 0.9672\n",
      "\n",
      "Epoch 212/299\n",
      "----------\n",
      "train Loss: 0.1439 Acc: 0.9469\n",
      "val Loss: 0.0904 Acc: 0.9701\n",
      "\n",
      "Epoch 213/299\n",
      "----------\n",
      "train Loss: 0.1459 Acc: 0.9454\n",
      "val Loss: 0.0977 Acc: 0.9647\n",
      "\n",
      "Epoch 214/299\n",
      "----------\n",
      "train Loss: 0.1519 Acc: 0.9417\n",
      "val Loss: 0.0927 Acc: 0.9697\n",
      "\n",
      "Epoch 215/299\n",
      "----------\n",
      "train Loss: 0.1384 Acc: 0.9498\n",
      "val Loss: 0.0898 Acc: 0.9693\n",
      "\n",
      "Epoch 216/299\n",
      "----------\n",
      "train Loss: 0.1454 Acc: 0.9446\n",
      "val Loss: 0.0938 Acc: 0.9689\n",
      "\n",
      "Epoch 217/299\n",
      "----------\n",
      "train Loss: 0.1447 Acc: 0.9472\n",
      "val Loss: 0.0985 Acc: 0.9660\n",
      "\n",
      "Epoch 218/299\n",
      "----------\n",
      "train Loss: 0.1390 Acc: 0.9472\n",
      "val Loss: 0.0884 Acc: 0.9697\n",
      "\n",
      "Epoch 219/299\n",
      "----------\n",
      "train Loss: 0.1409 Acc: 0.9477\n",
      "val Loss: 0.0930 Acc: 0.9701\n",
      "\n",
      "Epoch 220/299\n",
      "----------\n",
      "train Loss: 0.1438 Acc: 0.9478\n",
      "val Loss: 0.0937 Acc: 0.9697\n",
      "\n",
      "Epoch 221/299\n",
      "----------\n",
      "train Loss: 0.1371 Acc: 0.9465\n",
      "val Loss: 0.0918 Acc: 0.9660\n",
      "\n",
      "Epoch 222/299\n",
      "----------\n",
      "train Loss: 0.1422 Acc: 0.9469\n",
      "val Loss: 0.0894 Acc: 0.9689\n",
      "\n",
      "Epoch 223/299\n",
      "----------\n",
      "train Loss: 0.1485 Acc: 0.9431\n",
      "val Loss: 0.0937 Acc: 0.9676\n",
      "\n",
      "Epoch 224/299\n",
      "----------\n",
      "train Loss: 0.1458 Acc: 0.9448\n",
      "val Loss: 0.0906 Acc: 0.9660\n",
      "\n",
      "Epoch 225/299\n",
      "----------\n",
      "train Loss: 0.1452 Acc: 0.9473\n",
      "val Loss: 0.0901 Acc: 0.9685\n",
      "\n",
      "Epoch 226/299\n",
      "----------\n",
      "train Loss: 0.1493 Acc: 0.9439\n",
      "val Loss: 0.0964 Acc: 0.9672\n",
      "\n",
      "Epoch 227/299\n",
      "----------\n",
      "train Loss: 0.1413 Acc: 0.9475\n",
      "val Loss: 0.0896 Acc: 0.9660\n",
      "\n",
      "Epoch 228/299\n",
      "----------\n",
      "train Loss: 0.1413 Acc: 0.9478\n",
      "val Loss: 0.0954 Acc: 0.9680\n",
      "\n",
      "Epoch 229/299\n",
      "----------\n",
      "train Loss: 0.1459 Acc: 0.9451\n",
      "val Loss: 0.0905 Acc: 0.9689\n",
      "\n",
      "Epoch 230/299\n",
      "----------\n",
      "train Loss: 0.1483 Acc: 0.9437\n",
      "val Loss: 0.0947 Acc: 0.9660\n",
      "\n",
      "Epoch 231/299\n",
      "----------\n",
      "train Loss: 0.1411 Acc: 0.9480\n",
      "val Loss: 0.0954 Acc: 0.9693\n",
      "\n",
      "Epoch 232/299\n",
      "----------\n",
      "train Loss: 0.1452 Acc: 0.9451\n",
      "val Loss: 0.0890 Acc: 0.9689\n",
      "\n",
      "Epoch 233/299\n",
      "----------\n",
      "train Loss: 0.1417 Acc: 0.9482\n",
      "val Loss: 0.0925 Acc: 0.9668\n",
      "\n",
      "Epoch 234/299\n",
      "----------\n",
      "train Loss: 0.1440 Acc: 0.9469\n",
      "val Loss: 0.0995 Acc: 0.9655\n",
      "\n",
      "Epoch 235/299\n",
      "----------\n",
      "train Loss: 0.1403 Acc: 0.9472\n",
      "val Loss: 0.0915 Acc: 0.9701\n",
      "\n",
      "Epoch 236/299\n",
      "----------\n",
      "train Loss: 0.1414 Acc: 0.9482\n",
      "val Loss: 0.0955 Acc: 0.9655\n",
      "\n",
      "Epoch 237/299\n",
      "----------\n",
      "train Loss: 0.1421 Acc: 0.9459\n",
      "val Loss: 0.0894 Acc: 0.9693\n",
      "\n",
      "Epoch 238/299\n",
      "----------\n",
      "train Loss: 0.1442 Acc: 0.9462\n",
      "val Loss: 0.0952 Acc: 0.9672\n",
      "\n",
      "Epoch 239/299\n",
      "----------\n",
      "train Loss: 0.1401 Acc: 0.9465\n",
      "val Loss: 0.0932 Acc: 0.9676\n",
      "\n",
      "Epoch 240/299\n",
      "----------\n",
      "train Loss: 0.1403 Acc: 0.9488\n",
      "val Loss: 0.0948 Acc: 0.9676\n",
      "\n",
      "Epoch 241/299\n",
      "----------\n",
      "train Loss: 0.1389 Acc: 0.9487\n",
      "val Loss: 0.0962 Acc: 0.9664\n",
      "\n",
      "Epoch 242/299\n",
      "----------\n",
      "train Loss: 0.1457 Acc: 0.9445\n",
      "val Loss: 0.0890 Acc: 0.9689\n",
      "\n",
      "Epoch 243/299\n",
      "----------\n",
      "train Loss: 0.1421 Acc: 0.9475\n",
      "val Loss: 0.0928 Acc: 0.9685\n",
      "\n",
      "Epoch 244/299\n",
      "----------\n",
      "train Loss: 0.1430 Acc: 0.9468\n",
      "val Loss: 0.0924 Acc: 0.9685\n",
      "\n",
      "Epoch 245/299\n",
      "----------\n",
      "train Loss: 0.1432 Acc: 0.9472\n",
      "val Loss: 0.0903 Acc: 0.9676\n",
      "\n",
      "Epoch 246/299\n",
      "----------\n",
      "train Loss: 0.1372 Acc: 0.9475\n",
      "val Loss: 0.0954 Acc: 0.9701\n",
      "\n",
      "Epoch 247/299\n",
      "----------\n",
      "train Loss: 0.1402 Acc: 0.9483\n",
      "val Loss: 0.0881 Acc: 0.9693\n",
      "\n",
      "Epoch 248/299\n",
      "----------\n",
      "train Loss: 0.1442 Acc: 0.9452\n",
      "val Loss: 0.0965 Acc: 0.9693\n",
      "\n",
      "Epoch 249/299\n",
      "----------\n",
      "train Loss: 0.1453 Acc: 0.9444\n",
      "val Loss: 0.0952 Acc: 0.9668\n",
      "\n",
      "Epoch 250/299\n",
      "----------\n",
      "train Loss: 0.1514 Acc: 0.9439\n",
      "val Loss: 0.0912 Acc: 0.9693\n",
      "\n",
      "Epoch 251/299\n",
      "----------\n",
      "train Loss: 0.1385 Acc: 0.9483\n",
      "val Loss: 0.0932 Acc: 0.9672\n",
      "\n",
      "Epoch 252/299\n",
      "----------\n",
      "train Loss: 0.1469 Acc: 0.9460\n",
      "val Loss: 0.0921 Acc: 0.9693\n",
      "\n",
      "Epoch 253/299\n",
      "----------\n",
      "train Loss: 0.1472 Acc: 0.9473\n",
      "val Loss: 0.0881 Acc: 0.9693\n",
      "\n",
      "Epoch 254/299\n",
      "----------\n",
      "train Loss: 0.1387 Acc: 0.9481\n",
      "val Loss: 0.0880 Acc: 0.9685\n",
      "\n",
      "Epoch 255/299\n",
      "----------\n",
      "train Loss: 0.1431 Acc: 0.9468\n",
      "val Loss: 0.0988 Acc: 0.9651\n",
      "\n",
      "Epoch 256/299\n",
      "----------\n",
      "train Loss: 0.1421 Acc: 0.9489\n",
      "val Loss: 0.0987 Acc: 0.9647\n",
      "\n",
      "Epoch 257/299\n",
      "----------\n",
      "train Loss: 0.1392 Acc: 0.9496\n",
      "val Loss: 0.0912 Acc: 0.9672\n",
      "\n",
      "Epoch 258/299\n",
      "----------\n",
      "train Loss: 0.1446 Acc: 0.9460\n",
      "val Loss: 0.0896 Acc: 0.9685\n",
      "\n",
      "Epoch 259/299\n",
      "----------\n",
      "train Loss: 0.1412 Acc: 0.9454\n",
      "val Loss: 0.0937 Acc: 0.9668\n",
      "\n",
      "Epoch 260/299\n",
      "----------\n",
      "train Loss: 0.1379 Acc: 0.9507\n",
      "val Loss: 0.0926 Acc: 0.9697\n",
      "\n",
      "Epoch 261/299\n",
      "----------\n",
      "train Loss: 0.1453 Acc: 0.9452\n",
      "val Loss: 0.0938 Acc: 0.9685\n",
      "\n",
      "Epoch 262/299\n",
      "----------\n",
      "train Loss: 0.1420 Acc: 0.9482\n",
      "val Loss: 0.0910 Acc: 0.9685\n",
      "\n",
      "Epoch 263/299\n",
      "----------\n",
      "train Loss: 0.1363 Acc: 0.9496\n",
      "val Loss: 0.0896 Acc: 0.9697\n",
      "\n",
      "Epoch 264/299\n",
      "----------\n",
      "train Loss: 0.1417 Acc: 0.9479\n",
      "val Loss: 0.0929 Acc: 0.9685\n",
      "\n",
      "Epoch 265/299\n",
      "----------\n",
      "train Loss: 0.1447 Acc: 0.9468\n",
      "val Loss: 0.0898 Acc: 0.9680\n",
      "\n",
      "Epoch 266/299\n",
      "----------\n",
      "train Loss: 0.1388 Acc: 0.9484\n",
      "val Loss: 0.0917 Acc: 0.9676\n",
      "\n",
      "Epoch 267/299\n",
      "----------\n",
      "train Loss: 0.1457 Acc: 0.9450\n",
      "val Loss: 0.0893 Acc: 0.9705\n",
      "\n",
      "Epoch 268/299\n",
      "----------\n",
      "train Loss: 0.1488 Acc: 0.9413\n",
      "val Loss: 0.0889 Acc: 0.9680\n",
      "\n",
      "Epoch 269/299\n",
      "----------\n",
      "train Loss: 0.1420 Acc: 0.9481\n",
      "val Loss: 0.0875 Acc: 0.9701\n",
      "\n",
      "Epoch 270/299\n",
      "----------\n",
      "train Loss: 0.1417 Acc: 0.9476\n",
      "val Loss: 0.0898 Acc: 0.9676\n",
      "\n",
      "Epoch 271/299\n",
      "----------\n",
      "train Loss: 0.1427 Acc: 0.9479\n",
      "val Loss: 0.0879 Acc: 0.9680\n",
      "\n",
      "Epoch 272/299\n",
      "----------\n",
      "train Loss: 0.1398 Acc: 0.9465\n",
      "val Loss: 0.0939 Acc: 0.9693\n",
      "\n",
      "Epoch 273/299\n",
      "----------\n",
      "train Loss: 0.1451 Acc: 0.9488\n",
      "val Loss: 0.0924 Acc: 0.9680\n",
      "\n",
      "Epoch 274/299\n",
      "----------\n",
      "train Loss: 0.1396 Acc: 0.9486\n",
      "val Loss: 0.0921 Acc: 0.9676\n",
      "\n",
      "Epoch 275/299\n",
      "----------\n",
      "train Loss: 0.1411 Acc: 0.9495\n",
      "val Loss: 0.0926 Acc: 0.9701\n",
      "\n",
      "Epoch 276/299\n",
      "----------\n",
      "train Loss: 0.1417 Acc: 0.9450\n",
      "val Loss: 0.0909 Acc: 0.9685\n",
      "\n",
      "Epoch 277/299\n",
      "----------\n",
      "train Loss: 0.1453 Acc: 0.9451\n",
      "val Loss: 0.0885 Acc: 0.9693\n",
      "\n",
      "Epoch 278/299\n",
      "----------\n",
      "train Loss: 0.1398 Acc: 0.9466\n",
      "val Loss: 0.0896 Acc: 0.9672\n",
      "\n",
      "Epoch 279/299\n",
      "----------\n",
      "train Loss: 0.1361 Acc: 0.9485\n",
      "val Loss: 0.0892 Acc: 0.9680\n",
      "\n",
      "Epoch 280/299\n",
      "----------\n",
      "train Loss: 0.1374 Acc: 0.9483\n",
      "val Loss: 0.0894 Acc: 0.9689\n",
      "\n",
      "Epoch 281/299\n",
      "----------\n",
      "train Loss: 0.1405 Acc: 0.9455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0982 Acc: 0.9635\n",
      "\n",
      "Epoch 282/299\n",
      "----------\n",
      "train Loss: 0.1320 Acc: 0.9510\n",
      "val Loss: 0.0912 Acc: 0.9705\n",
      "\n",
      "Epoch 283/299\n",
      "----------\n",
      "train Loss: 0.1361 Acc: 0.9496\n",
      "val Loss: 0.0918 Acc: 0.9672\n",
      "\n",
      "Epoch 284/299\n",
      "----------\n",
      "train Loss: 0.1385 Acc: 0.9477\n",
      "val Loss: 0.0913 Acc: 0.9676\n",
      "\n",
      "Epoch 285/299\n",
      "----------\n",
      "train Loss: 0.1428 Acc: 0.9475\n",
      "val Loss: 0.0910 Acc: 0.9689\n",
      "\n",
      "Epoch 286/299\n",
      "----------\n",
      "train Loss: 0.1368 Acc: 0.9484\n",
      "val Loss: 0.0905 Acc: 0.9680\n",
      "\n",
      "Epoch 287/299\n",
      "----------\n",
      "train Loss: 0.1356 Acc: 0.9488\n",
      "val Loss: 0.0893 Acc: 0.9680\n",
      "\n",
      "Epoch 288/299\n",
      "----------\n",
      "train Loss: 0.1422 Acc: 0.9467\n",
      "val Loss: 0.0861 Acc: 0.9705\n",
      "\n",
      "Epoch 289/299\n",
      "----------\n",
      "train Loss: 0.1377 Acc: 0.9495\n",
      "val Loss: 0.0920 Acc: 0.9693\n",
      "\n",
      "Epoch 290/299\n",
      "----------\n",
      "train Loss: 0.1429 Acc: 0.9445\n",
      "val Loss: 0.0897 Acc: 0.9697\n",
      "\n",
      "Epoch 291/299\n",
      "----------\n",
      "train Loss: 0.1378 Acc: 0.9462\n",
      "val Loss: 0.0946 Acc: 0.9660\n",
      "\n",
      "Epoch 292/299\n",
      "----------\n",
      "train Loss: 0.1445 Acc: 0.9436\n",
      "val Loss: 0.0887 Acc: 0.9693\n",
      "\n",
      "Epoch 293/299\n",
      "----------\n",
      "train Loss: 0.1440 Acc: 0.9449\n",
      "val Loss: 0.0880 Acc: 0.9718\n",
      "\n",
      "Epoch 294/299\n",
      "----------\n",
      "train Loss: 0.1405 Acc: 0.9477\n",
      "val Loss: 0.0991 Acc: 0.9639\n",
      "\n",
      "Epoch 295/299\n",
      "----------\n",
      "train Loss: 0.1349 Acc: 0.9486\n",
      "val Loss: 0.0912 Acc: 0.9680\n",
      "\n",
      "Epoch 296/299\n",
      "----------\n",
      "train Loss: 0.1405 Acc: 0.9462\n",
      "val Loss: 0.0861 Acc: 0.9697\n",
      "\n",
      "Epoch 297/299\n",
      "----------\n",
      "train Loss: 0.1399 Acc: 0.9480\n",
      "val Loss: 0.0907 Acc: 0.9697\n",
      "\n",
      "Epoch 298/299\n",
      "----------\n",
      "train Loss: 0.1388 Acc: 0.9496\n",
      "val Loss: 0.0860 Acc: 0.9705\n",
      "\n",
      "Epoch 299/299\n",
      "----------\n",
      "train Loss: 0.1431 Acc: 0.9465\n",
      "val Loss: 0.0888 Acc: 0.9680\n",
      "\n",
      "\n",
      "##############################\n",
      "------ Summary ------\n",
      "model -> vgg\n",
      "epochs -> 300\n",
      "lr -> 0.0001\n",
      "batch size -> 8\n",
      "\n",
      "Training complete in 164m 40s\n",
      "Best val Acc: 0.971773\n",
      "##############################\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== END ====\n",
      "[[789   2   0]\n",
      " [  1 777  21]\n",
      " [  3  41 775]]\n",
      "\n",
      "Sensitivity or recall total\n",
      "0.9717725197177252\n",
      "\n",
      "Sensitivity or recall per classes\n",
      "[1.   0.97 0.95]\n",
      "\n",
      "Precision\n",
      "[0.99 0.95 0.97]\n",
      "\n",
      "F1 Score\n",
      "[1.   0.96 0.96]\n",
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "num_classes = 3\n",
    "\n",
    "_models = ['squeezenet', 'densenet', 'resnet', 'alexnet', 'vgg']\n",
    "lrs = [1e-4]\n",
    "_epoch = [500]\n",
    "batch_sizes = [8]\n",
    "\n",
    "for _model in _models:\n",
    "    for _epochs in _epoch:\n",
    "        for _lrs in lrs:\n",
    "            for _batch in batch_sizes:\n",
    "                               \n",
    "                print()\n",
    "\n",
    "                print('='*60)\n",
    "\n",
    "                print('==== INITIALIZING WITH PARAMETERS: ====')\n",
    "\n",
    "                print(f'model -> {_model}')\n",
    "\n",
    "                print(f'epochs -> {_epochs}')\n",
    "\n",
    "                print(f'lr -> {_lrs}')\n",
    "\n",
    "                print(f'batch size -> {_batch}')\n",
    "\n",
    "                print()\n",
    "\n",
    "\n",
    "                feature_extract = True\n",
    "                \n",
    "                model_ft, input_size = initialize_model(_model, num_classes, \n",
    "                                                        feature_extract, use_pretrained=True)\n",
    "                \n",
    "                # Send the model to GPU\n",
    "                model_ft = model_ft.to(device)\n",
    "\n",
    "                print('-'*20)\n",
    "                params_to_update = model_ft.parameters()\n",
    "                print(\"Params to learn:\")\n",
    "                if feature_extract:\n",
    "                    params_to_update = []\n",
    "                    for name,param in model_ft.named_parameters():\n",
    "                        if param.requires_grad == True:\n",
    "                            params_to_update.append(param)\n",
    "                            print(\"\\t\",name)\n",
    "                            \n",
    "                else:\n",
    "                    for name,param in model_ft.named_parameters():\n",
    "                        if param.requires_grad == True:\n",
    "                            print(\"\\t\",name)\n",
    "                            \n",
    "\n",
    "                print()\n",
    "                \n",
    "                print('-'*20)\n",
    "                \n",
    "                print()\n",
    "                print('== Epochs ==')\n",
    "                \n",
    "                optimizer_ft = optim.SGD(params_to_update, _lrs, momentum=0.9)\n",
    "                exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)\n",
    "\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "                model_ft, hist = train_model(model_ft, dataloaders, criterion, optimizer_ft,\n",
    "                                             num_epochs=_epochs, model_name=_model, lr=_lrs, batch_size=_batch)\n",
    "                \n",
    "                from sklearn.metrics import confusion_matrix\n",
    "\n",
    "                nb_classes = 3\n",
    "\n",
    "                # Initialize the prediction and label lists(tensors)\n",
    "                predlist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "                lbllist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for i, (inputs, classes) in enumerate(dataloaders['val']):\n",
    "                        inputs = inputs.to(device) #labels atuais\n",
    "                        classes = classes.to(device) #classes\n",
    "                        outputs = model_ft(inputs) #valores preditos = Passa o label atual e retorna o que o modelo predice\n",
    "                        _, preds = torch.max(outputs, 1) #pega o maior valor das predições\n",
    "\n",
    "                        # Append batch prediction results\n",
    "                        predlist=torch.cat([predlist,preds.view(-1).cpu()])\n",
    "                        lbllist=torch.cat([lbllist,classes.view(-1).cpu()])\n",
    "\n",
    "                # Confusion matrix\n",
    "                conf_mat=confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
    "                print(conf_mat)\n",
    "                print()\n",
    "                \n",
    "                from sklearn import metrics\n",
    "\n",
    "                #analise dos resultados do modelo\n",
    "                print('Sensitivity or recall total')\n",
    "                print (metrics.recall_score(lbllist.numpy(), predlist.numpy(), average='micro'))\n",
    "\n",
    "                print()\n",
    "                print('Sensitivity or recall per classes')\n",
    "                print (metrics.recall_score(lbllist.numpy(), predlist.numpy(), average=None))\n",
    "\n",
    "                print()\n",
    "                print('Precision')\n",
    "                print (metrics.precision_score(lbllist.numpy(), predlist.numpy(), average=None))\n",
    "\n",
    "                print()\n",
    "                print('F1 Score')\n",
    "                print (metrics.f1_score(lbllist.numpy(), predlist.numpy(), average=None))\n",
    "                \n",
    "                cm = confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
    "                np.set_printoptions(precision=2)\n",
    "\n",
    "                plt.figure()\n",
    "\n",
    "                plot_confusion_matrix(cm, classes=['norm', 'covid', 'pnemo'], _model, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Os resultados na matriz de confuzão e scores estão ruins, provavelmente pelo param.requires_grad = True \n",
    "\n",
    "Testar depois com False\n",
    "\n",
    "**Peguei o código original**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"------ RESULTADOS ------\")\n",
    "print()\n",
    "plt.figure(figsize=(13, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label=\"AUC Treino\")\n",
    "plt.plot(history.history['val_accuracy'], label=\"AUC VALIDAÇÃO\")\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label=\"loss validação\")\n",
    "plt.plot(history.history['val_loss'], label=\"AUC validação\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hook the feature extractor\n",
    "# features_blobs = []\n",
    "# def hook_feature(module, input, output):\n",
    "#     features_blobs.append(output.data.cpu().numpy())\n",
    "\n",
    "# model_ft._modules.get(finalconv_name).register_forward_hook(hook_feature)\n",
    "\n",
    "# # get the softmax weight\n",
    "# params = list(model_ft.parameters())\n",
    "# weight_softmax = np.squeeze(params[-2].data.numpy())\n",
    "\n",
    "# def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "#     # generate the class activation maps upsample to 256x256\n",
    "#     size_upsample = (256, 256)\n",
    "#     bz, nc, h, w = feature_conv.shape\n",
    "#     output_cam = []\n",
    "#     for idx in class_idx:\n",
    "#         cam = weight_softmax[idx].dot(feature_conv.reshape((nc, h*w)))\n",
    "#         cam = cam.reshape(h, w)\n",
    "#         cam = cam - np.min(cam)\n",
    "#         cam_img = cam / np.max(cam)\n",
    "#         cam_img = np.uint8(255 * cam_img)\n",
    "#         output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "#     return output_cam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize = transforms.Normalize(\n",
    "#    mean=[0.485, 0.456, 0.406],\n",
    "#    std=[0.229, 0.224, 0.225]\n",
    "# )\n",
    "# preprocess = transforms.Compose([\n",
    "#    transforms.Resize((224,224)),\n",
    "#    transforms.ToTensor(),\n",
    "#    normalize\n",
    "# ])\n",
    "\n",
    "# response = requests.get(IMG_URL)\n",
    "# img_pil = Image.open(io.BytesIO(response.content))\n",
    "# img_pil.save('test.jpg')\n",
    "\n",
    "# img_tensor = preprocess(img_pil)\n",
    "# img_variable = Variable(img_tensor.unsqueeze(0))\n",
    "# logit = model_ft(img_variable)\n",
    "\n",
    "# # download the imagenet category list\n",
    "# # classes = {int(key):value for (key, value)\n",
    "# #           in requests.get(LABELS_URL).json().items()}\n",
    "\n",
    "# # h_x = F.softmax(logit, dim=1).data.squeeze()\n",
    "# # probs, idx = h_x.sort(0, True)\n",
    "# # probs = probs.numpy()\n",
    "# # idx = idx.numpy()\n",
    "\n",
    "# # # output the prediction\n",
    "# # for i in range(0, 5):\n",
    "# #     print('{:.3f} -> {}'.format(probs[i], classes[idx[i]]))\n",
    "\n",
    "# # generate class activation mapping for the top1 prediction\n",
    "# CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[0]])\n",
    "\n",
    "# # render the CAM and output\n",
    "# # print('output CAM.jpg for the top1 prediction: %s'%classes[idx[0]])\n",
    "# img = cv2.imread('test.jpg')\n",
    "# height, width, _ = img.shape\n",
    "# heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
    "# result = heatmap * 0.3 + img * 0.5\n",
    "# cv2.imwrite('CAM.jpg', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import IPython.display as display\n",
    "# from PIL import Image\n",
    "# image_path = 'CAM.jpg'\n",
    "# display.display(Image.open(image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 1 FIIMMMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "nb_classes = 3\n",
    "\n",
    "# Initialize the prediction and label lists(tensors)\n",
    "predlist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "lbllist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, classes) in enumerate(dataloaders['val']):\n",
    "        inputs = inputs.to(device) #labels atuais\n",
    "        classes = classes.to(device) #classes\n",
    "        outputs = model_ft(inputs) #valores preditos = Passa o label atual e retorna o que o modelo predice\n",
    "        _, preds = torch.max(outputs, 1) #pega o maior valor das predições\n",
    "\n",
    "        # Append batch prediction results\n",
    "        predlist=torch.cat([predlist,preds.view(-1).cpu()])\n",
    "        lbllist=torch.cat([lbllist,classes.view(-1).cpu()])\n",
    "\n",
    "# Confusion matrix\n",
    "conf_mat=confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
    "print(conf_mat)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    #mean = np.array([0.485, 0.456, 0.405])\n",
    "    #std = np.array([0.229, 0.224, 0.225])\n",
    "    #inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    #plt.pause(1)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=10):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    for i, (inputs, classes) in enumerate(dataloaders['test']):\n",
    "        #inputs, labels = data\n",
    "\n",
    "        #inputs, labels = Variable(inputs), Variable(labels)\n",
    "        inputs = inputs.to(device) #labels atuais\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "        for j in range(inputs.size()[0]):\n",
    "            images_so_far += 1\n",
    "            ax = plt.figure(figsize=(20,20))\n",
    "            ax = plt.subplot(5, 2, images_so_far)\n",
    "            ax.axis('off')\n",
    "            ax.set_title('{}'.format(class_names[predlist[j]]))\n",
    "            imshow(inputs.cpu().data[j])\n",
    "\n",
    "            if images_so_far == num_images:\n",
    "                model.train(mode=was_training)\n",
    "                return\n",
    "    model.train(mode=was_training)\n",
    "\n",
    "#print(dir(model))\n",
    "visualize_model(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# savePath = \"test_model.pth\"\n",
    "# torch.save(model_ft.state_dict(), savePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRY CAM\n",
    "#do it using RESNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time\n",
    "import PIL\n",
    "import scipy.ndimage as nd\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "#transforms.RandomRotation(degrees=(-5, 5)),\n",
    "#transforms.ColorJitter(brightness=.02),\n",
    "    \n",
    "transformers = {\n",
    "    'train_transforms': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'test_transforms': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'valid_transforms': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "}\n",
    "\n",
    "trans = ['train_transforms','valid_transforms','test_transforms']\n",
    "\n",
    "path = \"/home/jimi/dissertacao/covid19/datasets/80-20/\"\n",
    "categories = ['train','val','test']\n",
    "dset = {x : torchvision.datasets.ImageFolder(path+x,\n",
    "                                             transform=transformers[y]) for x,y in zip(categories, trans)}\n",
    "\n",
    "dataset_sizes = ['train']\n",
    "\n",
    "\n",
    "num_threads = 4 \n",
    "dataloaders =  {x : torch.utils.data.DataLoader(dset[x], batch_size=16, shuffle=True, num_workers=num_threads)\n",
    "               for x in categories}\n",
    "\n",
    "dataset_sizes = {x : len(dset[x]) for x in ['train','val','test']}\n",
    "\n",
    "class_names = dset['train'].classes\n",
    "\n",
    "#class_names = image_datasets['train'].classes\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/500\n",
      "----------\n",
      "train loss:  0.2737  acc: 0.9047\n",
      "val loss:  0.1524  acc: 0.9498\n",
      "epoch 2/500\n",
      "----------\n",
      "train loss:  0.1842  acc: 0.9326\n",
      "val loss:  0.1218  acc: 0.9585\n",
      "epoch 3/500\n",
      "----------\n",
      "train loss:  0.1729  acc: 0.9340\n",
      "val loss:  0.1470  acc: 0.9452\n",
      "epoch 4/500\n",
      "----------\n",
      "train loss:  0.1445  acc: 0.9490\n",
      "val loss:  0.1041  acc: 0.9626\n",
      "epoch 5/500\n",
      "----------\n",
      "train loss:  0.1365  acc: 0.9504\n",
      "val loss:  0.1138  acc: 0.9589\n",
      "epoch 6/500\n",
      "----------\n",
      "train loss:  0.1380  acc: 0.9506\n",
      "val loss:  0.1104  acc: 0.9639\n",
      "epoch 7/500\n",
      "----------\n",
      "train loss:  0.1356  acc: 0.9516\n",
      "val loss:  0.1146  acc: 0.9606\n",
      "epoch 8/500\n",
      "----------\n",
      "train loss:  0.1322  acc: 0.9519\n",
      "val loss:  0.0998  acc: 0.9668\n",
      "epoch 9/500\n",
      "----------\n",
      "train loss:  0.1334  acc: 0.9534\n",
      "val loss:  0.1059  acc: 0.9618\n",
      "epoch 10/500\n",
      "----------\n",
      "train loss:  0.1276  acc: 0.9563\n",
      "val loss:  0.0962  acc: 0.9668\n",
      "epoch 11/500\n",
      "----------\n",
      "train loss:  0.1248  acc: 0.9565\n",
      "val loss:  0.1037  acc: 0.9639\n",
      "epoch 12/500\n",
      "----------\n",
      "train loss:  0.1316  acc: 0.9531\n",
      "val loss:  0.0999  acc: 0.9680\n",
      "epoch 13/500\n",
      "----------\n",
      "train loss:  0.1290  acc: 0.9517\n",
      "val loss:  0.1044  acc: 0.9622\n",
      "epoch 14/500\n",
      "----------\n",
      "train loss:  0.1320  acc: 0.9547\n",
      "val loss:  0.1070  acc: 0.9606\n",
      "epoch 15/500\n",
      "----------\n",
      "train loss:  0.1319  acc: 0.9521\n",
      "val loss:  0.1153  acc: 0.9577\n",
      "epoch 16/500\n",
      "----------\n",
      "train loss:  0.1270  acc: 0.9536\n",
      "val loss:  0.1062  acc: 0.9635\n",
      "epoch 17/500\n",
      "----------\n",
      "train loss:  0.1283  acc: 0.9526\n",
      "val loss:  0.1109  acc: 0.9601\n",
      "epoch 18/500\n",
      "----------\n",
      "train loss:  0.1316  acc: 0.9520\n",
      "val loss:  0.1098  acc: 0.9626\n",
      "epoch 19/500\n",
      "----------\n",
      "train loss:  0.1337  acc: 0.9503\n",
      "val loss:  0.1239  acc: 0.9535\n",
      "epoch 20/500\n",
      "----------\n",
      "train loss:  0.1347  acc: 0.9491\n",
      "val loss:  0.0985  acc: 0.9660\n",
      "epoch 21/500\n",
      "----------\n",
      "train loss:  0.1276  acc: 0.9516\n",
      "val loss:  0.1012  acc: 0.9639\n",
      "epoch 22/500\n",
      "----------\n",
      "train loss:  0.1300  acc: 0.9544\n",
      "val loss:  0.1162  acc: 0.9581\n",
      "epoch 23/500\n",
      "----------\n",
      "train loss:  0.1250  acc: 0.9550\n",
      "val loss:  0.1069  acc: 0.9618\n",
      "epoch 24/500\n",
      "----------\n",
      "train loss:  0.1311  acc: 0.9543\n",
      "val loss:  0.1061  acc: 0.9635\n",
      "epoch 25/500\n",
      "----------\n",
      "train loss:  0.1289  acc: 0.9538\n",
      "val loss:  0.1098  acc: 0.9589\n",
      "epoch 26/500\n",
      "----------\n",
      "train loss:  0.1312  acc: 0.9527\n",
      "val loss:  0.1057  acc: 0.9622\n",
      "epoch 27/500\n",
      "----------\n",
      "train loss:  0.1271  acc: 0.9531\n",
      "val loss:  0.1033  acc: 0.9626\n",
      "epoch 28/500\n",
      "----------\n",
      "train loss:  0.1274  acc: 0.9526\n",
      "val loss:  0.1073  acc: 0.9614\n",
      "epoch 29/500\n",
      "----------\n",
      "train loss:  0.1300  acc: 0.9527\n",
      "val loss:  0.1016  acc: 0.9639\n",
      "epoch 30/500\n",
      "----------\n",
      "train loss:  0.1321  acc: 0.9518\n",
      "val loss:  0.1128  acc: 0.9581\n",
      "epoch 31/500\n",
      "----------\n",
      "train loss:  0.1370  acc: 0.9489\n",
      "val loss:  0.1114  acc: 0.9601\n",
      "epoch 32/500\n",
      "----------\n",
      "train loss:  0.1243  acc: 0.9533\n",
      "val loss:  0.1031  acc: 0.9660\n",
      "epoch 33/500\n",
      "----------\n",
      "train loss:  0.1268  acc: 0.9534\n",
      "val loss:  0.1199  acc: 0.9556\n",
      "epoch 34/500\n",
      "----------\n",
      "train loss:  0.1285  acc: 0.9536\n",
      "val loss:  0.1128  acc: 0.9593\n",
      "epoch 35/500\n",
      "----------\n",
      "train loss:  0.1406  acc: 0.9516\n",
      "val loss:  0.1055  acc: 0.9622\n",
      "epoch 36/500\n",
      "----------\n",
      "train loss:  0.1300  acc: 0.9511\n",
      "val loss:  0.1107  acc: 0.9610\n",
      "epoch 37/500\n",
      "----------\n",
      "train loss:  0.1284  acc: 0.9517\n",
      "val loss:  0.1085  acc: 0.9597\n",
      "epoch 38/500\n",
      "----------\n",
      "train loss:  0.1275  acc: 0.9542\n",
      "val loss:  0.0996  acc: 0.9647\n",
      "epoch 39/500\n",
      "----------\n",
      "train loss:  0.1353  acc: 0.9521\n",
      "val loss:  0.1181  acc: 0.9543\n",
      "epoch 40/500\n",
      "----------\n",
      "train loss:  0.1329  acc: 0.9541\n",
      "val loss:  0.1063  acc: 0.9639\n",
      "epoch 41/500\n",
      "----------\n",
      "train loss:  0.1296  acc: 0.9530\n",
      "val loss:  0.1078  acc: 0.9610\n",
      "epoch 42/500\n",
      "----------\n",
      "train loss:  0.1295  acc: 0.9535\n",
      "val loss:  0.1060  acc: 0.9635\n",
      "epoch 43/500\n",
      "----------\n",
      "train loss:  0.1298  acc: 0.9542\n",
      "val loss:  0.1009  acc: 0.9651\n",
      "epoch 44/500\n",
      "----------\n",
      "train loss:  0.1283  acc: 0.9536\n",
      "val loss:  0.1038  acc: 0.9639\n",
      "epoch 45/500\n",
      "----------\n",
      "train loss:  0.1388  acc: 0.9499\n",
      "val loss:  0.1028  acc: 0.9639\n",
      "epoch 46/500\n",
      "----------\n",
      "train loss:  0.1309  acc: 0.9537\n",
      "val loss:  0.0991  acc: 0.9668\n",
      "epoch 47/500\n",
      "----------\n",
      "train loss:  0.1307  acc: 0.9507\n",
      "val loss:  0.1023  acc: 0.9664\n",
      "epoch 48/500\n",
      "----------\n",
      "train loss:  0.1318  acc: 0.9526\n",
      "val loss:  0.1076  acc: 0.9622\n",
      "epoch 49/500\n",
      "----------\n",
      "train loss:  0.1310  acc: 0.9551\n",
      "val loss:  0.1202  acc: 0.9552\n",
      "epoch 50/500\n",
      "----------\n",
      "train loss:  0.1339  acc: 0.9518\n",
      "val loss:  0.1087  acc: 0.9618\n",
      "epoch 51/500\n",
      "----------\n",
      "train loss:  0.1319  acc: 0.9523\n",
      "val loss:  0.1021  acc: 0.9631\n",
      "epoch 52/500\n",
      "----------\n",
      "train loss:  0.1303  acc: 0.9526\n",
      "val loss:  0.1088  acc: 0.9618\n",
      "epoch 53/500\n",
      "----------\n",
      "train loss:  0.1297  acc: 0.9543\n",
      "val loss:  0.1036  acc: 0.9622\n",
      "epoch 54/500\n",
      "----------\n",
      "train loss:  0.1336  acc: 0.9518\n",
      "val loss:  0.0952  acc: 0.9689\n",
      "epoch 55/500\n",
      "----------\n",
      "train loss:  0.1319  acc: 0.9532\n",
      "val loss:  0.1152  acc: 0.9593\n",
      "epoch 56/500\n",
      "----------\n",
      "train loss:  0.1329  acc: 0.9512\n",
      "val loss:  0.1127  acc: 0.9581\n",
      "epoch 57/500\n",
      "----------\n",
      "train loss:  0.1280  acc: 0.9547\n",
      "val loss:  0.1025  acc: 0.9651\n",
      "epoch 58/500\n",
      "----------\n",
      "train loss:  0.1265  acc: 0.9539\n",
      "val loss:  0.0960  acc: 0.9668\n",
      "epoch 59/500\n",
      "----------\n",
      "train loss:  0.1290  acc: 0.9536\n",
      "val loss:  0.0951  acc: 0.9680\n",
      "epoch 60/500\n",
      "----------\n",
      "train loss:  0.1284  acc: 0.9523\n",
      "val loss:  0.1050  acc: 0.9618\n",
      "epoch 61/500\n",
      "----------\n",
      "train loss:  0.1320  acc: 0.9500\n",
      "val loss:  0.1073  acc: 0.9610\n",
      "epoch 62/500\n",
      "----------\n",
      "train loss:  0.1303  acc: 0.9519\n",
      "val loss:  0.1039  acc: 0.9651\n",
      "epoch 63/500\n",
      "----------\n",
      "train loss:  0.1323  acc: 0.9517\n",
      "val loss:  0.1061  acc: 0.9626\n",
      "epoch 64/500\n",
      "----------\n",
      "train loss:  0.1330  acc: 0.9528\n",
      "val loss:  0.1126  acc: 0.9593\n",
      "epoch 65/500\n",
      "----------\n",
      "train loss:  0.1280  acc: 0.9547\n",
      "val loss:  0.0989  acc: 0.9664\n",
      "epoch 66/500\n",
      "----------\n",
      "train loss:  0.1297  acc: 0.9547\n",
      "val loss:  0.1056  acc: 0.9626\n",
      "epoch 67/500\n",
      "----------\n",
      "train loss:  0.1282  acc: 0.9518\n",
      "val loss:  0.1042  acc: 0.9626\n",
      "epoch 68/500\n",
      "----------\n",
      "train loss:  0.1314  acc: 0.9535\n",
      "val loss:  0.1111  acc: 0.9589\n",
      "epoch 69/500\n",
      "----------\n",
      "train loss:  0.1293  acc: 0.9538\n",
      "val loss:  0.1132  acc: 0.9585\n",
      "epoch 70/500\n",
      "----------\n",
      "train loss:  0.1306  acc: 0.9523\n",
      "val loss:  0.0992  acc: 0.9651\n",
      "epoch 71/500\n",
      "----------\n",
      "train loss:  0.1316  acc: 0.9515\n",
      "val loss:  0.0998  acc: 0.9668\n",
      "epoch 72/500\n",
      "----------\n",
      "train loss:  0.1304  acc: 0.9512\n",
      "val loss:  0.1066  acc: 0.9631\n",
      "epoch 73/500\n",
      "----------\n",
      "train loss:  0.1337  acc: 0.9510\n",
      "val loss:  0.1034  acc: 0.9651\n",
      "epoch 74/500\n",
      "----------\n",
      "train loss:  0.1285  acc: 0.9541\n",
      "val loss:  0.0953  acc: 0.9651\n",
      "epoch 75/500\n",
      "----------\n",
      "train loss:  0.1318  acc: 0.9510\n",
      "val loss:  0.1031  acc: 0.9660\n",
      "epoch 76/500\n",
      "----------\n",
      "train loss:  0.1329  acc: 0.9517\n",
      "val loss:  0.1089  acc: 0.9614\n",
      "epoch 77/500\n",
      "----------\n",
      "train loss:  0.1251  acc: 0.9544\n",
      "val loss:  0.0986  acc: 0.9664\n",
      "epoch 78/500\n",
      "----------\n",
      "train loss:  0.1333  acc: 0.9522\n",
      "val loss:  0.1083  acc: 0.9606\n",
      "epoch 79/500\n",
      "----------\n",
      "train loss:  0.1285  acc: 0.9540\n",
      "val loss:  0.1018  acc: 0.9651\n",
      "epoch 80/500\n",
      "----------\n",
      "train loss:  0.1306  acc: 0.9522\n",
      "val loss:  0.1023  acc: 0.9631\n",
      "epoch 81/500\n",
      "----------\n",
      "train loss:  0.1331  acc: 0.9522\n",
      "val loss:  0.1014  acc: 0.9647\n",
      "epoch 82/500\n",
      "----------\n",
      "train loss:  0.1281  acc: 0.9544\n",
      "val loss:  0.1156  acc: 0.9577\n",
      "epoch 83/500\n",
      "----------\n",
      "train loss:  0.1321  acc: 0.9537\n",
      "val loss:  0.1140  acc: 0.9585\n",
      "epoch 84/500\n",
      "----------\n",
      "train loss:  0.1275  acc: 0.9561\n",
      "val loss:  0.1123  acc: 0.9589\n",
      "epoch 85/500\n",
      "----------\n",
      "train loss:  0.1303  acc: 0.9547\n",
      "val loss:  0.0999  acc: 0.9651\n",
      "epoch 86/500\n",
      "----------\n",
      "train loss:  0.1311  acc: 0.9527\n",
      "val loss:  0.1060  acc: 0.9626\n",
      "epoch 87/500\n",
      "----------\n",
      "train loss:  0.1288  acc: 0.9534\n",
      "val loss:  0.1113  acc: 0.9610\n",
      "epoch 88/500\n",
      "----------\n",
      "train loss:  0.1283  acc: 0.9541\n",
      "val loss:  0.1067  acc: 0.9643\n",
      "epoch 89/500\n",
      "----------\n",
      "train loss:  0.1344  acc: 0.9530\n",
      "val loss:  0.1030  acc: 0.9639\n",
      "epoch 90/500\n",
      "----------\n",
      "train loss:  0.1276  acc: 0.9561\n",
      "val loss:  0.1074  acc: 0.9622\n",
      "epoch 91/500\n",
      "----------\n",
      "train loss:  0.1330  acc: 0.9526\n",
      "val loss:  0.1068  acc: 0.9606\n",
      "epoch 92/500\n",
      "----------\n",
      "train loss:  0.1271  acc: 0.9544\n",
      "val loss:  0.0989  acc: 0.9668\n",
      "epoch 93/500\n",
      "----------\n",
      "train loss:  0.1287  acc: 0.9564\n",
      "val loss:  0.1065  acc: 0.9597\n",
      "epoch 94/500\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  0.1298  acc: 0.9523\n",
      "val loss:  0.1062  acc: 0.9618\n",
      "epoch 95/500\n",
      "----------\n",
      "train loss:  0.1293  acc: 0.9537\n",
      "val loss:  0.1039  acc: 0.9643\n",
      "epoch 96/500\n",
      "----------\n",
      "train loss:  0.1278  acc: 0.9535\n",
      "val loss:  0.1018  acc: 0.9655\n",
      "epoch 97/500\n",
      "----------\n",
      "train loss:  0.1217  acc: 0.9587\n",
      "val loss:  0.1050  acc: 0.9664\n",
      "epoch 98/500\n",
      "----------\n",
      "train loss:  0.1287  acc: 0.9537\n",
      "val loss:  0.1074  acc: 0.9622\n",
      "epoch 99/500\n",
      "----------\n",
      "train loss:  0.1300  acc: 0.9527\n",
      "val loss:  0.1051  acc: 0.9618\n",
      "epoch 100/500\n",
      "----------\n",
      "train loss:  0.1276  acc: 0.9542\n",
      "val loss:  0.1061  acc: 0.9655\n",
      "epoch 101/500\n",
      "----------\n",
      "train loss:  0.1309  acc: 0.9510\n",
      "val loss:  0.1045  acc: 0.9626\n",
      "epoch 102/500\n",
      "----------\n",
      "train loss:  0.1305  acc: 0.9532\n",
      "val loss:  0.1043  acc: 0.9635\n",
      "epoch 103/500\n",
      "----------\n",
      "train loss:  0.1323  acc: 0.9520\n",
      "val loss:  0.1114  acc: 0.9597\n",
      "epoch 104/500\n",
      "----------\n",
      "train loss:  0.1272  acc: 0.9537\n",
      "val loss:  0.1235  acc: 0.9560\n",
      "epoch 105/500\n",
      "----------\n",
      "train loss:  0.1304  acc: 0.9557\n",
      "val loss:  0.1083  acc: 0.9622\n",
      "epoch 106/500\n",
      "----------\n",
      "train loss:  0.1311  acc: 0.9520\n",
      "val loss:  0.1015  acc: 0.9647\n",
      "epoch 107/500\n",
      "----------\n",
      "train loss:  0.1290  acc: 0.9541\n",
      "val loss:  0.0995  acc: 0.9668\n",
      "epoch 108/500\n",
      "----------\n",
      "train loss:  0.1292  acc: 0.9521\n",
      "val loss:  0.1043  acc: 0.9622\n",
      "epoch 109/500\n",
      "----------\n",
      "train loss:  0.1337  acc: 0.9519\n",
      "val loss:  0.1055  acc: 0.9610\n",
      "epoch 110/500\n",
      "----------\n",
      "train loss:  0.1317  acc: 0.9515\n",
      "val loss:  0.1041  acc: 0.9643\n",
      "epoch 111/500\n",
      "----------\n",
      "train loss:  0.1307  acc: 0.9545\n",
      "val loss:  0.1012  acc: 0.9643\n",
      "epoch 112/500\n",
      "----------\n",
      "train loss:  0.1315  acc: 0.9537\n",
      "val loss:  0.1072  acc: 0.9597\n",
      "epoch 113/500\n",
      "----------\n",
      "train loss:  0.1311  acc: 0.9531\n",
      "val loss:  0.1115  acc: 0.9572\n",
      "epoch 114/500\n",
      "----------\n",
      "train loss:  0.1252  acc: 0.9546\n",
      "val loss:  0.1053  acc: 0.9626\n",
      "epoch 115/500\n",
      "----------\n",
      "train loss:  0.1280  acc: 0.9531\n",
      "val loss:  0.1063  acc: 0.9635\n",
      "epoch 116/500\n",
      "----------\n",
      "train loss:  0.1342  acc: 0.9516\n",
      "val loss:  0.0991  acc: 0.9680\n",
      "epoch 117/500\n",
      "----------\n",
      "train loss:  0.1330  acc: 0.9521\n",
      "val loss:  0.1116  acc: 0.9568\n",
      "epoch 118/500\n",
      "----------\n",
      "train loss:  0.1291  acc: 0.9534\n",
      "val loss:  0.1044  acc: 0.9635\n",
      "epoch 119/500\n",
      "----------\n",
      "train loss:  0.1316  acc: 0.9535\n",
      "val loss:  0.1010  acc: 0.9651\n",
      "epoch 120/500\n",
      "----------\n",
      "train loss:  0.1272  acc: 0.9558\n",
      "val loss:  0.1004  acc: 0.9660\n",
      "epoch 121/500\n",
      "----------\n",
      "train loss:  0.1305  acc: 0.9544\n",
      "val loss:  0.1040  acc: 0.9639\n",
      "epoch 122/500\n",
      "----------\n",
      "train loss:  0.1333  acc: 0.9517\n",
      "val loss:  0.0976  acc: 0.9676\n",
      "epoch 123/500\n",
      "----------\n",
      "train loss:  0.1261  acc: 0.9546\n",
      "val loss:  0.1042  acc: 0.9639\n",
      "epoch 124/500\n",
      "----------\n",
      "train loss:  0.1301  acc: 0.9528\n",
      "val loss:  0.1128  acc: 0.9622\n",
      "epoch 125/500\n",
      "----------\n",
      "train loss:  0.1282  acc: 0.9544\n",
      "val loss:  0.1092  acc: 0.9589\n",
      "epoch 126/500\n",
      "----------\n",
      "train loss:  0.1319  acc: 0.9532\n",
      "val loss:  0.1067  acc: 0.9618\n",
      "epoch 127/500\n",
      "----------\n",
      "train loss:  0.1292  acc: 0.9510\n",
      "val loss:  0.0996  acc: 0.9651\n",
      "epoch 128/500\n",
      "----------\n",
      "train loss:  0.1246  acc: 0.9568\n",
      "val loss:  0.1006  acc: 0.9647\n",
      "epoch 129/500\n",
      "----------\n",
      "train loss:  0.1300  acc: 0.9516\n",
      "val loss:  0.1069  acc: 0.9610\n",
      "epoch 130/500\n",
      "----------\n",
      "train loss:  0.1302  acc: 0.9534\n",
      "val loss:  0.1079  acc: 0.9622\n",
      "epoch 131/500\n",
      "----------\n",
      "train loss:  0.1294  acc: 0.9550\n",
      "val loss:  0.0979  acc: 0.9660\n",
      "epoch 132/500\n",
      "----------\n",
      "train loss:  0.1259  acc: 0.9546\n",
      "val loss:  0.1064  acc: 0.9626\n",
      "epoch 133/500\n",
      "----------\n",
      "train loss:  0.1314  acc: 0.9513\n",
      "val loss:  0.1030  acc: 0.9639\n",
      "epoch 134/500\n",
      "----------\n",
      "train loss:  0.1345  acc: 0.9516\n",
      "val loss:  0.1077  acc: 0.9635\n",
      "epoch 135/500\n",
      "----------\n",
      "train loss:  0.1286  acc: 0.9546\n",
      "val loss:  0.1089  acc: 0.9593\n",
      "epoch 136/500\n",
      "----------\n",
      "train loss:  0.1299  acc: 0.9543\n",
      "val loss:  0.0960  acc: 0.9676\n",
      "epoch 137/500\n",
      "----------\n",
      "train loss:  0.1322  acc: 0.9531\n",
      "val loss:  0.1322  acc: 0.9456\n",
      "epoch 138/500\n",
      "----------\n",
      "train loss:  0.1346  acc: 0.9508\n",
      "val loss:  0.0978  acc: 0.9660\n",
      "epoch 139/500\n",
      "----------\n",
      "train loss:  0.1268  acc: 0.9532\n",
      "val loss:  0.1035  acc: 0.9643\n",
      "epoch 140/500\n",
      "----------\n",
      "train loss:  0.1328  acc: 0.9525\n",
      "val loss:  0.1063  acc: 0.9643\n",
      "epoch 141/500\n",
      "----------\n",
      "train loss:  0.1311  acc: 0.9513\n",
      "val loss:  0.0971  acc: 0.9664\n",
      "epoch 142/500\n",
      "----------\n",
      "train loss:  0.1269  acc: 0.9554\n",
      "val loss:  0.1129  acc: 0.9601\n",
      "epoch 143/500\n",
      "----------\n",
      "train loss:  0.1281  acc: 0.9525\n",
      "val loss:  0.1101  acc: 0.9589\n",
      "epoch 144/500\n",
      "----------\n",
      "train loss:  0.1308  acc: 0.9555\n",
      "val loss:  0.1245  acc: 0.9535\n",
      "epoch 145/500\n",
      "----------\n",
      "train loss:  0.1360  acc: 0.9504\n",
      "val loss:  0.1028  acc: 0.9631\n",
      "epoch 146/500\n",
      "----------\n",
      "train loss:  0.1315  acc: 0.9514\n",
      "val loss:  0.1033  acc: 0.9664\n",
      "epoch 147/500\n",
      "----------\n",
      "train loss:  0.1277  acc: 0.9556\n",
      "val loss:  0.1088  acc: 0.9606\n",
      "epoch 148/500\n",
      "----------\n",
      "train loss:  0.1327  acc: 0.9543\n",
      "val loss:  0.0968  acc: 0.9680\n",
      "epoch 149/500\n",
      "----------\n",
      "train loss:  0.1307  acc: 0.9526\n",
      "val loss:  0.1075  acc: 0.9606\n",
      "epoch 150/500\n",
      "----------\n",
      "train loss:  0.1330  acc: 0.9530\n",
      "val loss:  0.1038  acc: 0.9651\n",
      "epoch 151/500\n",
      "----------\n",
      "train loss:  0.1324  acc: 0.9511\n",
      "val loss:  0.1080  acc: 0.9597\n",
      "epoch 152/500\n",
      "----------\n",
      "train loss:  0.1331  acc: 0.9525\n",
      "val loss:  0.1082  acc: 0.9593\n",
      "epoch 153/500\n",
      "----------\n",
      "train loss:  0.1267  acc: 0.9549\n",
      "val loss:  0.1161  acc: 0.9572\n",
      "epoch 154/500\n",
      "----------\n",
      "train loss:  0.1292  acc: 0.9519\n",
      "val loss:  0.1091  acc: 0.9618\n",
      "epoch 155/500\n",
      "----------\n",
      "train loss:  0.1325  acc: 0.9521\n",
      "val loss:  0.1072  acc: 0.9606\n",
      "epoch 156/500\n",
      "----------\n",
      "train loss:  0.1321  acc: 0.9532\n",
      "val loss:  0.1025  acc: 0.9614\n",
      "epoch 157/500\n",
      "----------\n",
      "train loss:  0.1376  acc: 0.9526\n",
      "val loss:  0.1069  acc: 0.9593\n",
      "epoch 158/500\n",
      "----------\n",
      "train loss:  0.1324  acc: 0.9544\n",
      "val loss:  0.1050  acc: 0.9635\n",
      "epoch 159/500\n",
      "----------\n",
      "train loss:  0.1256  acc: 0.9548\n",
      "val loss:  0.1107  acc: 0.9622\n",
      "epoch 160/500\n",
      "----------\n",
      "train loss:  0.1261  acc: 0.9561\n",
      "val loss:  0.1064  acc: 0.9643\n",
      "epoch 161/500\n",
      "----------\n",
      "train loss:  0.1211  acc: 0.9564\n",
      "val loss:  0.1110  acc: 0.9597\n",
      "epoch 162/500\n",
      "----------\n",
      "train loss:  0.1307  acc: 0.9527\n",
      "val loss:  0.1024  acc: 0.9618\n",
      "epoch 163/500\n",
      "----------\n",
      "train loss:  0.1289  acc: 0.9543\n",
      "val loss:  0.1061  acc: 0.9635\n",
      "epoch 164/500\n",
      "----------\n",
      "train loss:  0.1274  acc: 0.9530\n",
      "val loss:  0.1119  acc: 0.9618\n",
      "epoch 165/500\n",
      "----------\n",
      "train loss:  0.1294  acc: 0.9529\n",
      "val loss:  0.1020  acc: 0.9660\n",
      "epoch 166/500\n",
      "----------\n",
      "train loss:  0.1321  acc: 0.9488\n",
      "val loss:  0.1103  acc: 0.9626\n",
      "epoch 167/500\n",
      "----------\n",
      "train loss:  0.1329  acc: 0.9515\n",
      "val loss:  0.1092  acc: 0.9614\n",
      "epoch 168/500\n",
      "----------\n",
      "train loss:  0.1308  acc: 0.9537\n",
      "val loss:  0.1163  acc: 0.9585\n",
      "epoch 169/500\n",
      "----------\n",
      "train loss:  0.1307  acc: 0.9535\n",
      "val loss:  0.1126  acc: 0.9601\n",
      "epoch 170/500\n",
      "----------\n",
      "train loss:  0.1330  acc: 0.9521\n",
      "val loss:  0.1094  acc: 0.9618\n",
      "epoch 171/500\n",
      "----------\n",
      "train loss:  0.1273  acc: 0.9529\n",
      "val loss:  0.1103  acc: 0.9610\n",
      "epoch 172/500\n",
      "----------\n",
      "train loss:  0.1287  acc: 0.9545\n",
      "val loss:  0.1013  acc: 0.9618\n",
      "epoch 173/500\n",
      "----------\n",
      "train loss:  0.1298  acc: 0.9540\n",
      "val loss:  0.1104  acc: 0.9597\n",
      "epoch 174/500\n",
      "----------\n",
      "train loss:  0.1276  acc: 0.9555\n",
      "val loss:  0.0997  acc: 0.9685\n",
      "epoch 175/500\n",
      "----------\n",
      "train loss:  0.1362  acc: 0.9486\n",
      "val loss:  0.1004  acc: 0.9676\n",
      "epoch 176/500\n",
      "----------\n",
      "train loss:  0.1279  acc: 0.9574\n",
      "val loss:  0.1065  acc: 0.9614\n",
      "epoch 177/500\n",
      "----------\n",
      "train loss:  0.1359  acc: 0.9511\n",
      "val loss:  0.1098  acc: 0.9610\n",
      "epoch 178/500\n",
      "----------\n",
      "train loss:  0.1308  acc: 0.9508\n",
      "val loss:  0.0985  acc: 0.9660\n",
      "epoch 179/500\n",
      "----------\n",
      "train loss:  0.1254  acc: 0.9576\n",
      "val loss:  0.1065  acc: 0.9622\n",
      "epoch 180/500\n",
      "----------\n",
      "train loss:  0.1305  acc: 0.9518\n",
      "val loss:  0.1134  acc: 0.9593\n",
      "epoch 181/500\n",
      "----------\n",
      "train loss:  0.1254  acc: 0.9547\n",
      "val loss:  0.1038  acc: 0.9622\n",
      "epoch 182/500\n",
      "----------\n",
      "train loss:  0.1262  acc: 0.9540\n",
      "val loss:  0.1086  acc: 0.9618\n",
      "epoch 183/500\n",
      "----------\n",
      "train loss:  0.1366  acc: 0.9509\n",
      "val loss:  0.1218  acc: 0.9523\n",
      "epoch 184/500\n",
      "----------\n",
      "train loss:  0.1294  acc: 0.9535\n",
      "val loss:  0.1023  acc: 0.9651\n",
      "epoch 185/500\n",
      "----------\n",
      "train loss:  0.1248  acc: 0.9562\n",
      "val loss:  0.1083  acc: 0.9610\n",
      "epoch 186/500\n",
      "----------\n",
      "train loss:  0.1243  acc: 0.9555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:  0.1123  acc: 0.9585\n",
      "epoch 187/500\n",
      "----------\n",
      "train loss:  0.1241  acc: 0.9539\n",
      "val loss:  0.1006  acc: 0.9664\n",
      "epoch 188/500\n",
      "----------\n",
      "train loss:  0.1333  acc: 0.9506\n",
      "val loss:  0.1071  acc: 0.9597\n",
      "epoch 189/500\n",
      "----------\n",
      "train loss:  0.1282  acc: 0.9560\n",
      "val loss:  0.1053  acc: 0.9614\n",
      "epoch 190/500\n",
      "----------\n",
      "train loss:  0.1367  acc: 0.9504\n",
      "val loss:  0.0996  acc: 0.9664\n",
      "epoch 191/500\n",
      "----------\n",
      "train loss:  0.1323  acc: 0.9546\n",
      "val loss:  0.1006  acc: 0.9660\n",
      "epoch 192/500\n",
      "----------\n",
      "train loss:  0.1280  acc: 0.9531\n",
      "val loss:  0.1146  acc: 0.9572\n",
      "epoch 193/500\n",
      "----------\n",
      "train loss:  0.1333  acc: 0.9521\n",
      "val loss:  0.1054  acc: 0.9618\n",
      "epoch 194/500\n",
      "----------\n",
      "train loss:  0.1248  acc: 0.9560\n",
      "val loss:  0.1085  acc: 0.9614\n",
      "epoch 195/500\n",
      "----------\n",
      "train loss:  0.1323  acc: 0.9530\n",
      "val loss:  0.1080  acc: 0.9601\n",
      "epoch 196/500\n",
      "----------\n",
      "train loss:  0.1348  acc: 0.9503\n",
      "val loss:  0.1054  acc: 0.9618\n",
      "epoch 197/500\n",
      "----------\n",
      "train loss:  0.1321  acc: 0.9525\n",
      "val loss:  0.1102  acc: 0.9589\n",
      "epoch 198/500\n",
      "----------\n",
      "train loss:  0.1324  acc: 0.9530\n",
      "val loss:  0.1395  acc: 0.9465\n",
      "epoch 199/500\n",
      "----------\n",
      "train loss:  0.1317  acc: 0.9526\n",
      "val loss:  0.1116  acc: 0.9601\n",
      "epoch 200/500\n",
      "----------\n",
      "train loss:  0.1310  acc: 0.9508\n",
      "val loss:  0.1134  acc: 0.9581\n",
      "epoch 201/500\n",
      "----------\n",
      "train loss:  0.1243  acc: 0.9523\n",
      "val loss:  0.1091  acc: 0.9614\n",
      "epoch 202/500\n",
      "----------\n",
      "train loss:  0.1275  acc: 0.9557\n",
      "val loss:  0.1053  acc: 0.9631\n",
      "epoch 203/500\n",
      "----------\n",
      "train loss:  0.1326  acc: 0.9535\n",
      "val loss:  0.1072  acc: 0.9614\n",
      "epoch 204/500\n",
      "----------\n",
      "train loss:  0.1293  acc: 0.9514\n",
      "val loss:  0.1054  acc: 0.9626\n",
      "epoch 205/500\n",
      "----------\n",
      "train loss:  0.1287  acc: 0.9527\n",
      "val loss:  0.1032  acc: 0.9635\n",
      "epoch 206/500\n",
      "----------\n",
      "train loss:  0.1292  acc: 0.9513\n",
      "val loss:  0.1122  acc: 0.9589\n",
      "epoch 207/500\n",
      "----------\n",
      "train loss:  0.1283  acc: 0.9531\n",
      "val loss:  0.1032  acc: 0.9622\n",
      "epoch 208/500\n",
      "----------\n",
      "train loss:  0.1335  acc: 0.9516\n",
      "val loss:  0.1052  acc: 0.9626\n",
      "epoch 209/500\n",
      "----------\n",
      "train loss:  0.1282  acc: 0.9529\n",
      "val loss:  0.1055  acc: 0.9651\n",
      "epoch 210/500\n",
      "----------\n",
      "train loss:  0.1351  acc: 0.9510\n",
      "val loss:  0.1105  acc: 0.9622\n",
      "epoch 211/500\n",
      "----------\n",
      "train loss:  0.1340  acc: 0.9508\n",
      "val loss:  0.1123  acc: 0.9589\n",
      "epoch 212/500\n",
      "----------\n",
      "train loss:  0.1300  acc: 0.9520\n",
      "val loss:  0.1024  acc: 0.9626\n",
      "epoch 213/500\n",
      "----------\n",
      "train loss:  0.1352  acc: 0.9514\n",
      "val loss:  0.1025  acc: 0.9643\n",
      "epoch 214/500\n",
      "----------\n",
      "train loss:  0.1335  acc: 0.9526\n",
      "val loss:  0.0989  acc: 0.9664\n",
      "epoch 215/500\n",
      "----------\n",
      "train loss:  0.1295  acc: 0.9527\n",
      "val loss:  0.0992  acc: 0.9676\n",
      "epoch 216/500\n",
      "----------\n",
      "train loss:  0.1294  acc: 0.9536\n",
      "val loss:  0.0973  acc: 0.9672\n",
      "epoch 217/500\n",
      "----------\n",
      "train loss:  0.1323  acc: 0.9508\n",
      "val loss:  0.1089  acc: 0.9639\n",
      "epoch 218/500\n",
      "----------\n",
      "train loss:  0.1304  acc: 0.9541\n",
      "val loss:  0.1048  acc: 0.9655\n",
      "epoch 219/500\n",
      "----------\n",
      "train loss:  0.1299  acc: 0.9526\n",
      "val loss:  0.1077  acc: 0.9622\n",
      "epoch 220/500\n",
      "----------\n",
      "train loss:  0.1278  acc: 0.9537\n",
      "val loss:  0.0986  acc: 0.9680\n",
      "epoch 221/500\n",
      "----------\n",
      "train loss:  0.1319  acc: 0.9550\n",
      "val loss:  0.1068  acc: 0.9626\n",
      "epoch 222/500\n",
      "----------\n",
      "train loss:  0.1297  acc: 0.9535\n",
      "val loss:  0.1004  acc: 0.9660\n",
      "epoch 223/500\n",
      "----------\n",
      "train loss:  0.1214  acc: 0.9569\n",
      "val loss:  0.1159  acc: 0.9585\n",
      "epoch 224/500\n",
      "----------\n",
      "train loss:  0.1320  acc: 0.9529\n",
      "val loss:  0.1160  acc: 0.9552\n",
      "epoch 225/500\n",
      "----------\n",
      "train loss:  0.1297  acc: 0.9540\n",
      "val loss:  0.1094  acc: 0.9618\n",
      "epoch 226/500\n",
      "----------\n",
      "train loss:  0.1305  acc: 0.9535\n",
      "val loss:  0.0997  acc: 0.9664\n",
      "epoch 227/500\n",
      "----------\n",
      "train loss:  0.1343  acc: 0.9528\n",
      "val loss:  0.1111  acc: 0.9622\n",
      "epoch 228/500\n",
      "----------\n",
      "train loss:  0.1321  acc: 0.9521\n",
      "val loss:  0.1129  acc: 0.9577\n",
      "epoch 229/500\n",
      "----------\n",
      "train loss:  0.1327  acc: 0.9512\n",
      "val loss:  0.1118  acc: 0.9593\n",
      "epoch 230/500\n",
      "----------\n",
      "train loss:  0.1344  acc: 0.9495\n",
      "val loss:  0.1167  acc: 0.9564\n",
      "epoch 231/500\n",
      "----------\n",
      "train loss:  0.1338  acc: 0.9509\n",
      "val loss:  0.0952  acc: 0.9676\n",
      "epoch 232/500\n",
      "----------\n",
      "train loss:  0.1315  acc: 0.9518\n",
      "val loss:  0.1047  acc: 0.9639\n",
      "epoch 233/500\n",
      "----------\n",
      "train loss:  0.1253  acc: 0.9549\n",
      "val loss:  0.1069  acc: 0.9626\n",
      "epoch 234/500\n",
      "----------\n",
      "train loss:  0.1275  acc: 0.9533\n",
      "val loss:  0.0973  acc: 0.9672\n",
      "epoch 235/500\n",
      "----------\n",
      "train loss:  0.1287  acc: 0.9532\n",
      "val loss:  0.1147  acc: 0.9560\n",
      "epoch 236/500\n",
      "----------\n",
      "train loss:  0.1276  acc: 0.9539\n",
      "val loss:  0.1173  acc: 0.9568\n",
      "epoch 237/500\n",
      "----------\n",
      "train loss:  0.1271  acc: 0.9549\n",
      "val loss:  0.1110  acc: 0.9631\n",
      "epoch 238/500\n",
      "----------\n",
      "train loss:  0.1241  acc: 0.9548\n",
      "val loss:  0.0979  acc: 0.9664\n",
      "epoch 239/500\n",
      "----------\n",
      "train loss:  0.1296  acc: 0.9528\n",
      "val loss:  0.1035  acc: 0.9622\n",
      "epoch 240/500\n",
      "----------\n",
      "train loss:  0.1280  acc: 0.9543\n",
      "val loss:  0.1153  acc: 0.9593\n",
      "epoch 241/500\n",
      "----------\n",
      "train loss:  0.1360  acc: 0.9495\n",
      "val loss:  0.1060  acc: 0.9651\n",
      "epoch 242/500\n",
      "----------\n",
      "train loss:  0.1274  acc: 0.9528\n",
      "val loss:  0.1072  acc: 0.9631\n",
      "epoch 243/500\n",
      "----------\n",
      "train loss:  0.1273  acc: 0.9533\n",
      "val loss:  0.1030  acc: 0.9635\n",
      "epoch 244/500\n",
      "----------\n",
      "train loss:  0.1255  acc: 0.9561\n",
      "val loss:  0.1034  acc: 0.9610\n",
      "epoch 245/500\n",
      "----------\n",
      "train loss:  0.1305  acc: 0.9540\n",
      "val loss:  0.1081  acc: 0.9610\n",
      "epoch 246/500\n",
      "----------\n",
      "train loss:  0.1337  acc: 0.9521\n",
      "val loss:  0.1068  acc: 0.9597\n",
      "epoch 247/500\n",
      "----------\n",
      "train loss:  0.1307  acc: 0.9512\n",
      "val loss:  0.1118  acc: 0.9614\n",
      "epoch 248/500\n",
      "----------\n",
      "train loss:  0.1339  acc: 0.9513\n",
      "val loss:  0.1104  acc: 0.9593\n",
      "epoch 249/500\n",
      "----------\n",
      "train loss:  0.1252  acc: 0.9555\n",
      "val loss:  0.1107  acc: 0.9585\n",
      "epoch 250/500\n",
      "----------\n",
      "train loss:  0.1328  acc: 0.9513\n",
      "val loss:  0.1056  acc: 0.9651\n",
      "epoch 251/500\n",
      "----------\n",
      "train loss:  0.1312  acc: 0.9520\n",
      "val loss:  0.1121  acc: 0.9606\n",
      "epoch 252/500\n",
      "----------\n",
      "train loss:  0.1271  acc: 0.9536\n",
      "val loss:  0.1080  acc: 0.9631\n",
      "epoch 253/500\n",
      "----------\n",
      "train loss:  0.1365  acc: 0.9490\n",
      "val loss:  0.0974  acc: 0.9680\n",
      "epoch 254/500\n",
      "----------\n",
      "train loss:  0.1369  acc: 0.9483\n",
      "val loss:  0.1075  acc: 0.9622\n",
      "epoch 255/500\n",
      "----------\n",
      "train loss:  0.1297  acc: 0.9546\n",
      "val loss:  0.1075  acc: 0.9639\n",
      "epoch 256/500\n",
      "----------\n",
      "train loss:  0.1323  acc: 0.9507\n",
      "val loss:  0.1219  acc: 0.9572\n",
      "epoch 257/500\n",
      "----------\n",
      "train loss:  0.1274  acc: 0.9541\n",
      "val loss:  0.1081  acc: 0.9631\n",
      "epoch 258/500\n",
      "----------\n",
      "train loss:  0.1320  acc: 0.9532\n",
      "val loss:  0.1159  acc: 0.9560\n",
      "epoch 259/500\n",
      "----------\n",
      "train loss:  0.1380  acc: 0.9515\n",
      "val loss:  0.1216  acc: 0.9560\n",
      "epoch 260/500\n",
      "----------\n",
      "train loss:  0.1300  acc: 0.9541\n",
      "val loss:  0.1023  acc: 0.9647\n",
      "epoch 261/500\n",
      "----------\n",
      "train loss:  0.1283  acc: 0.9548\n",
      "val loss:  0.1131  acc: 0.9597\n",
      "epoch 262/500\n",
      "----------\n",
      "train loss:  0.1339  acc: 0.9532\n",
      "val loss:  0.1047  acc: 0.9626\n",
      "epoch 263/500\n",
      "----------\n",
      "train loss:  0.1319  acc: 0.9530\n",
      "val loss:  0.1062  acc: 0.9622\n",
      "epoch 264/500\n",
      "----------\n",
      "train loss:  0.1262  acc: 0.9546\n",
      "val loss:  0.1076  acc: 0.9643\n",
      "epoch 265/500\n",
      "----------\n",
      "train loss:  0.1261  acc: 0.9538\n",
      "val loss:  0.1170  acc: 0.9585\n",
      "epoch 266/500\n",
      "----------\n",
      "train loss:  0.1299  acc: 0.9534\n",
      "val loss:  0.1158  acc: 0.9581\n",
      "epoch 267/500\n",
      "----------\n",
      "train loss:  0.1275  acc: 0.9530\n",
      "val loss:  0.1044  acc: 0.9622\n",
      "epoch 268/500\n",
      "----------\n",
      "train loss:  0.1264  acc: 0.9534\n",
      "val loss:  0.1081  acc: 0.9626\n",
      "epoch 269/500\n",
      "----------\n",
      "train loss:  0.1311  acc: 0.9523\n",
      "val loss:  0.1023  acc: 0.9660\n",
      "epoch 270/500\n",
      "----------\n",
      "train loss:  0.1264  acc: 0.9515\n",
      "val loss:  0.1211  acc: 0.9531\n",
      "epoch 271/500\n",
      "----------\n",
      "train loss:  0.1273  acc: 0.9541\n",
      "val loss:  0.1118  acc: 0.9577\n",
      "epoch 272/500\n",
      "----------\n",
      "train loss:  0.1266  acc: 0.9546\n",
      "val loss:  0.1092  acc: 0.9601\n",
      "epoch 273/500\n",
      "----------\n",
      "train loss:  0.1297  acc: 0.9516\n",
      "val loss:  0.1160  acc: 0.9606\n",
      "epoch 274/500\n",
      "----------\n",
      "train loss:  0.1300  acc: 0.9512\n",
      "val loss:  0.1018  acc: 0.9655\n",
      "epoch 275/500\n",
      "----------\n",
      "train loss:  0.1350  acc: 0.9507\n",
      "val loss:  0.1091  acc: 0.9606\n",
      "epoch 276/500\n",
      "----------\n",
      "train loss:  0.1356  acc: 0.9507\n",
      "val loss:  0.1180  acc: 0.9572\n",
      "epoch 277/500\n",
      "----------\n",
      "train loss:  0.1313  acc: 0.9521\n",
      "val loss:  0.1080  acc: 0.9606\n",
      "epoch 278/500\n",
      "----------\n",
      "train loss:  0.1311  acc: 0.9529\n",
      "val loss:  0.1049  acc: 0.9626\n",
      "epoch 279/500\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  0.1260  acc: 0.9560\n",
      "val loss:  0.1043  acc: 0.9635\n",
      "epoch 280/500\n",
      "----------\n",
      "train loss:  0.1308  acc: 0.9523\n",
      "val loss:  0.1057  acc: 0.9639\n",
      "epoch 281/500\n",
      "----------\n",
      "train loss:  0.1300  acc: 0.9541\n",
      "val loss:  0.0948  acc: 0.9685\n",
      "epoch 282/500\n",
      "----------\n",
      "train loss:  0.1354  acc: 0.9503\n",
      "val loss:  0.0990  acc: 0.9655\n",
      "epoch 283/500\n",
      "----------\n",
      "train loss:  0.1347  acc: 0.9512\n",
      "val loss:  0.1102  acc: 0.9606\n",
      "epoch 284/500\n",
      "----------\n",
      "train loss:  0.1294  acc: 0.9536\n",
      "val loss:  0.1009  acc: 0.9672\n",
      "epoch 285/500\n",
      "----------\n",
      "train loss:  0.1235  acc: 0.9556\n",
      "val loss:  0.1087  acc: 0.9610\n",
      "epoch 286/500\n",
      "----------\n",
      "train loss:  0.1344  acc: 0.9513\n",
      "val loss:  0.0982  acc: 0.9660\n",
      "epoch 287/500\n",
      "----------\n",
      "train loss:  0.1306  acc: 0.9522\n",
      "val loss:  0.1084  acc: 0.9631\n",
      "epoch 288/500\n",
      "----------\n",
      "train loss:  0.1263  acc: 0.9523\n",
      "val loss:  0.1133  acc: 0.9606\n",
      "epoch 289/500\n",
      "----------\n",
      "train loss:  0.1371  acc: 0.9515\n",
      "val loss:  0.0981  acc: 0.9672\n",
      "epoch 290/500\n",
      "----------\n",
      "train loss:  0.1306  acc: 0.9510\n",
      "val loss:  0.0966  acc: 0.9660\n",
      "epoch 291/500\n",
      "----------\n",
      "train loss:  0.1279  acc: 0.9535\n",
      "val loss:  0.1034  acc: 0.9643\n",
      "epoch 292/500\n",
      "----------\n",
      "train loss:  0.1294  acc: 0.9530\n",
      "val loss:  0.1053  acc: 0.9626\n",
      "epoch 293/500\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "##### RESNET\n",
    "##Build model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = torchvision.models.resnet152(pretrained=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.model.fc.in_features,3),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        for params in self.model.parameters():\n",
    "            params.requires_grad = True\n",
    "        self.model.fc = self.classifier\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def fit(self, dataloaders, num_epochs):\n",
    "        train_on_gpu = torch.cuda.is_available()\n",
    "        optimizer = optim.Adam(self.model.fc.parameters())\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, 4)\n",
    "        criterion = nn.NLLLoss()\n",
    "        since = time.time()\n",
    "        \n",
    "        best_model_wts = copy.deepcopy(self.model.state_dict())\n",
    "        best_acc = 0.0\n",
    "        \n",
    "        if train_on_gpu:\n",
    "            self.model = self.model.cuda()\n",
    "            \n",
    "        for epoch in range(1, num_epochs+1):\n",
    "            print(\"epoch {}/{}\".format(epoch, num_epochs))\n",
    "            print(\"-\" * 10)\n",
    "            \n",
    "            for phase in ['train','val']:\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "                    self.model.train()\n",
    "                else:\n",
    "                    self.model.eval()\n",
    "                \n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0.0\n",
    "                \n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    if train_on_gpu:\n",
    "                        inputs = inputs.cuda()\n",
    "                        labels = labels.cuda()\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = self.model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        \n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                    \n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "                print(\"{} loss:  {:.4f}  acc: {:.4f}\".format(phase, epoch_loss, epoch_acc))\n",
    "                \n",
    "                if phase == 'val' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(self.model.state_dict())\n",
    "        \n",
    "        time_elapsed = time.time() - since\n",
    "        print('time completed: {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 600))\n",
    "        print(\"best val acc: {:.4f}\".format(best_acc))\n",
    "        \n",
    "        self.model.load_state_dict(best_model_wts)\n",
    "        return self.model\n",
    "    \n",
    "model = Model()\n",
    "model_ft = model.fit(dataloaders,500)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "nb_classes = 3\n",
    "\n",
    "# Initialize the prediction and label lists(tensors)\n",
    "predlist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "lbllist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, classes) in enumerate(dataloaders['val']):\n",
    "        inputs = inputs.to(device) #labels atuais\n",
    "        classes = classes.to(device) #classes\n",
    "        outputs = model_ft(inputs) #valores preditos = Passa o label atual e retorna o que o modelo predice\n",
    "        _, preds = torch.max(outputs, 1) #pega o maior valor das predições\n",
    "\n",
    "        # Append batch prediction results\n",
    "        predlist=torch.cat([predlist,preds.view(-1).cpu()])\n",
    "        lbllist=torch.cat([lbllist,classes.view(-1).cpu()])\n",
    "\n",
    "# Confusion matrix\n",
    "conf_mat=confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
    "print(conf_mat)\n",
    "print()\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "#analise dos resultados do modelo\n",
    "print('Sensitivity or recall total')\n",
    "print (metrics.recall_score(lbllist.numpy(), predlist.numpy(), average='micro'))\n",
    "\n",
    "print()\n",
    "print('Sensitivity or recall per classes')\n",
    "print (metrics.recall_score(lbllist.numpy(), predlist.numpy(), average=None))\n",
    "\n",
    "print()\n",
    "print('Precision')\n",
    "print (metrics.precision_score(lbllist.numpy(), predlist.numpy(), average=None))\n",
    "\n",
    "print()\n",
    "print('F1 Score')\n",
    "print (metrics.f1_score(lbllist.numpy(), predlist.numpy(), average=None))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "nb_classes = 3\n",
    "\n",
    "# Initialize the prediction and label lists(tensors)\n",
    "predlist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "lbllist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, classes) in enumerate(dataloaders['val']):\n",
    "        inputs = inputs.to(device) #labels atuais\n",
    "        classes = classes.to(device) #classes\n",
    "        outputs = model_ft(inputs) #valores preditos = Passa o label atual e retorna o que o modelo predice\n",
    "        _, preds = torch.max(outputs, 1) #pega o maior valor das predições\n",
    "\n",
    "        # Append batch prediction results\n",
    "        predlist=torch.cat([predlist,preds.view(-1).cpu()])\n",
    "        lbllist=torch.cat([lbllist,classes.view(-1).cpu()])\n",
    "\n",
    "# Confusion matrix\n",
    "conf_mat=confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
    "print(conf_mat)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools \n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plot_confusion_matrix(cm, classes=['norm', 'covid', 'pnemo'], title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_test = \"/home/jimi/dissertacao/covid19/datasets/80-20/test/\"\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      #transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                      #                     [0.229, 0.224, 0.225])\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image):\n",
    "    image_tensor = test_transforms(image).float()\n",
    "    image_tensor = image_tensor.unsqueeze_(0)\n",
    "    input = Variable(image_tensor)\n",
    "    input = input.to(device)\n",
    "    output = model(input)\n",
    "    index = output.data.cpu().numpy().argmax()\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_images(num):\n",
    "    data = datasets.ImageFolder(data_dir_test, transform=test_transforms)\n",
    "    classes = data.classes\n",
    "    indices = list(range(len(data)))\n",
    "    np.random.shuffle(indices)\n",
    "    idx = indices[:num]\n",
    "    from torch.utils.data.sampler import SubsetRandomSampler\n",
    "    sampler = SubsetRandomSampler(idx)\n",
    "    loader = torch.utils.data.DataLoader(data,sampler=sampler, batch_size=num)\n",
    "    dataiter = iter(loader)\n",
    "    images, labels = dataiter.next()\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions\n",
    "to_pil = transforms.ToPILImage()\n",
    "images, labels = get_random_images(30)\n",
    "fig=plt.figure(figsize=(20,20))\n",
    "for ii in range(len(images)):\n",
    "    image = to_pil(images[ii])\n",
    "    index = predict_image(image)\n",
    "    \n",
    "    data = datasets.ImageFolder(data_dir_test, transform=test_transforms)\n",
    "    classes = data.classes\n",
    "    \n",
    "    #print (f'index: {index}')\n",
    "    #print (f'image: {image}')\n",
    "    #print (f'labes: {labels}')\n",
    "    #print (f'classes index :{classes[index]}')\n",
    "    #print (f'classes 2:{classes}')\n",
    "    \n",
    "    sub = fig.add_subplot(8, 4, ii+1)\n",
    "    \n",
    "    #print()\n",
    "    res = int(labels[ii]) == 1\n",
    "    #print(f'int(labels[ii]): {int(labels[ii])}')\n",
    "    #print(f'index: {index}')\n",
    "    #print(f'res = int(labels[ii]) == index: {res}')\n",
    "    #print()\n",
    "    \n",
    "    #print (f'res : {res}')\n",
    "    \n",
    "    sub.set_title(str(classes[1]) + \":\" + str(res))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(),\n",
    "                            transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])])\n",
    "def image_loader(image_name):\n",
    "    image = PIL.Image.open(image_name).convert(\"RGB\")\n",
    "    image = loader(image).float()\n",
    "    image = image.unsqueeze(0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerActivations():\n",
    "    features=[]\n",
    "    def __init__(self,model):\n",
    "        self.hooks = []\n",
    "        self.hooks.append(model.layer4.register_forward_hook(self.hook_fn))\n",
    "    def hook_fn(self,module,input,output):\n",
    "        self.features.append(output)\n",
    "    def remove(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/home/jimi/dissertacao/covid19/datasets/80-20/val/normal/1785.jpg'\n",
    "img = image_loader(image_path).cuda()\n",
    "\n",
    "acts = LayerActivations(model_ft)\n",
    "\n",
    "logps = model_ft(img).cuda()\n",
    "ps = torch.exp(logps)\n",
    "\n",
    "out_features = acts.features[0]\n",
    "out_features = torch.squeeze(out_features, dim=0)\n",
    "out_features = out_features.cpu().detach().numpy()\n",
    "out_features = np.transpose(out_features,axes=(1,2,0))\n",
    "\n",
    "W = model_ft.fc[0].weight\n",
    "top_probs, top_classes = torch.topk(ps, k=3)\n",
    "ps = ps.cpu().detach().numpy()\n",
    "pred = np.argmax(ps)\n",
    "w = W[pred,:]\n",
    "\n",
    "w = w.cpu().detach().numpy()\n",
    "cam = np.dot(out_features, w)\n",
    "#type(w)\n",
    "\n",
    "class_activation = nd.zoom(cam, zoom=(32,32),order=1)\n",
    "\n",
    "img = torch.squeeze(img,0)\n",
    "img = img.cpu().numpy()\n",
    "img = np.transpose(img,(1,2,0))\n",
    "mean = np.array([0.5,0.5,0.5])\n",
    "std =  np.array([0.5,0.5,0.5])\n",
    "#img = img.cpu().numpy()\n",
    "img = (img + mean) * std\n",
    "img = np.clip(img, a_max=1, a_min=0)\n",
    "\n",
    "plt.imshow(class_activation, cmap='jet',alpha=1)\n",
    "plt.imshow(img, alpha=0.55)\n",
    "plt.title(dset['val'].classes[pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/home/jimi/dissertacao/covid19/datasets/80-20/val/normal/860.jpg'\n",
    "img = image_loader(image_path).cuda()\n",
    "\n",
    "acts = LayerActivations(model_ft)\n",
    "\n",
    "logps = model_ft(img).cuda()\n",
    "ps = torch.exp(logps)\n",
    "\n",
    "out_features = acts.features[0]\n",
    "out_features = torch.squeeze(out_features, dim=0)\n",
    "out_features = out_features.cpu().detach().numpy()\n",
    "out_features = np.transpose(out_features,axes=(1,2,0))\n",
    "\n",
    "W = model_ft.fc[0].weight\n",
    "top_probs, top_classes = torch.topk(ps, k=3)\n",
    "ps = ps.cpu().detach().numpy()\n",
    "pred = np.argmax(ps)\n",
    "w = W[pred,:]\n",
    "\n",
    "w = w.cpu().detach().numpy()\n",
    "cam = np.dot(out_features, w)\n",
    "#type(w)\n",
    "\n",
    "class_activation = nd.zoom(cam, zoom=(32,32),order=1)\n",
    "\n",
    "img = torch.squeeze(img,0)\n",
    "img = img.cpu().numpy()\n",
    "img = np.transpose(img,(1,2,0))\n",
    "mean = np.array([0.5,0.5,0.5])\n",
    "std =  np.array([0.5,0.5,0.5])\n",
    "#img = img.cpu().numpy()\n",
    "img = (img + mean) * std\n",
    "img = np.clip(img, a_max=1, a_min=0)\n",
    "\n",
    "plt.imshow(class_activation, cmap='jet',alpha=1)\n",
    "plt.imshow(img, alpha=0.55)\n",
    "plt.title(dset['val'].classes[pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/home/jimi/dissertacao/covid19/datasets/80-20/val/normal/2480.jpg'\n",
    "img = image_loader(image_path).cuda()\n",
    "\n",
    "acts = LayerActivations(model_ft)\n",
    "\n",
    "logps = model_ft(img).cuda()\n",
    "ps = torch.exp(logps)\n",
    "\n",
    "out_features = acts.features[0]\n",
    "out_features = torch.squeeze(out_features, dim=0)\n",
    "out_features = out_features.cpu().detach().numpy()\n",
    "out_features = np.transpose(out_features,axes=(1,2,0))\n",
    "\n",
    "W = model_ft.fc[0].weight\n",
    "top_probs, top_classes = torch.topk(ps, k=3)\n",
    "ps = ps.cpu().detach().numpy()\n",
    "pred = np.argmax(ps)\n",
    "w = W[pred,:]\n",
    "\n",
    "w = w.cpu().detach().numpy()\n",
    "cam = np.dot(out_features, w)\n",
    "#type(w)\n",
    "\n",
    "class_activation = nd.zoom(cam, zoom=(32,32),order=1)\n",
    "\n",
    "img = torch.squeeze(img,0)\n",
    "img = img.cpu().numpy()\n",
    "img = np.transpose(img,(1,2,0))\n",
    "mean = np.array([0.5,0.5,0.5])\n",
    "std =  np.array([0.5,0.5,0.5])\n",
    "#img = img.cpu().numpy()\n",
    "img = (img + mean) * std\n",
    "img = np.clip(img, a_max=1, a_min=0)\n",
    "\n",
    "plt.imshow(class_activation, cmap='jet',alpha=1)\n",
    "plt.imshow(img, alpha=0.55)\n",
    "plt.title(dset['val'].classes[pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/home/jimi/dissertacao/covid19/datasets/80-20/test/covid/000001.png'\n",
    "img = image_loader(image_path).cuda()\n",
    "\n",
    "acts = LayerActivations(model_ft)\n",
    "\n",
    "logps = model_ft(img).cuda()\n",
    "ps = torch.exp(logps)\n",
    "\n",
    "out_features = acts.features[0]\n",
    "out_features = torch.squeeze(out_features, dim=0)\n",
    "out_features = out_features.cpu().detach().numpy()\n",
    "out_features = np.transpose(out_features,axes=(1,2,0))\n",
    "\n",
    "W = model_ft.fc[0].weight\n",
    "top_probs, top_classes = torch.topk(ps, k=3)\n",
    "ps = ps.cpu().detach().numpy()\n",
    "pred = np.argmax(ps)\n",
    "w = W[pred,:]\n",
    "\n",
    "w = w.cpu().detach().numpy()\n",
    "cam = np.dot(out_features, w)\n",
    "#type(w)\n",
    "\n",
    "class_activation = nd.zoom(cam, zoom=(32,32),order=1)\n",
    "\n",
    "img = torch.squeeze(img,0)\n",
    "img = img.cpu().numpy()\n",
    "img = np.transpose(img,(1,2,0))\n",
    "mean = np.array([0.5,0.5,0.5])\n",
    "std =  np.array([0.5,0.5,0.5])\n",
    "#img = img.cpu().numpy()\n",
    "img = (img + mean) * std\n",
    "img = np.clip(img, a_max=1, a_min=0)\n",
    "\n",
    "plt.imshow(class_activation, cmap='jet',alpha=1)\n",
    "plt.imshow(img, alpha=0.55)\n",
    "plt.title(dset['val'].classes[pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/home/jimi/dissertacao/covid19/datasets/80-20/test/covid/000001-9-a.jpg'\n",
    "img = image_loader(image_path).cuda()\n",
    "\n",
    "acts = LayerActivations(model_ft)\n",
    "\n",
    "logps = model_ft(img).cuda()\n",
    "ps = torch.exp(logps)\n",
    "\n",
    "out_features = acts.features[0]\n",
    "out_features = torch.squeeze(out_features, dim=0)\n",
    "out_features = out_features.cpu().detach().numpy()\n",
    "out_features = np.transpose(out_features,axes=(1,2,0))\n",
    "\n",
    "W = model_ft.fc[0].weight\n",
    "top_probs, top_classes = torch.topk(ps, k=3)\n",
    "ps = ps.cpu().detach().numpy()\n",
    "pred = np.argmax(ps)\n",
    "w = W[pred,:]\n",
    "\n",
    "w = w.cpu().detach().numpy()\n",
    "cam = np.dot(out_features, w)\n",
    "#type(w)\n",
    "\n",
    "class_activation = nd.zoom(cam, zoom=(32,32),order=1)\n",
    "\n",
    "img = torch.squeeze(img,0)\n",
    "img = img.cpu().numpy()\n",
    "img = np.transpose(img,(1,2,0))\n",
    "mean = np.array([0.5,0.5,0.5])\n",
    "std =  np.array([0.5,0.5,0.5])\n",
    "#img = img.cpu().numpy()\n",
    "img = (img + mean) * std\n",
    "img = np.clip(img, a_max=1, a_min=0)\n",
    "\n",
    "plt.imshow(class_activation, cmap='jet',alpha=1)\n",
    "plt.imshow(img, alpha=0.55)\n",
    "plt.title(dset['val'].classes[pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/home/jimi/dissertacao/covid19/datasets/80-20/test/covid/000001-10.jpg'\n",
    "img = image_loader(image_path).cuda()\n",
    "\n",
    "acts = LayerActivations(model_ft)\n",
    "\n",
    "logps = model_ft(img).cuda()\n",
    "ps = torch.exp(logps)\n",
    "\n",
    "out_features = acts.features[0]\n",
    "out_features = torch.squeeze(out_features, dim=0)\n",
    "out_features = out_features.cpu().detach().numpy()\n",
    "out_features = np.transpose(out_features,axes=(1,2,0))\n",
    "\n",
    "W = model_ft.fc[0].weight\n",
    "top_probs, top_classes = torch.topk(ps, k=3)\n",
    "ps = ps.cpu().detach().numpy()\n",
    "pred = np.argmax(ps)\n",
    "w = W[pred,:]\n",
    "\n",
    "w = w.cpu().detach().numpy()\n",
    "cam = np.dot(out_features, w)\n",
    "#type(w)\n",
    "\n",
    "class_activation = nd.zoom(cam, zoom=(32,32),order=1)\n",
    "\n",
    "img = torch.squeeze(img,0)\n",
    "img = img.cpu().numpy()\n",
    "img = np.transpose(img,(1,2,0))\n",
    "mean = np.array([0.5,0.5,0.5])\n",
    "std =  np.array([0.5,0.5,0.5])\n",
    "#img = img.cpu().numpy()\n",
    "img = (img + mean) * std\n",
    "img = np.clip(img, a_max=1, a_min=0)\n",
    "\n",
    "plt.imshow(class_activation, cmap='jet',alpha=1)\n",
    "plt.imshow(img, alpha=0.55)\n",
    "plt.title(dset['val'].classes[pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 2 FIIMMMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 2 FIIMMMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 2 FIIMMMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 3 ref - https://github.com/ironWolf1990/pytorch-covid19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 3 ref- https://github.com/ironWolf1990/pytorch-covid19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 3 ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 3 ref- https://github.com/ironWolf1990/pytorch-covid19\n",
    "########## CAM try 3 ref\n",
    "########## CAM try 3 ref\n",
    "########## CAM try 3 ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm, gridspec\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "def plot_with_labels(lowDWeights, labels):\n",
    "    plt.cla()\n",
    "    X, Y = lowDWeights[:, 0], lowDWeights[:, 1]\n",
    "    for x, y, s in zip(X, Y, labels):\n",
    "        cmap = cm.get_cmap(\"rainbow\")\n",
    "        c = cmap(int(255 * s / 9))\n",
    "        plt.text(x, y, s, backgroundcolor=c, fontsize=9)\n",
    "    plt.xlim(X.min(), X.max())\n",
    "    plt.ylim(Y.min(), Y.max())\n",
    "    plt.show()\n",
    "    plt.pause(0.01)\n",
    "\n",
    "def data_viz(layer, label):\n",
    "    # https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents-notebooks/401_CNN.ipynb\n",
    "    tsne = TSNE(perplexity=30, n_components=2, init=\"pca\", n_iter=5000)\n",
    "    plot_only = 500\n",
    "    low_dim_embs = tsne.fit_transform(layer.data.numpy()[:plot_only, :])\n",
    "    labels = label.numpy()[:plot_only]\n",
    "    plot_with_labels(low_dim_embs, labels)\n",
    "\n",
    "\n",
    "def plot_test_image_result(img, ps, le, cam=None):\n",
    "\n",
    "    _ = plt.figure(figsize=(8, 6))\n",
    "    gs = gridspec.GridSpec(1, 2, width_ratios=[3, 1])\n",
    "    ax1, ax2 = plt.subplot(gs[0]), plt.subplot(gs[1])\n",
    "\n",
    "    if cam is not None:\n",
    "        ax1.imshow(cam, alpha=0.6)\n",
    "        ax1.imshow(img, alpha=0.4)\n",
    "    else:\n",
    "        ax1.imshow(img)\n",
    "\n",
    "    ax2.barh(np.arange(len(ps)), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(len(ps)))\n",
    "\n",
    "    for i, v in enumerate(ps):\n",
    "        ax2.text(\n",
    "            .01,\n",
    "            i-0.1,\n",
    "            f'{v:.3f}',\n",
    "            color='blue',\n",
    "            fontweight='bold')\n",
    "\n",
    "    if le is None:\n",
    "        ax2.set_yticklabels(np.arange(len(ps)))\n",
    "    else:\n",
    "        ax2.set_yticklabels(le.inverse_transform(np.arange(len(ps))))\n",
    "\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makedataset.py\n",
    "\n",
    "from os.path import isfile, join\n",
    "from os import walk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import tee\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    path = \"./data\"\n",
    "    sample_per_category = 500\n",
    "    seed = 24\n",
    "    split_frac = 0.20\n",
    "\n",
    "    df_raw = None\n",
    "\n",
    "    genFiles = (\n",
    "        (dirpath, dirnames, filenames) for (dirpath, dirnames, filenames) in walk(path)\n",
    "    )\n",
    "\n",
    "    files, genFiles = tee(genFiles)\n",
    "    file_count = sum(len(f) for _, _, f in files)\n",
    "\n",
    "    df_raw = pd.DataFrame(\n",
    "        data=np.nan, index=np.arange(0, file_count - 1), columns=[\"LABEL\", \"FILE\"]\n",
    "    )\n",
    "\n",
    "    files, genFiles = tee(genFiles)\n",
    "    idx = 0\n",
    "    for r, _, f in files:\n",
    "        for _f in f:\n",
    "            if isfile(join(r, _f)) and _f.endswith(\n",
    "                (\".jpeg\", \".png\", \"jpg\", \".JPEG\", \".PNG\", \"JPG\")\n",
    "            ):\n",
    "                path = \"/\".join((r, _f))\n",
    "                *_, label = r.split(\"/\")\n",
    "                df_raw.iloc[idx] = [label, path]\n",
    "                idx += 1\n",
    "\n",
    "    df_raw.to_csv(\"./data/raw.csv\", index=False)\n",
    "\n",
    "    # # 3-class\n",
    "    df_main = pd.DataFrame(\n",
    "        data=np.nan,\n",
    "        index=np.arange(0, sample_per_category * 3),\n",
    "        columns=[\"FILE\", \"LABEL\"],\n",
    "    )\n",
    "\n",
    "    df_main = df_raw.groupby(\"LABEL\").apply(\n",
    "        lambda s: s.sample(n=min(len(s), sample_per_category), random_state=seed)\n",
    "    )\n",
    "\n",
    "    train_df, test_df = train_test_split(\n",
    "        df_main, random_state=seed, test_size=split_frac, shuffle=True\n",
    "    )\n",
    "\n",
    "    train_df.to_csv(\"./data/3_class_train_df.csv\", index=False)\n",
    "    test_df.to_csv(\"./data/3_class_test_df.csv\", index=False)\n",
    "\n",
    "    print(f\"\\n3_class_train_df:\\n{train_df['LABEL'].value_counts()}\")\n",
    "    print(f\"3_class_test_df:\\n{test_df['LABEL'].value_counts()}\")\n",
    "\n",
    "    # 2-class\n",
    "    df_main = pd.DataFrame(\n",
    "        data=np.nan,\n",
    "        index=np.arange(0, sample_per_category * 2),\n",
    "        columns=[\"FILE\", \"LABEL\"],\n",
    "    )\n",
    "\n",
    "    index = df_raw[df_raw[\"LABEL\"] == \"pneumonia\"].index\n",
    "    df_raw.drop(index, inplace=True)\n",
    "\n",
    "    df_main = df_raw.groupby(\"LABEL\").apply(\n",
    "        lambda s: s.sample(n=min(len(s), sample_per_category), random_state=seed)\n",
    "    )\n",
    "\n",
    "    train_df, test_df = train_test_split(\n",
    "        df_main, random_state=seed, test_size=split_frac, shuffle=True\n",
    "    )\n",
    "\n",
    "    train_df.to_csv(\"./data/2_class_train_df.csv\", index=False)\n",
    "    test_df.to_csv(\"./data/2_class_test_df.csv\", index=False)\n",
    "\n",
    "    print(f\"\\n2_class_train_df:\\n{train_df['LABEL'].value_counts()}\")\n",
    "    print(f\"2_class_test_df:\\n{test_df['LABEL'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# architectures.py\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class Rn50(nn.Module):\n",
    "    def __init__(self, device, train_base=False, classes=2):\n",
    "        super(Rn50, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.net_back = resnet50(pretrained=True).to(self.device)\n",
    "        self._trainable(train_base)\n",
    "\n",
    "        fc_size = self.net_back.fc.in_features\n",
    "        self.net_back.fc = Identity()\n",
    "\n",
    "        self.net_head = nn.Sequential(\n",
    "            nn.Linear(in_features=fc_size, out_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=1024, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=512, out_features=classes),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        ).to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net_back(x.to(self.device))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.net_head(x.to(self.device))\n",
    "\n",
    "    def _trainable(self, flag):\n",
    "        for param in self.net_back.parameters():\n",
    "            param.requires_grad = flag\n",
    "\n",
    "\n",
    "# old way\n",
    "# modules = list(resnet50(pretrained=True).children())[:-1]\n",
    "# self.net_back = nn.Sequential(*modules).to(self.device)\n",
    "# fc_size = list(self.net_back.parameters())[-1].size(0)\n",
    "# self.net_head = nn.Sequential(...).to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activationmap.py\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class FeatureBuffer():\n",
    "\n",
    "    features=None\n",
    "\n",
    "    def __init__(self, m):\n",
    "        self.hook = m.register_forward_hook(self.hook_fn)\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        # self.features = ((output.cpu()).data).numpy()\n",
    "        self.features = output\n",
    "\n",
    "    def remove(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "\n",
    "# def GradCam(model, input_image_tensor):\n",
    "\n",
    "#     https://github.com/tyui592/class_activation_map/blob/master/cam.py\n",
    "#     https://github.com/daixiangzi/Grad_Cam-pytorch-resnet50/blob/578db29d13b0e7d17aa53d9bac116674771618ec/test_grad_cam.py#L19\n",
    "#     https://snappishproductions.com/blog/2018/01/03/class-activation-mapping-in-pytorch.html.html\n",
    "#     https://github.com/MarcoCBA/Class-Activation-Maps-PyTorch/blob/master/class_activation_maps.ipynb\n",
    "\n",
    "#     print(model)\n",
    "\n",
    "#     final_conv_layer = model.net_back._modules.get('layer4')\n",
    "#     fc_layer = model.net_head._modules.get('0')\n",
    "#     fb = FeatureBuffer(final_conv_layer)\n",
    "\n",
    "#     model = model.eval()\n",
    "#     out = model(input_image_tensor)\n",
    "\n",
    "#      # based on model caluculate output!!!\n",
    "#     probabilities = torch.exp(out)\n",
    "#     _, predicted = torch.max(probabilities, 1)\n",
    "#     feature_maps = fb.features\n",
    "\n",
    "#     print(\"Output's shape: \", out.shape)\n",
    "#     print(\"Feature maps's shape: \", feature_maps.shape)\n",
    "\n",
    "#     weights_and_biases = list(fc_layer.parameters())\n",
    "#     class_weights = weights_and_biases[0][predicted]\n",
    "#     print(\"Weights's shape: \", weights_and_biases[0].shape)\n",
    "#     print(\"Biases's shape: \", weights_and_biases[1].shape)\n",
    "#     print(\"Class weights's shape :\", class_weights.shape)\n",
    "\n",
    "#     class_weights = class_weights.reshape((-1, 1, 1))\n",
    "#     feature_maps = feature_maps.flatten(start_dim=0, end_dim=1)\n",
    "#     print(\"Class weights's shape :\", class_weights.shape)\n",
    "#     print(\"Feature maps's shape: \", feature_maps.shape)\n",
    "\n",
    "#     class_activation_maps = np.array(torch.sum(feature_maps * class_weights, dim=0).detach(), dtype=np.float32)\n",
    "#     plt.figure(figsize=(6, 6))\n",
    "#     plt.imshow(class_activation_maps)\n",
    "#     plt.show()\n",
    "\n",
    "#     resized_cam = cv2.resize(class_activation_maps, dsize=(224, 224), interpolation=cv2.INTER_LANCZOS4)\n",
    "#     plt.figure(figsize=(6, 6))\n",
    "#     plt.imshow(resized_cam)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "\n",
    "import math\n",
    "from time import time\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    from torchsummary import summary\n",
    "except:\n",
    "    MODELSUMMARY = False\n",
    "else:\n",
    "    MODELSUMMARY = True\n",
    "\n",
    "try:\n",
    "    from sklearn.manifold import TSNE\n",
    "except:\n",
    "    VIZTSNE = False\n",
    "else:\n",
    "    VIZTSNE = True\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    device,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    dataloaders,\n",
    "    dataloader_len,\n",
    "    input_shape,\n",
    "    scheduler=None,\n",
    "    num_epochs=50,\n",
    "):\n",
    "\n",
    "    if MODELSUMMARY:\n",
    "        summary(model, input_data=input_shape)\n",
    "\n",
    "    start = time()\n",
    "    best_model_wts = deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        t_epoch = time()\n",
    "        print(f\"epoch: {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for idx, (labels, inputs) in enumerate(dataloaders[phase]):\n",
    "                iter_batch = math.ceil(\n",
    "                    dataloader_len[phase] / dataloaders[phase].batch_size\n",
    "                )\n",
    "                print(f\"[phase: {phase}] batch: {idx+1}/{iter_batch}\", end=\"\\r\")\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase == \"train\":\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataloader_len[phase]\n",
    "            epoch_acc = running_corrects.double() / dataloader_len[phase]\n",
    "            print(f\"[phase: {phase}] Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "            if phase == \"val\" and epoch_acc > best_acc:\n",
    "                print(f\"[saving model] epoch: {epoch+1} Acc: {epoch_acc:.4f}\")\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = deepcopy(model.state_dict())\n",
    "\n",
    "        t_elapsed = time() - t_epoch\n",
    "        print(f\"epoch training complete in {t_elapsed//60:.0f}m {t_elapsed%60:.0f}s\")\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time() - start\n",
    "    print(f\"training complete in {time_elapsed//60:.0f}m {time_elapsed%60:.0f}s\")\n",
    "    print(f\"best val Acc: {best_acc:4f}\")\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    checkpoint = {\n",
    "        \"input_shape\": input_shape,\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, \"./models/checkpoint.pth\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.py\n",
    "\n",
    "import math\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from scripts.activationmap import FeatureBuffer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#from .utils import plot_test_image_result\n",
    "\n",
    "\n",
    "def test_model(model, testloader, device, encoder=None):\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    true_list = list()\n",
    "    pred_list = list()\n",
    "\n",
    "    for idx, (labels, inputs) in enumerate(testloader):\n",
    "        iter_batch = math.ceil(len(testloader.dataset)/testloader.batch_size)\n",
    "        print(f'[phase: test] batch: {idx+1}/{iter_batch}', end='\\r')\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            probabilities = torch.exp(outputs)\n",
    "            _, predicted = torch.max(probabilities, 1)\n",
    "\n",
    "            total = idx + 1\n",
    "            correct += torch.sum(predicted == labels.data)\n",
    "            true_list.append((labels.data.cpu()).numpy().item())\n",
    "            pred_list.append((predicted.cpu()).numpy().item())\n",
    "\n",
    "    acc = 100*(correct.item()/total)\n",
    "    print(f\"[phase: test] total: {total}, correct: {correct}, acc: {acc:.3f}\")\n",
    "\n",
    "    print(classification_report(tuple(true_list), tuple(pred_list)))\n",
    "\n",
    "    y_true = pd.Series(true_list, name='Actual')\n",
    "    y_pred = pd.Series(pred_list, name='Predicted')\n",
    "    cm = pd.crosstab(y_true, y_pred,  margins=True)\n",
    "\n",
    "    print(\"confusion matrix\")\n",
    "    if encoder is not None:\n",
    "        print({i : encoder.classes_[i] for i in range(0, len(encoder.classes_))})\n",
    "    print(cm)\n",
    "\n",
    "\n",
    "\n",
    "def test_image(model, image, in_shape, transform, device, labelencoder=None, cam=None):\n",
    "    \"\"\"\n",
    "    GradCam\n",
    "    \"\"\"\n",
    "\n",
    "    if cam is not None:\n",
    "        final_conv_layer = model.net_back._modules.get('layer4')\n",
    "        fc_layer = model.net_head._modules.get('0')\n",
    "        fb = FeatureBuffer(final_conv_layer)\n",
    "\n",
    "    input_tensor = transform(image).unsqueeze(0)\n",
    "    inputs = input_tensor.to(device)\n",
    "\n",
    "    model = model.eval()\n",
    "    outputs = model(inputs)\n",
    "    probabilities = torch.exp(outputs)\n",
    "    prob = (probabilities.cpu()).detach().numpy().flatten()\n",
    "\n",
    "    if cam is not None:\n",
    "        _, predicted = torch.max(probabilities, 1)\n",
    "        feature_maps = fb.features\n",
    "\n",
    "        weights_and_biases = list(fc_layer.parameters())\n",
    "        class_weights = weights_and_biases[0][predicted]\n",
    "\n",
    "        class_weights = class_weights.reshape((-1, 1, 1))\n",
    "        feature_maps = feature_maps.flatten(start_dim=0, end_dim=1)\n",
    "\n",
    "        class_activation_maps = np.array(\n",
    "            torch.sum(feature_maps * class_weights, dim=0).cpu().detach(),\n",
    "            dtype=np.float32)\n",
    "\n",
    "        cam_map = cv2.resize(\n",
    "            class_activation_maps,\n",
    "            dsize=in_shape,\n",
    "            interpolation=cv2.INTER_LANCZOS4)\n",
    "\n",
    "    if cam is not None:\n",
    "        plot_test_image_result(image.resize(in_shape), prob, labelencoder, cam_map)\n",
    "    else:\n",
    "        plot_test_image_result(image, prob, labelencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate.py \n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "from scripts.datagen import Datagen\n",
    "from scripts.architectures import Rn50\n",
    "from scripts.test import test_model, test_image\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda:0\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "test_file = \"data/3_class_test_df.csv\"\n",
    "image_file = \"data/raw/covid/covid_001.jpg\"\n",
    "num_workers = 2\n",
    "batch_size = 1\n",
    "input_shape = (256, 256)\n",
    "le = LabelEncoder()\n",
    "\n",
    "df = pd.read_csv(test_file)\n",
    "\n",
    "test_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(input_shape),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_set = Datagen(df, l_encoder=le, transforms=test_transforms)\n",
    "label_enc = test_set.get_le()\n",
    "device = get_device()\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set, batch_size=batch_size, num_workers=num_workers,\n",
    ")\n",
    "\n",
    "model = Rn50(device=device, classes=3)\n",
    "model.load_state_dict(torch.load(\"./models/checkpoint.pth\")[\"state_dict\"])\n",
    "\n",
    "test_model(\n",
    "    model=model,\n",
    "    testloader=test_loader,\n",
    "    device=device,\n",
    "    encoder=label_enc)\n",
    "\n",
    "input_image = Image.open(image_file).convert(\"RGB\")\n",
    "test_image(\n",
    "    model=model,\n",
    "    image=input_image,\n",
    "    in_shape=input_shape,\n",
    "    transform=test_transforms,\n",
    "    device=device,\n",
    "    labelencoder=label_enc,\n",
    "    cam=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datagen.py\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class Datagen(Dataset):\n",
    "    def __init__(self, dataframe, transforms=None, l_encoder=None):\n",
    "        self.df = dataframe\n",
    "        self.transforms = transforms\n",
    "        self.encoder = l_encoder\n",
    "\n",
    "        if self.encoder is not None:\n",
    "            self.df[\"LABEL\"] = self.encoder.fit_transform(self.df[\"LABEL\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        label = self.df.iloc[idx, 0]\n",
    "        image_file = self.df.iloc[idx, 1]\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        # print(image.shape)\n",
    "        return (label, image)\n",
    "\n",
    "    def get_le(self):\n",
    "        return self.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from scripts.datagen import Datagen\n",
    "from scripts.architectures import Rn50\n",
    "from scripts.train import train_model\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda:0\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "\n",
    "train_file = \"data/3_class_train_df.csv\"\n",
    "num_workers = 2\n",
    "val_split = 0.2\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "input_shape = (3, 256, 256)\n",
    "le = LabelEncoder()\n",
    "\n",
    "df = pd.read_csv(train_file)\n",
    "\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "validation_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_set = Datagen(df, l_encoder=le, transforms=train_transforms)\n",
    "validation_set = Datagen(df, l_encoder=le, transforms=validation_transforms)\n",
    "\n",
    "train_idx, val_idx = train_test_split(list(range(len(df))), test_size=val_split)\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    # shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    sampler=train_sampler,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    validation_set,\n",
    "    # shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    sampler=valid_sampler,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "device = get_device()\n",
    "net = Rn50(device=device, classes=3)\n",
    "\n",
    "dataloaders = {\"train\": train_loader, \"val\": valid_loader}\n",
    "dataloader_len = {\"train\": len(train_idx), \"val\": len(val_idx)}\n",
    "\n",
    "criteration = nn.NLLLoss()\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "model = train_model(\n",
    "    model=net,\n",
    "    device=device,\n",
    "    criterion=criteration,\n",
    "    optimizer=optimizer,\n",
    "    dataloaders=dataloaders,\n",
    "    dataloader_len=dataloader_len,\n",
    "    input_shape=input_shape,\n",
    "    num_epochs=num_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 3 FIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 3 FIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 3 FIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 3 FIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 3 FIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 3 FIM\n",
    "########## CAM try 3 FIM\n",
    "########## CAM try 3 FIM\n",
    "########## CAM try 3 FIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple implementation of CAM in PyTorch for the networks such as ResNet, DenseNet, SqueezeNet, Inception\n",
    "\n",
    "import io\n",
    "import requests\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pdb\n",
    "\n",
    "# input image\n",
    "LABELS_URL = 'https://s3.amazonaws.com/outcome-blog/imagenet/labels.json'\n",
    "IMG_URL = 'https://diariodonordeste.verdesmares.com.br/image/contentid/policy:1.2966908:1594933666/ferramenta-sesa.jpg'\n",
    "\n",
    "# networks such as googlenet, resnet, densenet already use global average pooling at the end, so CAM could be used directly.\n",
    "model_id = 1\n",
    "if model_id == 1:\n",
    "    net = models.squeezenet1_1(pretrained=True)\n",
    "    finalconv_name = 'features' # this is the last conv layer of the network\n",
    "elif model_id == 2:\n",
    "    net = models.resnet18(pretrained=True)\n",
    "    finalconv_name = 'layer4'\n",
    "elif model_id == 3:\n",
    "    net = models.densenet161(pretrained=True)\n",
    "    finalconv_name = 'features'\n",
    "\n",
    "#net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hook the feature extractor\n",
    "features_blobs = []\n",
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(output.data.cpu().numpy())\n",
    "\n",
    "net._modules.get(finalconv_name).register_forward_hook(hook_feature)\n",
    "\n",
    "# get the softmax weight\n",
    "params = list(net.parameters())\n",
    "weight_softmax = np.squeeze(params[-2].data.numpy())\n",
    "\n",
    "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "    # generate the class activation maps upsample to 256x256\n",
    "    size_upsample = (256, 256)\n",
    "    bz, nc, h, w = feature_conv.shape\n",
    "    output_cam = []\n",
    "    for idx in class_idx:\n",
    "        cam = weight_softmax[idx].dot(feature_conv.reshape((nc, h*w)))\n",
    "        cam = cam.reshape(h, w)\n",
    "        cam = cam - np.min(cam)\n",
    "        cam_img = cam / np.max(cam)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "    return output_cam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(\n",
    "   mean=[0.485, 0.456, 0.406],\n",
    "   std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "preprocess = transforms.Compose([\n",
    "   transforms.Resize((224,224)),\n",
    "   transforms.ToTensor(),\n",
    "   normalize\n",
    "])\n",
    "\n",
    "response = requests.get(IMG_URL)\n",
    "img_pil = Image.open(io.BytesIO(response.content))\n",
    "img_pil.save('test.jpg')\n",
    "\n",
    "img_tensor = preprocess(img_pil)\n",
    "img_variable = Variable(img_tensor.unsqueeze(0))\n",
    "logit = net(img_variable) ## aqui tem algo\n",
    "\n",
    "# # download the imagenet category list\n",
    "classes = {int(key):value for (key, value)\n",
    "          in requests.get(LABELS_URL).json().items()}\n",
    "\n",
    "h_x = F.softmax(logit, dim=1).data.squeeze()\n",
    "probs, idx = h_x.sort(0, True)\n",
    "probs = probs.numpy()\n",
    "idx = idx.numpy()\n",
    "\n",
    "# # output the prediction\n",
    "for i in range(0, 5):\n",
    "    print('{:.3f} -> {}'.format(probs[i], classes[idx[i]]))\n",
    "\n",
    "# generate class activation mapping for the top1 prediction\n",
    "CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[0]])\n",
    "\n",
    "# render the CAM and output\n",
    "print('output CAM.jpg for the top1 prediction: %s'%classes[idx[0]])\n",
    "img = cv2.imread('test.jpg')\n",
    "height, width, _ = img.shape\n",
    "heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
    "result = heatmap * 0.3 + img * 0.5\n",
    "cv2.imwrite('CAM.jpg', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "image_path = 'CAM.jpg'\n",
    "display.display(Image.open(image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
