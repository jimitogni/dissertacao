{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy \n",
    "import cv2\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import shutil, sys\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = '/home/jimi/dissertacao/covid19/datasets/80-20/'\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "#model_name = \"densenet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "#num_classes = 3\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 16\n",
    "\n",
    "# Number of epochs to train for\n",
    "\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True\n",
    "\n",
    "#num_epochs = 500\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "#transforms.RandomRotation(degrees=(-5, 5)),\n",
    "#transforms.ColorJitter(brightness=.02),\n",
    "    \n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x]) for x in ['train', 'val', 'test']}\n",
    "\n",
    "# Create training and validation dataloaders\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
    "                    batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "\n",
    "# trans = ['train','val','test']\n",
    "# categories = ['train','val','test']\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#device.reset()\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# Path to data\n",
    "data_dir = '/home/jimi/dissertacao/covid19/datasets/80-20/'\n",
    "train_dir = data_dir+'train/'\n",
    "test_dir = data_dir+'test/'\n",
    "val_dir = data_dir+'val/'\n",
    "\n",
    "normal_dir = data_dir+'normal/'\n",
    "pneumonia_dir = data_dir+'pneumonia/'\n",
    "covid_dir = data_dir+'covid/'\n",
    "\n",
    "len_covid = len([iq for iq in os.scandir(normal_dir)])\n",
    "len_normal = len([iq for iq in os.scandir(pneumonia_dir)])\n",
    "len_pneumonia = len([iq for iq in os.scandir(covid_dir)])\n",
    "\n",
    "len_train_dir = len([iq for iq in os.scandir(train_dir+'covid/')]) + len([iq for iq in os.scandir(train_dir+'normal/')]) + len([iq for iq in os.scandir(train_dir+'pneumonia/')])\n",
    "len_test_dir = len([iq for iq in os.scandir(test_dir+'covid/')]) + len([iq for iq in os.scandir(test_dir+'normal/')]) + len([iq for iq in os.scandir(test_dir+'pneumonia/')])\n",
    "len_val_dir = len([iq for iq in os.scandir(val_dir+'covid/')]) + len([iq for iq in os.scandir(val_dir+'normal/')]) + len([iq for iq in os.scandir(val_dir+'pneumonia/')])\n",
    "\n",
    "print('Files per classes')\n",
    "print(\"----\"*10)\n",
    "print(\"normal : \", len_covid)\n",
    "\n",
    "print(\"pneumonia : \", len_normal)\n",
    "\n",
    "print(\"covid : \", len_pneumonia)\n",
    "\n",
    "print(\"-\"*20)\n",
    "\n",
    "print('Train, test, validation')\n",
    "\n",
    "print(\"-\"*20)\n",
    "\n",
    "print(\"len_train_dir : \", len_train_dir)\n",
    "\n",
    "print(\"len_test_dir : \", len_test_dir)\n",
    "\n",
    "print(\"len_val_dir : \", len_val_dir)\n",
    "\n",
    "\n",
    "#############################################################\n",
    "\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs, \n",
    "                model_name, lr, batch_size, opt_name, crt_name):\n",
    "    since = time.time()\n",
    "    is_inception = False\n",
    "    \n",
    "    #tensorboard\n",
    "    writer = SummaryWriter(f'runs/dg_{model_name}_lr={lr}_epoch={num_epochs}_batch_size={batch_size}')\n",
    "    step = 0\n",
    "\n",
    "    val_acc_history = []\n",
    "    val_loss_history = []\n",
    "    \n",
    "    train_acc_history = []\n",
    "    train_loss_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('<br>')\n",
    "        print('-' * 10)\n",
    "        print('<br>')\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('<br>')\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            print('<br>')\n",
    "            print('<br>')\n",
    "\n",
    "            writer.add_scalar('training loss', loss, global_step=step)\n",
    "            writer.add_scalar('training accuracy', epoch_acc, global_step=step)\n",
    "            step += 1\n",
    "            \n",
    "            #only to plot the graph\n",
    "            if phase == 'train':\n",
    "                train_acc_history.append(epoch_acc)\n",
    "                train_loss_history.append(epoch_loss)\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "                val_loss_history.append(epoch_loss)\n",
    "\n",
    "        print()\n",
    "        \n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print()\n",
    "    print('<br>')\n",
    "    print('#'*30)\n",
    "    print('<br>')\n",
    "    print('------ Summary ------')\n",
    "    print('<br>')\n",
    "    print(f'model -> {_model}')\n",
    "    print('<br>')\n",
    "    print(f'epochs -> {_epochs}')\n",
    "    print('<br>')\n",
    "    print(f'lr -> {_lrs}')\n",
    "    print('<br>')\n",
    "    print(f'batch size -> {_batch}')\n",
    "    print('<br>')\n",
    "    print(f'optimizer -> {opt_name}')\n",
    "    print('<br>')\n",
    "    print(f'criteriun -> {crt_name}')\n",
    "    print('<br>')\n",
    "    print()\n",
    "    print('<br>')\n",
    "    print('<br>')\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('<br>')\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    print('<br>')\n",
    "    print('#'*30)\n",
    "    print('<br>')\n",
    "    print('<br>')\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(13, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(val_acc_history, label=\"Validation Accuracy\")\n",
    "    plt.plot(train_acc_history, label=\"Validation Loss\")\n",
    "    plt.title('Accuracy and Loss in Validation Dataset')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_loss_history, label=\"val_loss_history\")\n",
    "    plt.plot(train_loss_history, label=\"train_loss_history\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('hist_'+_model+'_opt_'+opt_name+'_crt_'+crt_name+'.png')\n",
    "    plt.show()\n",
    "\n",
    "    print('<br>')\n",
    "    print()\n",
    "\n",
    "    print('==== END ====')\n",
    "    print('<br>')\n",
    "    print('<br>')\n",
    "\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history\n",
    "\n",
    "\n",
    "#############################################################\n",
    "\n",
    "def visualize_model(model, num_images=4):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 3, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)\n",
    "\n",
    "#############################################################\n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "#############################################################\n",
    "\n",
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "\n",
    "#############################################################\n",
    "\n",
    "import itertools \n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig('cm_'+title+'.png')\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "num_classes = 3\n",
    "\n",
    "_models = ['squeezenet', 'densenet', 'resnet', 'alexnet', 'vgg']\n",
    "lrs = [1e-4]\n",
    "_epoch = [300]\n",
    "batch_sizes = [8]\n",
    "opt = [1, 2, 3]\n",
    "crt = [1, 2]\n",
    "\n",
    "for _model in _models:\n",
    "    for _epochs in _epoch:\n",
    "        for _lrs in lrs:\n",
    "            for _batch in batch_sizes:\n",
    "                for _opt in opt:\n",
    "                    for _crt in crt:\n",
    "                               \n",
    "                        print()\n",
    "                        print('='*60)\n",
    "                        print('<br>')\n",
    "                        print('<br>')\n",
    "                        print('==== INITIALIZING WITH PARAMETERS: ====')\n",
    "                        print('<br>')\n",
    "                        print(f'model -> {_model}')\n",
    "                        print('<br>')\n",
    "                        print(f'epochs -> {_epochs}')\n",
    "                        print('<br>')\n",
    "                        print(f'lr -> {_lrs}')\n",
    "                        print('<br>')\n",
    "                        print(f'batch size -> {_batch}')\n",
    "                        print('<br>')\n",
    "                        print(f'optimizer -> {_opt}')\n",
    "                        print('<br>')\n",
    "                        print(f'criteriun -> {_crt}')\n",
    "                        print('<br>')\n",
    "                        print('<br>')\n",
    "                        print()\n",
    "\n",
    "                        feature_extract = True\n",
    "\n",
    "                        model_ft, input_size = initialize_model(_model, num_classes, \n",
    "                                                                feature_extract, use_pretrained=True)\n",
    "\n",
    "                        # Send the model to GPU\n",
    "                        model_ft = model_ft.to(device)\n",
    "\n",
    "                        print('-'*20)\n",
    "                        params_to_update = model_ft.parameters()\n",
    "                        print(\"Params to learn:\")\n",
    "                        if feature_extract:\n",
    "                            params_to_update = []\n",
    "                            for name,param in model_ft.named_parameters():\n",
    "                                if param.requires_grad == True:\n",
    "                                    params_to_update.append(param)\n",
    "                                    print(\"\\t\",name)\n",
    "                                    print('<br>')\n",
    "\n",
    "                        else:\n",
    "                            for name,param in model_ft.named_parameters():\n",
    "                                if param.requires_grad == True:\n",
    "                                    print(\"\\t\",name)\n",
    "                                    print('<br>')\n",
    "\n",
    "\n",
    "                        print()\n",
    "                        print('-'*20)\n",
    "                        print()\n",
    "                        print('== Epochs ==')\n",
    "\n",
    "                        if _opt == 1:\n",
    "                            optimizer_ft = optim.SGD(params_to_update, _lrs, momentum=0.9)\n",
    "                            opt_name = 'SGD'\n",
    "\n",
    "                        if _opt == 2:\n",
    "                            optimizer_ft = optim.Adam(params_to_update, _lrs)\n",
    "                            opt_name = 'ADAM'\n",
    "                            \n",
    "                        if _opt == 3:\n",
    "                            optimizer_ft = optim.RMSprop(params_to_update, _lrs)\n",
    "                            opt_name = 'RMSprop'\n",
    "\n",
    "\n",
    "                        exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)\n",
    "\n",
    "                        #tray nn.NLLLoss\n",
    "                        if _crt == 1:\n",
    "                            criterion = nn.CrossEntropyLoss()\n",
    "                            crt_name = 'CrossEntropyLoss'\n",
    "                        if _crt == 2:\n",
    "                            criterion = nn.NLLLoss()\n",
    "                            crt_name = 'NLLLoss'\n",
    "\n",
    "                        model_ft, hist = train_model(model_ft, dataloaders, criterion, optimizer_ft,\n",
    "                                                num_epochs=_epochs, model_name=_model, lr=_lrs,\n",
    "                                                batch_size=_batch, opt_name=opt_name, crt_name=crt_name)\n",
    "\n",
    "                        from sklearn.metrics import confusion_matrix\n",
    "\n",
    "                        nb_classes = 3\n",
    "\n",
    "                        # Initialize the prediction and label lists(tensors)\n",
    "                        predlist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "                        lbllist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "\n",
    "                        with torch.no_grad():\n",
    "                            for i, (inputs, classes) in enumerate(dataloaders['val']):\n",
    "                                inputs = inputs.to(device) #labels atuais\n",
    "                                classes = classes.to(device) #classes\n",
    "                                outputs = model_ft(inputs) #valores preditos = Passa o label atual e retorna o que o modelo predice\n",
    "                                _, preds = torch.max(outputs, 1) #pega o maior valor das predições\n",
    "\n",
    "                                # Append batch prediction results\n",
    "                                predlist=torch.cat([predlist,preds.view(-1).cpu()])\n",
    "                                lbllist=torch.cat([lbllist,classes.view(-1).cpu()])\n",
    "\n",
    "                        # Confusion matrix\n",
    "                        print('<br>')\n",
    "                        conf_mat=confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
    "                        print(conf_mat)\n",
    "                        print()\n",
    "\n",
    "                        print('<br>')\n",
    "                        from sklearn import metrics\n",
    "\n",
    "                        #analise dos resultados do modelo\n",
    "                        print('Sensitivity or recall total')\n",
    "                        print (metrics.recall_score(lbllist.numpy(), predlist.numpy(), average='micro'))\n",
    "\n",
    "                        print('<br>')\n",
    "                        print()\n",
    "                        print('Sensitivity or recall per classes')\n",
    "                        print (metrics.recall_score(lbllist.numpy(), predlist.numpy(), average=None))\n",
    "\n",
    "                        print('<br>')\n",
    "                        print()\n",
    "                        print('Precision')\n",
    "                        print (metrics.precision_score(lbllist.numpy(), predlist.numpy(), average=None))\n",
    "\n",
    "                        print('<br>')\n",
    "                        print()\n",
    "                        print('F1 Score')\n",
    "                        print (metrics.f1_score(lbllist.numpy(), predlist.numpy(), average=None))\n",
    "\n",
    "                        cm = confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
    "                        np.set_printoptions(precision=2)\n",
    "\n",
    "                        plt.figure()\n",
    "\n",
    "                        plot_confusion_matrix(cm, classes=['norm', 'covid', 'pnemo'], \n",
    "                        title='Confusion matrix model'+_model+'_opt_'+opt_name+'_criteriun_'+crt_name)\n",
    "\n",
    "#############################################################\n",
    "\n",
    "\n",
    "\n",
    "#############################################################\n",
    "\n",
    "\n",
    "\n",
    "#############################################################\n",
    "\n",
    "\n",
    "\n",
    "#############################################################\n",
    "\n",
    "\n",
    "\n",
    "#############################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
