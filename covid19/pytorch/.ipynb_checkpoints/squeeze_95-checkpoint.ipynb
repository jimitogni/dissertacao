{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.5.0\n",
      "Torchvision Version:  0.6.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy \n",
    "import random\n",
    "import shutil, sys\n",
    "import pandas as pd\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = '/home/jimi/dissertacao/covid19/datasets/dataset_original/'\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"squeezenet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 3\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 8\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 200\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files per classes\n",
      "----------------------------------------\n",
      "normal :  1341\n",
      "pneumonia :  1345\n",
      "covid :  505\n",
      "--------------------\n",
      "Train, test, validation\n",
      "--------------------\n",
      "len_train_dir :  1786\n",
      "len_test_dir :  639\n",
      "len_val_dir :  766\n"
     ]
    }
   ],
   "source": [
    "# Path to data\n",
    "data_dir = '/home/jimi/dissertacao/covid19/datasets/dataset_original/'\n",
    "train_dir = data_dir+'train/'\n",
    "test_dir = data_dir+'test/'\n",
    "val_dir = data_dir+'val/'\n",
    "\n",
    "normal_dir = data_dir+'normal/'\n",
    "pneumonia_dir = data_dir+'pneumonia/'\n",
    "covid_dir = data_dir+'covid/'\n",
    "\n",
    "len_covid = len([iq for iq in os.scandir(normal_dir)])\n",
    "len_normal = len([iq for iq in os.scandir(pneumonia_dir)])\n",
    "len_pneumonia = len([iq for iq in os.scandir(covid_dir)])\n",
    "\n",
    "len_train_dir = len([iq for iq in os.scandir(train_dir+'covid/')]) + len([iq for iq in os.scandir(train_dir+'normal/')]) + len([iq for iq in os.scandir(train_dir+'pneumonia/')])\n",
    "len_test_dir = len([iq for iq in os.scandir(test_dir+'covid/')]) + len([iq for iq in os.scandir(test_dir+'normal/')]) + len([iq for iq in os.scandir(test_dir+'pneumonia/')])\n",
    "len_val_dir = len([iq for iq in os.scandir(val_dir+'covid/')]) + len([iq for iq in os.scandir(val_dir+'normal/')]) + len([iq for iq in os.scandir(val_dir+'pneumonia/')])\n",
    "\n",
    "print('Files per classes')\n",
    "print(\"----\"*10)\n",
    "print(\"normal : \", len_covid)\n",
    "print(\"pneumonia : \", len_normal)\n",
    "print(\"covid : \", len_pneumonia)\n",
    "print(\"-\"*20)\n",
    "print('Train, test, validation')\n",
    "print(\"-\"*20)\n",
    "print(\"len_train_dir : \", len_train_dir)\n",
    "print(\"len_test_dir : \", len_test_dir)\n",
    "print(\"len_val_dir : \", len_val_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SqueezeNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (3): Fire(\n",
      "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Fire(\n",
      "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Fire(\n",
      "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (7): Fire(\n",
      "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (8): Fire(\n",
      "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (9): Fire(\n",
      "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (10): Fire(\n",
      "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (12): Fire(\n",
      "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Conv2d(512, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "#transforms.RandomRotation(degrees=(-5, 5)),\n",
    "#transforms.ColorJitter(brightness=.02),\n",
    "    \n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t classifier.1.weight\n",
      "\t classifier.1.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Squeezenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 0.6339 Acc: 0.7312\n",
      "val Loss: 0.4432 Acc: 0.8564\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 0.4350 Acc: 0.8264\n",
      "val Loss: 0.4176 Acc: 0.8551\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 0.4452 Acc: 0.8387\n",
      "val Loss: 0.3884 Acc: 0.8655\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 0.4112 Acc: 0.8443\n",
      "val Loss: 0.3914 Acc: 0.8642\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 0.4102 Acc: 0.8522\n",
      "val Loss: 0.3946 Acc: 0.8525\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 0.3775 Acc: 0.8634\n",
      "val Loss: 0.2687 Acc: 0.9191\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 0.3806 Acc: 0.8544\n",
      "val Loss: 0.2522 Acc: 0.9269\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 0.3416 Acc: 0.8690\n",
      "val Loss: 0.2415 Acc: 0.9230\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 0.3748 Acc: 0.8611\n",
      "val Loss: 0.3459 Acc: 0.8851\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 0.3440 Acc: 0.8656\n",
      "val Loss: 0.2706 Acc: 0.9047\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 0.3691 Acc: 0.8707\n",
      "val Loss: 0.3770 Acc: 0.8616\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 0.3402 Acc: 0.8707\n",
      "val Loss: 0.4278 Acc: 0.8251\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 0.3546 Acc: 0.8673\n",
      "val Loss: 0.3387 Acc: 0.8825\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 0.3527 Acc: 0.8679\n",
      "val Loss: 0.2576 Acc: 0.9243\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 0.3411 Acc: 0.8712\n",
      "val Loss: 0.2753 Acc: 0.9230\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 0.3350 Acc: 0.8740\n",
      "val Loss: 0.2144 Acc: 0.9295\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 0.3411 Acc: 0.8695\n",
      "val Loss: 0.2644 Acc: 0.9178\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 0.3411 Acc: 0.8802\n",
      "val Loss: 0.2433 Acc: 0.9295\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 0.3240 Acc: 0.8707\n",
      "val Loss: 0.2480 Acc: 0.9151\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 0.3119 Acc: 0.8875\n",
      "val Loss: 0.2064 Acc: 0.9243\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 0.3254 Acc: 0.8746\n",
      "val Loss: 0.3069 Acc: 0.8916\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 0.3337 Acc: 0.8796\n",
      "val Loss: 0.2542 Acc: 0.9230\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 0.3383 Acc: 0.8763\n",
      "val Loss: 0.2121 Acc: 0.9269\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 0.3295 Acc: 0.8757\n",
      "val Loss: 0.2190 Acc: 0.9256\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 0.3160 Acc: 0.8852\n",
      "val Loss: 0.2297 Acc: 0.9191\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 0.3481 Acc: 0.8718\n",
      "val Loss: 0.3160 Acc: 0.8943\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 0.3150 Acc: 0.8858\n",
      "val Loss: 0.2844 Acc: 0.8982\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 0.3231 Acc: 0.8796\n",
      "val Loss: 0.2401 Acc: 0.9308\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 0.3215 Acc: 0.8807\n",
      "val Loss: 0.2402 Acc: 0.9295\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 0.3201 Acc: 0.8757\n",
      "val Loss: 0.2849 Acc: 0.9086\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 0.3099 Acc: 0.8852\n",
      "val Loss: 0.2067 Acc: 0.9334\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 0.3133 Acc: 0.8802\n",
      "val Loss: 0.2050 Acc: 0.9360\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 0.3038 Acc: 0.8858\n",
      "val Loss: 0.2013 Acc: 0.9347\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 0.3185 Acc: 0.8841\n",
      "val Loss: 0.2227 Acc: 0.9386\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 0.3114 Acc: 0.8880\n",
      "val Loss: 0.2360 Acc: 0.9243\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 0.3085 Acc: 0.8897\n",
      "val Loss: 0.2270 Acc: 0.9295\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 0.3074 Acc: 0.8852\n",
      "val Loss: 0.2356 Acc: 0.9243\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 0.3209 Acc: 0.8751\n",
      "val Loss: 0.2370 Acc: 0.9230\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 0.2939 Acc: 0.8992\n",
      "val Loss: 0.2516 Acc: 0.9138\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 0.3105 Acc: 0.8830\n",
      "val Loss: 0.3602 Acc: 0.8590\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 0.2928 Acc: 0.8891\n",
      "val Loss: 0.2640 Acc: 0.9086\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 0.3033 Acc: 0.8880\n",
      "val Loss: 0.2273 Acc: 0.9334\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 0.2986 Acc: 0.8869\n",
      "val Loss: 0.2405 Acc: 0.9204\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 0.3094 Acc: 0.8830\n",
      "val Loss: 0.2075 Acc: 0.9321\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 0.3044 Acc: 0.8847\n",
      "val Loss: 0.2006 Acc: 0.9347\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 0.3186 Acc: 0.8802\n",
      "val Loss: 0.2717 Acc: 0.9125\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 0.3034 Acc: 0.8863\n",
      "val Loss: 0.2197 Acc: 0.9308\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 0.3048 Acc: 0.8925\n",
      "val Loss: 0.2417 Acc: 0.9151\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 0.3104 Acc: 0.8847\n",
      "val Loss: 0.2540 Acc: 0.9138\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 0.2919 Acc: 0.8875\n",
      "val Loss: 0.2596 Acc: 0.9164\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 0.3039 Acc: 0.8813\n",
      "val Loss: 0.2746 Acc: 0.9060\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 0.2791 Acc: 0.8981\n",
      "val Loss: 0.2490 Acc: 0.9217\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 0.2939 Acc: 0.8858\n",
      "val Loss: 0.2747 Acc: 0.9008\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 0.2895 Acc: 0.8936\n",
      "val Loss: 0.2663 Acc: 0.9008\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 0.2908 Acc: 0.8908\n",
      "val Loss: 0.2896 Acc: 0.8956\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 0.3016 Acc: 0.8785\n",
      "val Loss: 0.2214 Acc: 0.9282\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 0.3082 Acc: 0.8852\n",
      "val Loss: 0.2478 Acc: 0.9191\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 0.3093 Acc: 0.8903\n",
      "val Loss: 0.2981 Acc: 0.8969\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 0.2841 Acc: 0.8975\n",
      "val Loss: 0.2314 Acc: 0.9243\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 0.2962 Acc: 0.8942\n",
      "val Loss: 0.2159 Acc: 0.9282\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 0.2811 Acc: 0.8897\n",
      "val Loss: 0.2945 Acc: 0.8969\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 0.2989 Acc: 0.8785\n",
      "val Loss: 0.2249 Acc: 0.9321\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 0.2873 Acc: 0.8897\n",
      "val Loss: 0.2386 Acc: 0.9217\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 0.2761 Acc: 0.8992\n",
      "val Loss: 0.2176 Acc: 0.9360\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 0.3071 Acc: 0.8908\n",
      "val Loss: 0.2575 Acc: 0.9112\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 0.3016 Acc: 0.8953\n",
      "val Loss: 0.2119 Acc: 0.9282\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 0.2866 Acc: 0.8942\n",
      "val Loss: 0.1894 Acc: 0.9452\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 0.2962 Acc: 0.8931\n",
      "val Loss: 0.2076 Acc: 0.9334\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 0.2754 Acc: 0.9026\n",
      "val Loss: 0.1852 Acc: 0.9413\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 0.2921 Acc: 0.8847\n",
      "val Loss: 0.2274 Acc: 0.9269\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 0.2718 Acc: 0.8964\n",
      "val Loss: 0.3407 Acc: 0.8721\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 0.2788 Acc: 0.9020\n",
      "val Loss: 0.2860 Acc: 0.9008\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 0.3063 Acc: 0.8841\n",
      "val Loss: 0.1975 Acc: 0.9413\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 0.2972 Acc: 0.8931\n",
      "val Loss: 0.1966 Acc: 0.9373\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 0.3061 Acc: 0.8830\n",
      "val Loss: 0.2523 Acc: 0.9151\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 0.3090 Acc: 0.8908\n",
      "val Loss: 0.2363 Acc: 0.9191\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 0.2740 Acc: 0.9020\n",
      "val Loss: 0.2318 Acc: 0.9308\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 0.2810 Acc: 0.8964\n",
      "val Loss: 0.2063 Acc: 0.9308\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 0.3125 Acc: 0.8869\n",
      "val Loss: 0.2584 Acc: 0.9151\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 0.2945 Acc: 0.8858\n",
      "val Loss: 0.2374 Acc: 0.9191\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 0.2767 Acc: 0.8992\n",
      "val Loss: 0.1920 Acc: 0.9386\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 0.2602 Acc: 0.9048\n",
      "val Loss: 0.1853 Acc: 0.9373\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 0.2809 Acc: 0.8975\n",
      "val Loss: 0.2510 Acc: 0.9191\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 0.2794 Acc: 0.8987\n",
      "val Loss: 0.1839 Acc: 0.9413\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 0.2526 Acc: 0.9020\n",
      "val Loss: 0.2635 Acc: 0.8995\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 0.3139 Acc: 0.8824\n",
      "val Loss: 0.1888 Acc: 0.9426\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 0.3183 Acc: 0.8796\n",
      "val Loss: 0.2074 Acc: 0.9386\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 0.2949 Acc: 0.8863\n",
      "val Loss: 0.2429 Acc: 0.9138\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 0.2929 Acc: 0.8886\n",
      "val Loss: 0.2425 Acc: 0.9164\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 0.2800 Acc: 0.8869\n",
      "val Loss: 0.2603 Acc: 0.8982\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 0.2933 Acc: 0.8981\n",
      "val Loss: 0.2964 Acc: 0.8916\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 0.2942 Acc: 0.8830\n",
      "val Loss: 0.1950 Acc: 0.9321\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 0.2727 Acc: 0.9003\n",
      "val Loss: 0.1887 Acc: 0.9413\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 0.3104 Acc: 0.8936\n",
      "val Loss: 0.1920 Acc: 0.9373\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 0.3036 Acc: 0.8897\n",
      "val Loss: 0.1973 Acc: 0.9413\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 0.2873 Acc: 0.8975\n",
      "val Loss: 0.2038 Acc: 0.9373\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 0.2629 Acc: 0.9087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2251 Acc: 0.9269\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 0.3025 Acc: 0.8880\n",
      "val Loss: 0.2819 Acc: 0.9034\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 0.2871 Acc: 0.8886\n",
      "val Loss: 0.2052 Acc: 0.9399\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 0.2816 Acc: 0.9020\n",
      "val Loss: 0.2051 Acc: 0.9347\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 0.3071 Acc: 0.8830\n",
      "val Loss: 0.3446 Acc: 0.8616\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 0.3046 Acc: 0.8919\n",
      "val Loss: 0.2108 Acc: 0.9399\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 0.3110 Acc: 0.8863\n",
      "val Loss: 0.3046 Acc: 0.8995\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 0.3060 Acc: 0.8757\n",
      "val Loss: 0.2288 Acc: 0.9269\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 0.3062 Acc: 0.8847\n",
      "val Loss: 0.1970 Acc: 0.9360\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 0.2961 Acc: 0.8947\n",
      "val Loss: 0.2604 Acc: 0.9086\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 0.2655 Acc: 0.9015\n",
      "val Loss: 0.2158 Acc: 0.9230\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 0.2831 Acc: 0.8925\n",
      "val Loss: 0.2021 Acc: 0.9334\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 0.2689 Acc: 0.9015\n",
      "val Loss: 0.2209 Acc: 0.9230\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 0.2697 Acc: 0.8914\n",
      "val Loss: 0.2654 Acc: 0.9112\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 0.2944 Acc: 0.8886\n",
      "val Loss: 0.2149 Acc: 0.9282\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 0.2696 Acc: 0.9037\n",
      "val Loss: 0.2297 Acc: 0.9269\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 0.3058 Acc: 0.8903\n",
      "val Loss: 0.2292 Acc: 0.9204\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 0.2826 Acc: 0.8942\n",
      "val Loss: 0.3043 Acc: 0.8877\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 0.3048 Acc: 0.8897\n",
      "val Loss: 0.3261 Acc: 0.8773\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 0.2664 Acc: 0.8987\n",
      "val Loss: 0.3350 Acc: 0.8708\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 0.2857 Acc: 0.8835\n",
      "val Loss: 0.2424 Acc: 0.9204\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 0.2890 Acc: 0.8847\n",
      "val Loss: 0.2711 Acc: 0.9034\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 0.2684 Acc: 0.9026\n",
      "val Loss: 0.1947 Acc: 0.9373\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 0.2727 Acc: 0.9037\n",
      "val Loss: 0.2423 Acc: 0.9164\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 0.2986 Acc: 0.8998\n",
      "val Loss: 0.2007 Acc: 0.9347\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 0.2839 Acc: 0.8914\n",
      "val Loss: 0.2034 Acc: 0.9360\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 0.2945 Acc: 0.8947\n",
      "val Loss: 0.2122 Acc: 0.9295\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 0.3001 Acc: 0.8947\n",
      "val Loss: 0.2367 Acc: 0.9269\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 0.2723 Acc: 0.9009\n",
      "val Loss: 0.2293 Acc: 0.9217\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 0.2898 Acc: 0.8942\n",
      "val Loss: 0.2080 Acc: 0.9386\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 0.2697 Acc: 0.9037\n",
      "val Loss: 0.2426 Acc: 0.9269\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 0.2878 Acc: 0.8970\n",
      "val Loss: 0.2062 Acc: 0.9347\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 0.2853 Acc: 0.8908\n",
      "val Loss: 0.1726 Acc: 0.9413\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 0.2701 Acc: 0.9059\n",
      "val Loss: 0.1661 Acc: 0.9491\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 0.2702 Acc: 0.8992\n",
      "val Loss: 0.2249 Acc: 0.9269\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 0.2688 Acc: 0.9003\n",
      "val Loss: 0.2392 Acc: 0.9178\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 0.2928 Acc: 0.8886\n",
      "val Loss: 0.1826 Acc: 0.9439\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 0.2817 Acc: 0.8959\n",
      "val Loss: 0.2056 Acc: 0.9282\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 0.2883 Acc: 0.8919\n",
      "val Loss: 0.1914 Acc: 0.9426\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 0.2738 Acc: 0.8914\n",
      "val Loss: 0.2539 Acc: 0.9112\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 0.2801 Acc: 0.8998\n",
      "val Loss: 0.1951 Acc: 0.9308\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 0.2907 Acc: 0.8903\n",
      "val Loss: 0.1710 Acc: 0.9465\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 0.2746 Acc: 0.8936\n",
      "val Loss: 0.2527 Acc: 0.9125\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 0.3098 Acc: 0.8863\n",
      "val Loss: 0.1927 Acc: 0.9334\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 0.2997 Acc: 0.8987\n",
      "val Loss: 0.2142 Acc: 0.9360\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 0.2903 Acc: 0.8852\n",
      "val Loss: 0.2407 Acc: 0.9243\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 0.2752 Acc: 0.8947\n",
      "val Loss: 0.2128 Acc: 0.9282\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 0.2877 Acc: 0.8886\n",
      "val Loss: 0.1957 Acc: 0.9465\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 0.2761 Acc: 0.8964\n",
      "val Loss: 0.2729 Acc: 0.9060\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 0.2823 Acc: 0.8992\n",
      "val Loss: 0.2184 Acc: 0.9295\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 0.2846 Acc: 0.8964\n",
      "val Loss: 0.2728 Acc: 0.9021\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 0.2828 Acc: 0.8919\n",
      "val Loss: 0.2507 Acc: 0.9138\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 0.2850 Acc: 0.8953\n",
      "val Loss: 0.2459 Acc: 0.9178\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 0.2850 Acc: 0.8959\n",
      "val Loss: 0.1992 Acc: 0.9347\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 0.2716 Acc: 0.9020\n",
      "val Loss: 0.2457 Acc: 0.9125\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 0.2785 Acc: 0.8953\n",
      "val Loss: 0.2502 Acc: 0.9191\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 0.2439 Acc: 0.9104\n",
      "val Loss: 0.1634 Acc: 0.9504\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 0.2781 Acc: 0.8959\n",
      "val Loss: 0.1762 Acc: 0.9517\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 0.3022 Acc: 0.8947\n",
      "val Loss: 0.2784 Acc: 0.9034\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 0.2967 Acc: 0.8936\n",
      "val Loss: 0.2785 Acc: 0.9008\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 0.2545 Acc: 0.9110\n",
      "val Loss: 0.2215 Acc: 0.9217\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 0.2943 Acc: 0.9009\n",
      "val Loss: 0.2242 Acc: 0.9308\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 0.3032 Acc: 0.8959\n",
      "val Loss: 0.1781 Acc: 0.9426\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 0.2889 Acc: 0.8942\n",
      "val Loss: 0.2015 Acc: 0.9321\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 0.2792 Acc: 0.8970\n",
      "val Loss: 0.2198 Acc: 0.9308\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 0.2585 Acc: 0.9003\n",
      "val Loss: 0.2048 Acc: 0.9347\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 0.2822 Acc: 0.8936\n",
      "val Loss: 0.3019 Acc: 0.8877\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 0.2800 Acc: 0.8964\n",
      "val Loss: 0.2402 Acc: 0.9217\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 0.2871 Acc: 0.8936\n",
      "val Loss: 0.2392 Acc: 0.9191\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 0.2856 Acc: 0.8987\n",
      "val Loss: 0.2701 Acc: 0.9086\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 0.2832 Acc: 0.8975\n",
      "val Loss: 0.2422 Acc: 0.9217\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 0.2680 Acc: 0.9037\n",
      "val Loss: 0.2568 Acc: 0.9178\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 0.2739 Acc: 0.8981\n",
      "val Loss: 0.2719 Acc: 0.9086\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 0.2763 Acc: 0.8869\n",
      "val Loss: 0.2202 Acc: 0.9321\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 0.2788 Acc: 0.8998\n",
      "val Loss: 0.1914 Acc: 0.9373\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 0.2789 Acc: 0.8936\n",
      "val Loss: 0.3213 Acc: 0.8799\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 0.2828 Acc: 0.8998\n",
      "val Loss: 0.2015 Acc: 0.9413\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 0.2575 Acc: 0.9104\n",
      "val Loss: 0.1854 Acc: 0.9439\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 0.2691 Acc: 0.8992\n",
      "val Loss: 0.2572 Acc: 0.9112\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 0.2597 Acc: 0.9026\n",
      "val Loss: 0.2307 Acc: 0.9217\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 0.2779 Acc: 0.8981\n",
      "val Loss: 0.1843 Acc: 0.9491\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 0.2873 Acc: 0.8908\n",
      "val Loss: 0.2074 Acc: 0.9334\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 0.2835 Acc: 0.8998\n",
      "val Loss: 0.2260 Acc: 0.9243\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 0.2761 Acc: 0.8947\n",
      "val Loss: 0.2066 Acc: 0.9347\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 0.2745 Acc: 0.9003\n",
      "val Loss: 0.2435 Acc: 0.9217\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 0.2810 Acc: 0.8970\n",
      "val Loss: 0.2282 Acc: 0.9308\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 0.2880 Acc: 0.8858\n",
      "val Loss: 0.2926 Acc: 0.8943\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 0.2614 Acc: 0.9059\n",
      "val Loss: 0.2618 Acc: 0.9204\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 0.2630 Acc: 0.8992\n",
      "val Loss: 0.2735 Acc: 0.9047\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 0.2658 Acc: 0.8964\n",
      "val Loss: 0.2447 Acc: 0.9204\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 0.2726 Acc: 0.8936\n",
      "val Loss: 0.1620 Acc: 0.9452\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 0.2929 Acc: 0.8869\n",
      "val Loss: 0.2036 Acc: 0.9399\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 0.2746 Acc: 0.8998\n",
      "val Loss: 0.2106 Acc: 0.9321\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 0.2838 Acc: 0.8925\n",
      "val Loss: 0.2082 Acc: 0.9452\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 0.2676 Acc: 0.8947\n",
      "val Loss: 0.2222 Acc: 0.9269\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 0.2949 Acc: 0.8925\n",
      "val Loss: 0.2423 Acc: 0.9217\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2735 Acc: 0.8959\n",
      "val Loss: 0.2158 Acc: 0.9282\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 0.2586 Acc: 0.9059\n",
      "val Loss: 0.3113 Acc: 0.8903\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 0.2761 Acc: 0.8987\n",
      "val Loss: 0.2796 Acc: 0.9047\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 0.2514 Acc: 0.9031\n",
      "val Loss: 0.1885 Acc: 0.9373\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 0.2585 Acc: 0.9127\n",
      "val Loss: 0.2003 Acc: 0.9413\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 0.2760 Acc: 0.8975\n",
      "val Loss: 0.1934 Acc: 0.9399\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 0.2539 Acc: 0.9104\n",
      "val Loss: 0.1896 Acc: 0.9426\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 0.2686 Acc: 0.9065\n",
      "val Loss: 0.1995 Acc: 0.9386\n",
      "\n",
      "Training complete in 152m 16s\n",
      "Best val Acc: 0.951697\n"
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, \n",
    "                             optimizer_ft, num_epochs=num_epochs, \n",
    "                             is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
