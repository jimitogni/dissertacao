{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.5.0\n",
      "Torchvision Version:  0.6.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy \n",
    "import random\n",
    "import shutil, sys\n",
    "import pandas as pd\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = '/home/jimi/dissertacao/covid19/datasets/dataset_original/'\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"vgg\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 3\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 8\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 200\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files per classes\n",
      "----------------------------------------\n",
      "normal :  1341\n",
      "pneumonia :  1345\n",
      "covid :  505\n",
      "--------------------\n",
      "Train, test, validation\n",
      "--------------------\n",
      "len_train_dir :  1786\n",
      "len_test_dir :  639\n",
      "len_val_dir :  766\n"
     ]
    }
   ],
   "source": [
    "# Path to data\n",
    "data_dir = '/home/jimi/dissertacao/covid19/datasets/dataset_original/'\n",
    "train_dir = data_dir+'train/'\n",
    "test_dir = data_dir+'test/'\n",
    "val_dir = data_dir+'val/'\n",
    "\n",
    "normal_dir = data_dir+'normal/'\n",
    "pneumonia_dir = data_dir+'pneumonia/'\n",
    "covid_dir = data_dir+'covid/'\n",
    "\n",
    "len_covid = len([iq for iq in os.scandir(normal_dir)])\n",
    "len_normal = len([iq for iq in os.scandir(pneumonia_dir)])\n",
    "len_pneumonia = len([iq for iq in os.scandir(covid_dir)])\n",
    "\n",
    "len_train_dir = len([iq for iq in os.scandir(train_dir+'covid/')]) + len([iq for iq in os.scandir(train_dir+'normal/')]) + len([iq for iq in os.scandir(train_dir+'pneumonia/')])\n",
    "len_test_dir = len([iq for iq in os.scandir(test_dir+'covid/')]) + len([iq for iq in os.scandir(test_dir+'normal/')]) + len([iq for iq in os.scandir(test_dir+'pneumonia/')])\n",
    "len_val_dir = len([iq for iq in os.scandir(val_dir+'covid/')]) + len([iq for iq in os.scandir(val_dir+'normal/')]) + len([iq for iq in os.scandir(val_dir+'pneumonia/')])\n",
    "\n",
    "print('Files per classes')\n",
    "print(\"----\"*10)\n",
    "print(\"normal : \", len_covid)\n",
    "print(\"pneumonia : \", len_normal)\n",
    "print(\"covid : \", len_pneumonia)\n",
    "print(\"-\"*20)\n",
    "print('Train, test, validation')\n",
    "print(\"-\"*20)\n",
    "print(\"len_train_dir : \", len_train_dir)\n",
    "print(\"len_test_dir : \", len_test_dir)\n",
    "print(\"len_val_dir : \", len_val_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (24): ReLU(inplace=True)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "#transforms.RandomRotation(degrees=(-5, 5)),\n",
    "#transforms.ColorJitter(brightness=.02),\n",
    "    \n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Squeezenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 0.5659 Acc: 0.7760\n",
      "val Loss: 0.2897 Acc: 0.8943\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 0.4555 Acc: 0.8191\n",
      "val Loss: 0.2611 Acc: 0.9138\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 0.4700 Acc: 0.8158\n",
      "val Loss: 0.2600 Acc: 0.9164\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 0.4480 Acc: 0.8113\n",
      "val Loss: 0.3217 Acc: 0.8916\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 0.4011 Acc: 0.8382\n",
      "val Loss: 0.2238 Acc: 0.9243\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 0.4591 Acc: 0.8085\n",
      "val Loss: 0.2436 Acc: 0.9256\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 0.4616 Acc: 0.8197\n",
      "val Loss: 0.2446 Acc: 0.9230\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 0.4281 Acc: 0.8253\n",
      "val Loss: 0.2242 Acc: 0.9321\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 0.4621 Acc: 0.8186\n",
      "val Loss: 0.2111 Acc: 0.9256\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 0.4355 Acc: 0.8298\n",
      "val Loss: 0.2618 Acc: 0.9125\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 0.4303 Acc: 0.8399\n",
      "val Loss: 0.2118 Acc: 0.9243\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 0.4350 Acc: 0.8315\n",
      "val Loss: 0.2525 Acc: 0.9230\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 0.4134 Acc: 0.8365\n",
      "val Loss: 0.3060 Acc: 0.8943\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 0.4466 Acc: 0.8259\n",
      "val Loss: 0.2209 Acc: 0.9178\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 0.4339 Acc: 0.8247\n",
      "val Loss: 0.2405 Acc: 0.9282\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 0.4455 Acc: 0.8208\n",
      "val Loss: 0.3122 Acc: 0.8930\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 0.4462 Acc: 0.8219\n",
      "val Loss: 0.2263 Acc: 0.9321\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 0.4384 Acc: 0.8292\n",
      "val Loss: 0.1963 Acc: 0.9399\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 0.4364 Acc: 0.8348\n",
      "val Loss: 0.1965 Acc: 0.9321\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 0.4438 Acc: 0.8292\n",
      "val Loss: 0.2145 Acc: 0.9243\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 0.4300 Acc: 0.8309\n",
      "val Loss: 0.2333 Acc: 0.9295\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 0.4445 Acc: 0.8275\n",
      "val Loss: 0.2002 Acc: 0.9360\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 0.4415 Acc: 0.8359\n",
      "val Loss: 0.2240 Acc: 0.9178\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 0.4469 Acc: 0.8331\n",
      "val Loss: 0.1911 Acc: 0.9347\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 0.4313 Acc: 0.8393\n",
      "val Loss: 0.2232 Acc: 0.9386\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 0.4338 Acc: 0.8320\n",
      "val Loss: 0.1855 Acc: 0.9439\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 0.4632 Acc: 0.8169\n",
      "val Loss: 0.2090 Acc: 0.9334\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 0.4386 Acc: 0.8275\n",
      "val Loss: 0.1925 Acc: 0.9347\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 0.4313 Acc: 0.8488\n",
      "val Loss: 0.2298 Acc: 0.9282\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 0.4204 Acc: 0.8387\n",
      "val Loss: 0.2382 Acc: 0.9178\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 0.4340 Acc: 0.8275\n",
      "val Loss: 0.2265 Acc: 0.9295\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 0.4440 Acc: 0.8259\n",
      "val Loss: 0.2051 Acc: 0.9347\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 0.4395 Acc: 0.8292\n",
      "val Loss: 0.2037 Acc: 0.9347\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 0.4347 Acc: 0.8275\n",
      "val Loss: 0.2039 Acc: 0.9295\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 0.4275 Acc: 0.8270\n",
      "val Loss: 0.2272 Acc: 0.9243\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 0.4232 Acc: 0.8303\n",
      "val Loss: 0.3120 Acc: 0.8930\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 0.4265 Acc: 0.8359\n",
      "val Loss: 0.2089 Acc: 0.9321\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 0.4342 Acc: 0.8399\n",
      "val Loss: 0.1900 Acc: 0.9347\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 0.4133 Acc: 0.8404\n",
      "val Loss: 0.2067 Acc: 0.9321\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 0.4479 Acc: 0.8287\n",
      "val Loss: 0.2358 Acc: 0.9256\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 0.3976 Acc: 0.8410\n",
      "val Loss: 0.2340 Acc: 0.9373\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 0.4451 Acc: 0.8326\n",
      "val Loss: 0.2487 Acc: 0.9269\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 0.4246 Acc: 0.8387\n",
      "val Loss: 0.2615 Acc: 0.9191\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 0.4481 Acc: 0.8287\n",
      "val Loss: 0.1756 Acc: 0.9413\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 0.4118 Acc: 0.8455\n",
      "val Loss: 0.1905 Acc: 0.9386\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 0.4188 Acc: 0.8466\n",
      "val Loss: 0.2995 Acc: 0.8995\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 0.4585 Acc: 0.8259\n",
      "val Loss: 0.2101 Acc: 0.9439\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 0.4293 Acc: 0.8303\n",
      "val Loss: 0.2033 Acc: 0.9334\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 0.4331 Acc: 0.8275\n",
      "val Loss: 0.1720 Acc: 0.9426\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 0.4266 Acc: 0.8359\n",
      "val Loss: 0.2452 Acc: 0.9243\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 0.4280 Acc: 0.8404\n",
      "val Loss: 0.3804 Acc: 0.8799\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 0.4270 Acc: 0.8399\n",
      "val Loss: 0.2845 Acc: 0.9112\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 0.4408 Acc: 0.8303\n",
      "val Loss: 0.1969 Acc: 0.9334\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 0.4398 Acc: 0.8348\n",
      "val Loss: 0.1827 Acc: 0.9360\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 0.4259 Acc: 0.8331\n",
      "val Loss: 0.2078 Acc: 0.9347\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 0.4196 Acc: 0.8477\n",
      "val Loss: 0.2742 Acc: 0.9125\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 0.4528 Acc: 0.8231\n",
      "val Loss: 0.2033 Acc: 0.9334\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 0.4254 Acc: 0.8303\n",
      "val Loss: 0.3701 Acc: 0.8799\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 0.4238 Acc: 0.8387\n",
      "val Loss: 0.2238 Acc: 0.9386\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 0.4342 Acc: 0.8354\n",
      "val Loss: 0.2017 Acc: 0.9321\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 0.4295 Acc: 0.8281\n",
      "val Loss: 0.2476 Acc: 0.9282\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 0.4151 Acc: 0.8432\n",
      "val Loss: 0.1899 Acc: 0.9373\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 0.4641 Acc: 0.8348\n",
      "val Loss: 0.1837 Acc: 0.9373\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 0.4539 Acc: 0.8247\n",
      "val Loss: 0.1895 Acc: 0.9413\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 0.4335 Acc: 0.8371\n",
      "val Loss: 0.2230 Acc: 0.9373\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 0.4644 Acc: 0.8247\n",
      "val Loss: 0.2127 Acc: 0.9269\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 0.4459 Acc: 0.8253\n",
      "val Loss: 0.2788 Acc: 0.9178\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 0.4553 Acc: 0.8259\n",
      "val Loss: 0.2831 Acc: 0.9151\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 0.4302 Acc: 0.8382\n",
      "val Loss: 0.2094 Acc: 0.9360\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 0.4438 Acc: 0.8393\n",
      "val Loss: 0.2014 Acc: 0.9347\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 0.4334 Acc: 0.8320\n",
      "val Loss: 0.2087 Acc: 0.9386\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 0.4227 Acc: 0.8415\n",
      "val Loss: 0.2348 Acc: 0.9282\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 0.4524 Acc: 0.8337\n",
      "val Loss: 0.2012 Acc: 0.9373\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 0.4369 Acc: 0.8264\n",
      "val Loss: 0.2347 Acc: 0.9243\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 0.4192 Acc: 0.8365\n",
      "val Loss: 0.2055 Acc: 0.9347\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 0.4478 Acc: 0.8337\n",
      "val Loss: 0.1963 Acc: 0.9295\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 0.4324 Acc: 0.8337\n",
      "val Loss: 0.2185 Acc: 0.9269\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 0.4570 Acc: 0.8253\n",
      "val Loss: 0.2220 Acc: 0.9308\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 0.4315 Acc: 0.8348\n",
      "val Loss: 0.2258 Acc: 0.9386\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 0.4308 Acc: 0.8264\n",
      "val Loss: 0.2082 Acc: 0.9334\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 0.4367 Acc: 0.8343\n",
      "val Loss: 0.2302 Acc: 0.9321\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 0.4133 Acc: 0.8477\n",
      "val Loss: 0.1857 Acc: 0.9399\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 0.4263 Acc: 0.8516\n",
      "val Loss: 0.2240 Acc: 0.9243\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 0.4229 Acc: 0.8371\n",
      "val Loss: 0.1803 Acc: 0.9386\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 0.4273 Acc: 0.8432\n",
      "val Loss: 0.2714 Acc: 0.9191\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 0.4611 Acc: 0.8242\n",
      "val Loss: 0.2699 Acc: 0.9178\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 0.4493 Acc: 0.8387\n",
      "val Loss: 0.2286 Acc: 0.9360\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 0.4636 Acc: 0.8264\n",
      "val Loss: 0.2208 Acc: 0.9204\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 0.3814 Acc: 0.8539\n",
      "val Loss: 0.4221 Acc: 0.8616\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 0.4329 Acc: 0.8449\n",
      "val Loss: 0.3214 Acc: 0.8969\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 0.4551 Acc: 0.8320\n",
      "val Loss: 0.2170 Acc: 0.9334\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 0.4302 Acc: 0.8359\n",
      "val Loss: 0.2487 Acc: 0.9230\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 0.4289 Acc: 0.8326\n",
      "val Loss: 0.2683 Acc: 0.9164\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 0.4333 Acc: 0.8415\n",
      "val Loss: 0.4491 Acc: 0.8629\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 0.4381 Acc: 0.8393\n",
      "val Loss: 0.1897 Acc: 0.9373\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 0.4151 Acc: 0.8354\n",
      "val Loss: 0.3290 Acc: 0.8943\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 0.4609 Acc: 0.8371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2222 Acc: 0.9321\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 0.4230 Acc: 0.8471\n",
      "val Loss: 0.2019 Acc: 0.9321\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 0.4157 Acc: 0.8443\n",
      "val Loss: 0.2598 Acc: 0.9243\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 0.4624 Acc: 0.8259\n",
      "val Loss: 0.2087 Acc: 0.9373\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 0.3982 Acc: 0.8432\n",
      "val Loss: 0.2489 Acc: 0.9230\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 0.4426 Acc: 0.8449\n",
      "val Loss: 0.2176 Acc: 0.9230\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 0.4466 Acc: 0.8287\n",
      "val Loss: 0.2148 Acc: 0.9373\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 0.4418 Acc: 0.8359\n",
      "val Loss: 0.2037 Acc: 0.9295\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 0.4015 Acc: 0.8539\n",
      "val Loss: 0.1868 Acc: 0.9334\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 0.4093 Acc: 0.8443\n",
      "val Loss: 0.2199 Acc: 0.9217\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 0.4249 Acc: 0.8443\n",
      "val Loss: 0.2427 Acc: 0.9178\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 0.4113 Acc: 0.8415\n",
      "val Loss: 0.1847 Acc: 0.9386\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 0.4345 Acc: 0.8432\n",
      "val Loss: 0.2055 Acc: 0.9308\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 0.4539 Acc: 0.8309\n",
      "val Loss: 0.1993 Acc: 0.9373\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 0.4200 Acc: 0.8455\n",
      "val Loss: 0.2658 Acc: 0.9125\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 0.4550 Acc: 0.8320\n",
      "val Loss: 0.2368 Acc: 0.9282\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 0.4369 Acc: 0.8320\n",
      "val Loss: 0.1877 Acc: 0.9321\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 0.4664 Acc: 0.8303\n",
      "val Loss: 0.2140 Acc: 0.9191\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 0.4722 Acc: 0.8264\n",
      "val Loss: 0.1950 Acc: 0.9347\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 0.4274 Acc: 0.8443\n",
      "val Loss: 0.1929 Acc: 0.9386\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 0.4415 Acc: 0.8287\n",
      "val Loss: 0.1900 Acc: 0.9347\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 0.4414 Acc: 0.8259\n",
      "val Loss: 0.1820 Acc: 0.9386\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 0.4518 Acc: 0.8275\n",
      "val Loss: 0.1756 Acc: 0.9386\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 0.4139 Acc: 0.8415\n",
      "val Loss: 0.2081 Acc: 0.9347\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 0.4178 Acc: 0.8371\n",
      "val Loss: 0.2053 Acc: 0.9413\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 0.4154 Acc: 0.8415\n",
      "val Loss: 0.2386 Acc: 0.9308\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 0.4648 Acc: 0.8247\n",
      "val Loss: 0.2144 Acc: 0.9269\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 0.4403 Acc: 0.8354\n",
      "val Loss: 0.1990 Acc: 0.9347\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 0.4322 Acc: 0.8432\n",
      "val Loss: 0.3642 Acc: 0.8903\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 0.4599 Acc: 0.8309\n",
      "val Loss: 0.2898 Acc: 0.9151\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 0.4397 Acc: 0.8331\n",
      "val Loss: 0.2430 Acc: 0.9282\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 0.4456 Acc: 0.8371\n",
      "val Loss: 0.2097 Acc: 0.9282\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 0.4363 Acc: 0.8404\n",
      "val Loss: 0.2070 Acc: 0.9334\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 0.3991 Acc: 0.8427\n",
      "val Loss: 0.2082 Acc: 0.9321\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 0.4269 Acc: 0.8393\n",
      "val Loss: 0.1889 Acc: 0.9413\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 0.4011 Acc: 0.8555\n",
      "val Loss: 0.1994 Acc: 0.9334\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 0.4159 Acc: 0.8427\n",
      "val Loss: 0.2240 Acc: 0.9308\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 0.4096 Acc: 0.8483\n",
      "val Loss: 0.3138 Acc: 0.8995\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 0.4631 Acc: 0.8359\n",
      "val Loss: 0.1997 Acc: 0.9347\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 0.4438 Acc: 0.8359\n",
      "val Loss: 0.2222 Acc: 0.9269\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 0.4610 Acc: 0.8287\n",
      "val Loss: 0.2607 Acc: 0.9204\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 0.4253 Acc: 0.8382\n",
      "val Loss: 0.1744 Acc: 0.9452\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 0.4325 Acc: 0.8303\n",
      "val Loss: 0.2235 Acc: 0.9295\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 0.4603 Acc: 0.8247\n",
      "val Loss: 0.2591 Acc: 0.9243\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 0.4495 Acc: 0.8298\n",
      "val Loss: 0.2116 Acc: 0.9360\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 0.4109 Acc: 0.8427\n",
      "val Loss: 0.1887 Acc: 0.9426\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 0.4576 Acc: 0.8253\n",
      "val Loss: 0.2114 Acc: 0.9413\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 0.3883 Acc: 0.8572\n",
      "val Loss: 0.1706 Acc: 0.9465\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 0.4249 Acc: 0.8438\n",
      "val Loss: 0.1996 Acc: 0.9373\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 0.4343 Acc: 0.8387\n",
      "val Loss: 0.1925 Acc: 0.9399\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 0.4402 Acc: 0.8303\n",
      "val Loss: 0.1948 Acc: 0.9334\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 0.4233 Acc: 0.8415\n",
      "val Loss: 0.2237 Acc: 0.9399\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 0.4283 Acc: 0.8354\n",
      "val Loss: 0.1984 Acc: 0.9452\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 0.4556 Acc: 0.8387\n",
      "val Loss: 0.2016 Acc: 0.9308\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 0.4249 Acc: 0.8449\n",
      "val Loss: 0.2072 Acc: 0.9256\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 0.4592 Acc: 0.8343\n",
      "val Loss: 0.2070 Acc: 0.9243\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 0.4061 Acc: 0.8449\n",
      "val Loss: 0.1969 Acc: 0.9347\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 0.4041 Acc: 0.8466\n",
      "val Loss: 0.1902 Acc: 0.9426\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 0.4549 Acc: 0.8270\n",
      "val Loss: 0.2400 Acc: 0.9321\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 0.3942 Acc: 0.8494\n",
      "val Loss: 0.2430 Acc: 0.9360\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 0.4142 Acc: 0.8432\n",
      "val Loss: 0.2049 Acc: 0.9439\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 0.4187 Acc: 0.8449\n",
      "val Loss: 0.4235 Acc: 0.8734\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 0.4643 Acc: 0.8343\n",
      "val Loss: 0.1917 Acc: 0.9347\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 0.4625 Acc: 0.8287\n",
      "val Loss: 0.1974 Acc: 0.9334\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 0.4469 Acc: 0.8393\n",
      "val Loss: 0.2170 Acc: 0.9269\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 0.4345 Acc: 0.8354\n",
      "val Loss: 0.1830 Acc: 0.9399\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 0.4665 Acc: 0.8359\n",
      "val Loss: 0.2212 Acc: 0.9282\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 0.4725 Acc: 0.8231\n",
      "val Loss: 0.2151 Acc: 0.9256\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 0.4358 Acc: 0.8331\n",
      "val Loss: 0.2159 Acc: 0.9308\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 0.4294 Acc: 0.8466\n",
      "val Loss: 0.1927 Acc: 0.9373\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 0.4382 Acc: 0.8415\n",
      "val Loss: 0.2795 Acc: 0.9191\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 0.4124 Acc: 0.8483\n",
      "val Loss: 0.2031 Acc: 0.9386\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 0.4266 Acc: 0.8432\n",
      "val Loss: 0.3082 Acc: 0.9099\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 0.4528 Acc: 0.8410\n",
      "val Loss: 0.2427 Acc: 0.9295\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 0.4307 Acc: 0.8376\n",
      "val Loss: 0.1960 Acc: 0.9386\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 0.4251 Acc: 0.8443\n",
      "val Loss: 0.3882 Acc: 0.8747\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 0.4831 Acc: 0.8298\n",
      "val Loss: 0.1858 Acc: 0.9413\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 0.4169 Acc: 0.8410\n",
      "val Loss: 0.2351 Acc: 0.9386\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 0.4452 Acc: 0.8348\n",
      "val Loss: 0.2455 Acc: 0.9282\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 0.4337 Acc: 0.8337\n",
      "val Loss: 0.3202 Acc: 0.9034\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 0.4640 Acc: 0.8259\n",
      "val Loss: 0.2252 Acc: 0.9373\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 0.4523 Acc: 0.8427\n",
      "val Loss: 0.2633 Acc: 0.9321\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 0.4641 Acc: 0.8292\n",
      "val Loss: 0.2317 Acc: 0.9269\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 0.4184 Acc: 0.8533\n",
      "val Loss: 0.2155 Acc: 0.9282\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 0.3926 Acc: 0.8511\n",
      "val Loss: 0.1974 Acc: 0.9295\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 0.4346 Acc: 0.8404\n",
      "val Loss: 0.1878 Acc: 0.9360\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 0.4699 Acc: 0.8253\n",
      "val Loss: 0.2354 Acc: 0.9373\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 0.4450 Acc: 0.8371\n",
      "val Loss: 0.2085 Acc: 0.9386\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 0.4186 Acc: 0.8326\n",
      "val Loss: 0.3057 Acc: 0.9256\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 0.4284 Acc: 0.8421\n",
      "val Loss: 0.2091 Acc: 0.9347\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 0.4338 Acc: 0.8382\n",
      "val Loss: 0.1898 Acc: 0.9360\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 0.4229 Acc: 0.8354\n",
      "val Loss: 0.2131 Acc: 0.9373\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 0.4008 Acc: 0.8415\n",
      "val Loss: 0.1875 Acc: 0.9465\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 0.4256 Acc: 0.8494\n",
      "val Loss: 0.1975 Acc: 0.9399\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 0.4207 Acc: 0.8511\n",
      "val Loss: 0.2159 Acc: 0.9334\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 0.4243 Acc: 0.8410\n",
      "val Loss: 0.2386 Acc: 0.9269\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.4232 Acc: 0.8348\n",
      "val Loss: 0.2306 Acc: 0.9360\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 0.4747 Acc: 0.8247\n",
      "val Loss: 0.2823 Acc: 0.9256\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 0.4392 Acc: 0.8382\n",
      "val Loss: 0.2005 Acc: 0.9308\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 0.4458 Acc: 0.8326\n",
      "val Loss: 0.2543 Acc: 0.9282\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 0.4413 Acc: 0.8343\n",
      "val Loss: 0.2475 Acc: 0.9334\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 0.4867 Acc: 0.8203\n",
      "val Loss: 0.1792 Acc: 0.9478\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 0.4399 Acc: 0.8393\n",
      "val Loss: 0.3160 Acc: 0.9099\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 0.4076 Acc: 0.8427\n",
      "val Loss: 0.1872 Acc: 0.9413\n",
      "\n",
      "Training complete in 155m 9s\n",
      "Best val Acc: 0.947781\n"
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, \n",
    "                             optimizer_ft, num_epochs=num_epochs, \n",
    "                             is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
