{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.5.0\n",
      "Torchvision Version:  0.6.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy \n",
    "import random\n",
    "import shutil, sys\n",
    "import pandas as pd\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = '/home/jimi/dissertacao/covid19/datasets/dataset_original/'\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"densenet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 3\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 8\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 200\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files per classes\n",
      "----------------------------------------\n",
      "normal :  1341\n",
      "pneumonia :  1345\n",
      "covid :  505\n",
      "--------------------\n",
      "Train, test, validation\n",
      "--------------------\n",
      "len_train_dir :  1786\n",
      "len_test_dir :  639\n",
      "len_val_dir :  766\n"
     ]
    }
   ],
   "source": [
    "# Path to data\n",
    "data_dir = '/home/jimi/dissertacao/covid19/datasets/dataset_original/'\n",
    "train_dir = data_dir+'train/'\n",
    "test_dir = data_dir+'test/'\n",
    "val_dir = data_dir+'val/'\n",
    "\n",
    "normal_dir = data_dir+'normal/'\n",
    "pneumonia_dir = data_dir+'pneumonia/'\n",
    "covid_dir = data_dir+'covid/'\n",
    "\n",
    "len_covid = len([iq for iq in os.scandir(normal_dir)])\n",
    "len_normal = len([iq for iq in os.scandir(pneumonia_dir)])\n",
    "len_pneumonia = len([iq for iq in os.scandir(covid_dir)])\n",
    "\n",
    "len_train_dir = len([iq for iq in os.scandir(train_dir+'covid/')]) + len([iq for iq in os.scandir(train_dir+'normal/')]) + len([iq for iq in os.scandir(train_dir+'pneumonia/')])\n",
    "len_test_dir = len([iq for iq in os.scandir(test_dir+'covid/')]) + len([iq for iq in os.scandir(test_dir+'normal/')]) + len([iq for iq in os.scandir(test_dir+'pneumonia/')])\n",
    "len_val_dir = len([iq for iq in os.scandir(val_dir+'covid/')]) + len([iq for iq in os.scandir(val_dir+'normal/')]) + len([iq for iq in os.scandir(val_dir+'pneumonia/')])\n",
    "\n",
    "print('Files per classes')\n",
    "print(\"----\"*10)\n",
    "print(\"normal : \", len_covid)\n",
    "print(\"pneumonia : \", len_normal)\n",
    "print(\"covid : \", len_pneumonia)\n",
    "print(\"-\"*20)\n",
    "print('Train, test, validation')\n",
    "print(\"-\"*20)\n",
    "print(\"len_train_dir : \", len_train_dir)\n",
    "print(\"len_test_dir : \", len_test_dir)\n",
    "print(\"len_val_dir : \", len_val_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet(\n",
      "  (features): Sequential(\n",
      "    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu0): ReLU(inplace=True)\n",
      "    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (denseblock1): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition1): _Transition(\n",
      "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock2): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition2): _Transition(\n",
      "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock3): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer17): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer18): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer19): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer20): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer21): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer22): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer23): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer24): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition3): _Transition(\n",
      "      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock4): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=1024, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "#transforms.RandomRotation(degrees=(-5, 5)),\n",
    "#transforms.ColorJitter(brightness=.02),\n",
    "    \n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t classifier.weight\n",
      "\t classifier.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Squeezenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 0.6647 Acc: 0.6982\n",
      "val Loss: 0.3556 Acc: 0.8773\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 0.4979 Acc: 0.8024\n",
      "val Loss: 0.5317 Acc: 0.7689\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 0.4724 Acc: 0.8242\n",
      "val Loss: 0.3295 Acc: 0.8786\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 0.4628 Acc: 0.8147\n",
      "val Loss: 0.2669 Acc: 0.9073\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 0.4380 Acc: 0.8219\n",
      "val Loss: 0.2853 Acc: 0.9034\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 0.4614 Acc: 0.8063\n",
      "val Loss: 0.2408 Acc: 0.9138\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 0.4232 Acc: 0.8421\n",
      "val Loss: 0.2426 Acc: 0.9230\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 0.4197 Acc: 0.8460\n",
      "val Loss: 0.2325 Acc: 0.9112\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 0.4179 Acc: 0.8443\n",
      "val Loss: 0.2531 Acc: 0.9099\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 0.4259 Acc: 0.8315\n",
      "val Loss: 0.2363 Acc: 0.9204\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 0.4007 Acc: 0.8387\n",
      "val Loss: 0.2464 Acc: 0.9151\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 0.4033 Acc: 0.8449\n",
      "val Loss: 0.2878 Acc: 0.8982\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 0.4509 Acc: 0.8236\n",
      "val Loss: 0.2446 Acc: 0.9099\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 0.4049 Acc: 0.8387\n",
      "val Loss: 0.2252 Acc: 0.9204\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 0.3873 Acc: 0.8438\n",
      "val Loss: 0.2277 Acc: 0.9164\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 0.4165 Acc: 0.8186\n",
      "val Loss: 0.2593 Acc: 0.9099\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 0.4074 Acc: 0.8382\n",
      "val Loss: 0.2423 Acc: 0.9073\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 0.4025 Acc: 0.8455\n",
      "val Loss: 0.3392 Acc: 0.8773\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 0.4004 Acc: 0.8488\n",
      "val Loss: 0.2171 Acc: 0.9217\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 0.4123 Acc: 0.8393\n",
      "val Loss: 0.2524 Acc: 0.9112\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 0.4246 Acc: 0.8382\n",
      "val Loss: 0.2676 Acc: 0.9073\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 0.3992 Acc: 0.8505\n",
      "val Loss: 0.2097 Acc: 0.9243\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 0.4320 Acc: 0.8270\n",
      "val Loss: 0.2223 Acc: 0.9164\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 0.4205 Acc: 0.8343\n",
      "val Loss: 0.2974 Acc: 0.9008\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 0.3706 Acc: 0.8550\n",
      "val Loss: 0.2208 Acc: 0.9269\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 0.4060 Acc: 0.8449\n",
      "val Loss: 0.2932 Acc: 0.8995\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 0.4033 Acc: 0.8371\n",
      "val Loss: 0.2922 Acc: 0.8982\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 0.3959 Acc: 0.8432\n",
      "val Loss: 0.2283 Acc: 0.9164\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 0.4398 Acc: 0.8253\n",
      "val Loss: 0.2280 Acc: 0.9138\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 0.4099 Acc: 0.8393\n",
      "val Loss: 0.2696 Acc: 0.9073\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 0.4389 Acc: 0.8320\n",
      "val Loss: 0.2178 Acc: 0.9217\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 0.3869 Acc: 0.8522\n",
      "val Loss: 0.2111 Acc: 0.9217\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 0.3989 Acc: 0.8387\n",
      "val Loss: 0.2144 Acc: 0.9295\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 0.3676 Acc: 0.8511\n",
      "val Loss: 0.2951 Acc: 0.8773\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 0.4024 Acc: 0.8449\n",
      "val Loss: 0.3521 Acc: 0.8747\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 0.4052 Acc: 0.8376\n",
      "val Loss: 0.2429 Acc: 0.9151\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 0.3900 Acc: 0.8477\n",
      "val Loss: 0.2504 Acc: 0.9164\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 0.3959 Acc: 0.8511\n",
      "val Loss: 0.2020 Acc: 0.9308\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 0.3938 Acc: 0.8511\n",
      "val Loss: 0.4061 Acc: 0.8120\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 0.4654 Acc: 0.8186\n",
      "val Loss: 0.2327 Acc: 0.9112\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 0.3617 Acc: 0.8651\n",
      "val Loss: 0.3229 Acc: 0.8773\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 0.4184 Acc: 0.8376\n",
      "val Loss: 0.2338 Acc: 0.9112\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 0.4057 Acc: 0.8455\n",
      "val Loss: 0.2808 Acc: 0.8982\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 0.3750 Acc: 0.8561\n",
      "val Loss: 0.2082 Acc: 0.9269\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 0.4601 Acc: 0.8231\n",
      "val Loss: 0.2241 Acc: 0.9178\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 0.4102 Acc: 0.8488\n",
      "val Loss: 0.2071 Acc: 0.9295\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 0.4074 Acc: 0.8499\n",
      "val Loss: 0.2546 Acc: 0.9099\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 0.4024 Acc: 0.8511\n",
      "val Loss: 0.4430 Acc: 0.8407\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 0.4039 Acc: 0.8376\n",
      "val Loss: 0.2663 Acc: 0.9086\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 0.3993 Acc: 0.8477\n",
      "val Loss: 0.2198 Acc: 0.9282\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 0.3764 Acc: 0.8494\n",
      "val Loss: 0.3332 Acc: 0.8486\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 0.4251 Acc: 0.8303\n",
      "val Loss: 0.2236 Acc: 0.9204\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 0.3733 Acc: 0.8623\n",
      "val Loss: 0.3216 Acc: 0.8851\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 0.3902 Acc: 0.8539\n",
      "val Loss: 0.5254 Acc: 0.8107\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 0.4516 Acc: 0.8331\n",
      "val Loss: 0.2189 Acc: 0.9230\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 0.3701 Acc: 0.8505\n",
      "val Loss: 0.2665 Acc: 0.9138\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 0.4584 Acc: 0.8231\n",
      "val Loss: 0.2484 Acc: 0.9151\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 0.3693 Acc: 0.8572\n",
      "val Loss: 0.2436 Acc: 0.9204\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 0.3817 Acc: 0.8527\n",
      "val Loss: 0.1980 Acc: 0.9334\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 0.3600 Acc: 0.8544\n",
      "val Loss: 0.2197 Acc: 0.9178\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 0.3847 Acc: 0.8449\n",
      "val Loss: 0.2133 Acc: 0.9269\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 0.4135 Acc: 0.8393\n",
      "val Loss: 0.2391 Acc: 0.9164\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 0.4259 Acc: 0.8371\n",
      "val Loss: 0.2241 Acc: 0.9243\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 0.4046 Acc: 0.8550\n",
      "val Loss: 0.2700 Acc: 0.9060\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 0.4060 Acc: 0.8449\n",
      "val Loss: 0.2121 Acc: 0.9256\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 0.3763 Acc: 0.8567\n",
      "val Loss: 0.3300 Acc: 0.8943\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 0.3675 Acc: 0.8533\n",
      "val Loss: 0.2301 Acc: 0.9204\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 0.3932 Acc: 0.8578\n",
      "val Loss: 0.1973 Acc: 0.9269\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 0.3763 Acc: 0.8628\n",
      "val Loss: 0.3070 Acc: 0.8890\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 0.3762 Acc: 0.8611\n",
      "val Loss: 0.2193 Acc: 0.9230\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 0.3698 Acc: 0.8595\n",
      "val Loss: 0.2875 Acc: 0.8969\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 0.4159 Acc: 0.8443\n",
      "val Loss: 0.2189 Acc: 0.9243\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 0.3663 Acc: 0.8695\n",
      "val Loss: 0.1971 Acc: 0.9334\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 0.3962 Acc: 0.8438\n",
      "val Loss: 0.2043 Acc: 0.9230\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 0.3631 Acc: 0.8589\n",
      "val Loss: 0.2217 Acc: 0.9321\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 0.3799 Acc: 0.8583\n",
      "val Loss: 0.2477 Acc: 0.9125\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 0.3571 Acc: 0.8595\n",
      "val Loss: 0.2292 Acc: 0.9191\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 0.3956 Acc: 0.8499\n",
      "val Loss: 0.3117 Acc: 0.8982\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 0.4354 Acc: 0.8292\n",
      "val Loss: 0.2069 Acc: 0.9282\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 0.3704 Acc: 0.8494\n",
      "val Loss: 0.2557 Acc: 0.9125\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 0.3857 Acc: 0.8494\n",
      "val Loss: 0.2132 Acc: 0.9269\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 0.3685 Acc: 0.8611\n",
      "val Loss: 0.2761 Acc: 0.8943\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 0.3609 Acc: 0.8522\n",
      "val Loss: 0.1990 Acc: 0.9373\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 0.3406 Acc: 0.8679\n",
      "val Loss: 0.2360 Acc: 0.9230\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 0.3661 Acc: 0.8522\n",
      "val Loss: 0.2715 Acc: 0.9073\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 0.3819 Acc: 0.8432\n",
      "val Loss: 0.2372 Acc: 0.9230\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 0.3575 Acc: 0.8662\n",
      "val Loss: 0.3331 Acc: 0.8864\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 0.3952 Acc: 0.8527\n",
      "val Loss: 0.2314 Acc: 0.9256\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 0.4154 Acc: 0.8337\n",
      "val Loss: 0.2173 Acc: 0.9243\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 0.3727 Acc: 0.8494\n",
      "val Loss: 0.4101 Acc: 0.8577\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 0.4263 Acc: 0.8404\n",
      "val Loss: 0.2073 Acc: 0.9373\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 0.3803 Acc: 0.8477\n",
      "val Loss: 0.3018 Acc: 0.8969\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 0.3541 Acc: 0.8651\n",
      "val Loss: 0.2065 Acc: 0.9269\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 0.3837 Acc: 0.8471\n",
      "val Loss: 0.2669 Acc: 0.9164\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 0.3810 Acc: 0.8511\n",
      "val Loss: 0.2520 Acc: 0.9021\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 0.3648 Acc: 0.8623\n",
      "val Loss: 0.3852 Acc: 0.8668\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 0.4067 Acc: 0.8376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2084 Acc: 0.9308\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 0.3987 Acc: 0.8449\n",
      "val Loss: 0.2162 Acc: 0.9282\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 0.4271 Acc: 0.8359\n",
      "val Loss: 0.2062 Acc: 0.9308\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 0.3910 Acc: 0.8595\n",
      "val Loss: 0.2116 Acc: 0.9308\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 0.3957 Acc: 0.8511\n",
      "val Loss: 0.3428 Acc: 0.8825\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 0.4066 Acc: 0.8449\n",
      "val Loss: 0.2516 Acc: 0.9060\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 0.4087 Acc: 0.8544\n",
      "val Loss: 0.2714 Acc: 0.8786\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 0.3825 Acc: 0.8522\n",
      "val Loss: 0.2723 Acc: 0.9125\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 0.3935 Acc: 0.8533\n",
      "val Loss: 0.2159 Acc: 0.9282\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 0.3920 Acc: 0.8544\n",
      "val Loss: 0.3252 Acc: 0.8642\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 0.3828 Acc: 0.8589\n",
      "val Loss: 0.2192 Acc: 0.9295\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 0.3971 Acc: 0.8415\n",
      "val Loss: 0.2204 Acc: 0.9269\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 0.3677 Acc: 0.8527\n",
      "val Loss: 0.2402 Acc: 0.9321\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 0.3487 Acc: 0.8623\n",
      "val Loss: 0.2213 Acc: 0.9347\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 0.3921 Acc: 0.8555\n",
      "val Loss: 0.2196 Acc: 0.9256\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 0.3736 Acc: 0.8499\n",
      "val Loss: 0.2239 Acc: 0.9217\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 0.3831 Acc: 0.8533\n",
      "val Loss: 0.2167 Acc: 0.9138\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 0.3857 Acc: 0.8488\n",
      "val Loss: 0.2427 Acc: 0.9256\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 0.3896 Acc: 0.8522\n",
      "val Loss: 0.2238 Acc: 0.9256\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 0.4203 Acc: 0.8315\n",
      "val Loss: 0.3417 Acc: 0.8786\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 0.3830 Acc: 0.8589\n",
      "val Loss: 0.2368 Acc: 0.9086\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 0.4099 Acc: 0.8354\n",
      "val Loss: 0.2652 Acc: 0.9086\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 0.3732 Acc: 0.8595\n",
      "val Loss: 0.2246 Acc: 0.9217\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 0.3822 Acc: 0.8561\n",
      "val Loss: 0.3456 Acc: 0.8864\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 0.4097 Acc: 0.8455\n",
      "val Loss: 0.2081 Acc: 0.9256\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 0.3799 Acc: 0.8561\n",
      "val Loss: 0.2580 Acc: 0.9125\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 0.4075 Acc: 0.8471\n",
      "val Loss: 0.2914 Acc: 0.8969\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 0.3743 Acc: 0.8679\n",
      "val Loss: 0.2374 Acc: 0.9073\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 0.3721 Acc: 0.8583\n",
      "val Loss: 0.2156 Acc: 0.9360\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 0.4115 Acc: 0.8466\n",
      "val Loss: 0.3951 Acc: 0.8681\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 0.3750 Acc: 0.8572\n",
      "val Loss: 0.3013 Acc: 0.8943\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 0.3686 Acc: 0.8477\n",
      "val Loss: 0.2549 Acc: 0.9191\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 0.3883 Acc: 0.8449\n",
      "val Loss: 0.2334 Acc: 0.9243\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 0.3893 Acc: 0.8449\n",
      "val Loss: 0.2306 Acc: 0.9125\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 0.3293 Acc: 0.8779\n",
      "val Loss: 0.2465 Acc: 0.9073\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 0.4017 Acc: 0.8522\n",
      "val Loss: 0.2075 Acc: 0.9256\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 0.4080 Acc: 0.8477\n",
      "val Loss: 0.2016 Acc: 0.9347\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 0.3694 Acc: 0.8662\n",
      "val Loss: 0.2421 Acc: 0.9164\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 0.3536 Acc: 0.8527\n",
      "val Loss: 0.2636 Acc: 0.9073\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 0.3754 Acc: 0.8539\n",
      "val Loss: 0.2407 Acc: 0.9112\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 0.3326 Acc: 0.8740\n",
      "val Loss: 0.4047 Acc: 0.8577\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 0.3774 Acc: 0.8511\n",
      "val Loss: 0.3112 Acc: 0.8851\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 0.3792 Acc: 0.8567\n",
      "val Loss: 0.2033 Acc: 0.9217\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 0.3542 Acc: 0.8712\n",
      "val Loss: 0.2075 Acc: 0.9321\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 0.3684 Acc: 0.8583\n",
      "val Loss: 0.2556 Acc: 0.9086\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 0.3792 Acc: 0.8511\n",
      "val Loss: 0.2135 Acc: 0.9256\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 0.4003 Acc: 0.8387\n",
      "val Loss: 0.2629 Acc: 0.9138\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 0.3783 Acc: 0.8516\n",
      "val Loss: 0.3191 Acc: 0.8890\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 0.3471 Acc: 0.8662\n",
      "val Loss: 0.2972 Acc: 0.8943\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 0.4018 Acc: 0.8399\n",
      "val Loss: 0.3718 Acc: 0.8773\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 0.3416 Acc: 0.8723\n",
      "val Loss: 0.2271 Acc: 0.9204\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 0.3382 Acc: 0.8763\n",
      "val Loss: 0.2882 Acc: 0.8930\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 0.3679 Acc: 0.8617\n",
      "val Loss: 0.1925 Acc: 0.9413\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 0.3906 Acc: 0.8611\n",
      "val Loss: 0.2510 Acc: 0.9099\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 0.3492 Acc: 0.8600\n",
      "val Loss: 0.2321 Acc: 0.9230\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 0.3567 Acc: 0.8695\n",
      "val Loss: 0.2311 Acc: 0.9282\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 0.3870 Acc: 0.8516\n",
      "val Loss: 0.2425 Acc: 0.9151\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 0.3588 Acc: 0.8729\n",
      "val Loss: 0.2863 Acc: 0.9034\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 0.3938 Acc: 0.8544\n",
      "val Loss: 0.3297 Acc: 0.8903\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 0.3423 Acc: 0.8645\n",
      "val Loss: 0.2133 Acc: 0.9243\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 0.3206 Acc: 0.8751\n",
      "val Loss: 0.2030 Acc: 0.9308\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 0.3578 Acc: 0.8684\n",
      "val Loss: 0.2373 Acc: 0.9191\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 0.4025 Acc: 0.8477\n",
      "val Loss: 0.2097 Acc: 0.9295\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 0.3620 Acc: 0.8606\n",
      "val Loss: 0.1923 Acc: 0.9373\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 0.3359 Acc: 0.8662\n",
      "val Loss: 0.2073 Acc: 0.9282\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 0.3663 Acc: 0.8595\n",
      "val Loss: 0.3037 Acc: 0.8995\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 0.3772 Acc: 0.8589\n",
      "val Loss: 0.2512 Acc: 0.9060\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 0.4205 Acc: 0.8331\n",
      "val Loss: 0.2169 Acc: 0.9164\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 0.3957 Acc: 0.8455\n",
      "val Loss: 0.2111 Acc: 0.9321\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 0.3938 Acc: 0.8410\n",
      "val Loss: 0.3673 Acc: 0.8799\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 0.3342 Acc: 0.8639\n",
      "val Loss: 0.2839 Acc: 0.8969\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 0.4056 Acc: 0.8443\n",
      "val Loss: 0.2159 Acc: 0.9269\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 0.3624 Acc: 0.8511\n",
      "val Loss: 0.2783 Acc: 0.8799\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 0.3823 Acc: 0.8600\n",
      "val Loss: 0.2016 Acc: 0.9256\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 0.3483 Acc: 0.8628\n",
      "val Loss: 0.2415 Acc: 0.9191\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 0.3796 Acc: 0.8645\n",
      "val Loss: 0.2345 Acc: 0.9138\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 0.3881 Acc: 0.8567\n",
      "val Loss: 0.1980 Acc: 0.9334\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 0.3575 Acc: 0.8550\n",
      "val Loss: 0.2173 Acc: 0.9230\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 0.3762 Acc: 0.8550\n",
      "val Loss: 0.2186 Acc: 0.9269\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 0.3391 Acc: 0.8735\n",
      "val Loss: 0.2176 Acc: 0.9217\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 0.4102 Acc: 0.8343\n",
      "val Loss: 0.2613 Acc: 0.9099\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 0.3616 Acc: 0.8651\n",
      "val Loss: 0.1974 Acc: 0.9295\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 0.3933 Acc: 0.8505\n",
      "val Loss: 0.1866 Acc: 0.9373\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 0.3867 Acc: 0.8572\n",
      "val Loss: 0.2448 Acc: 0.9151\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 0.3930 Acc: 0.8555\n",
      "val Loss: 0.1974 Acc: 0.9295\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 0.4004 Acc: 0.8455\n",
      "val Loss: 0.2054 Acc: 0.9308\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 0.3744 Acc: 0.8583\n",
      "val Loss: 0.2403 Acc: 0.9047\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 0.3862 Acc: 0.8449\n",
      "val Loss: 0.2035 Acc: 0.9282\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 0.3862 Acc: 0.8477\n",
      "val Loss: 0.2870 Acc: 0.9034\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 0.3723 Acc: 0.8589\n",
      "val Loss: 0.2149 Acc: 0.9308\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 0.3713 Acc: 0.8572\n",
      "val Loss: 0.3904 Acc: 0.8538\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 0.3482 Acc: 0.8667\n",
      "val Loss: 0.2860 Acc: 0.9034\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 0.3798 Acc: 0.8628\n",
      "val Loss: 0.2459 Acc: 0.9112\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 0.3726 Acc: 0.8600\n",
      "val Loss: 0.2069 Acc: 0.9308\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 0.4164 Acc: 0.8443\n",
      "val Loss: 0.2016 Acc: 0.9347\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 0.3796 Acc: 0.8572\n",
      "val Loss: 0.2245 Acc: 0.9243\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3736 Acc: 0.8651\n",
      "val Loss: 0.1920 Acc: 0.9360\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 0.3883 Acc: 0.8539\n",
      "val Loss: 0.2317 Acc: 0.9217\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 0.3459 Acc: 0.8735\n",
      "val Loss: 0.2866 Acc: 0.8982\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 0.3743 Acc: 0.8578\n",
      "val Loss: 0.2122 Acc: 0.9282\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 0.3553 Acc: 0.8617\n",
      "val Loss: 0.2225 Acc: 0.9269\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 0.3775 Acc: 0.8572\n",
      "val Loss: 0.2210 Acc: 0.9308\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 0.3798 Acc: 0.8656\n",
      "val Loss: 0.2262 Acc: 0.9230\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 0.3598 Acc: 0.8651\n",
      "val Loss: 0.2233 Acc: 0.9295\n",
      "\n",
      "Training complete in 175m 44s\n",
      "Best val Acc: 0.941253\n"
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, \n",
    "                             optimizer_ft, num_epochs=num_epochs, \n",
    "                             is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
