{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.5.0\n",
      "Torchvision Version:  0.6.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy \n",
    "import random\n",
    "import shutil, sys\n",
    "import pandas as pd\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = '/home/jimi/dissertacao/covid19/datasets/dataset_original/'\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"resnet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 3\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 8\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 200\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files per classes\n",
      "----------------------------------------\n",
      "normal :  1341\n",
      "pneumonia :  1345\n",
      "covid :  505\n",
      "--------------------\n",
      "Train, test, validation\n",
      "--------------------\n",
      "len_train_dir :  1786\n",
      "len_test_dir :  639\n",
      "len_val_dir :  766\n"
     ]
    }
   ],
   "source": [
    "# Path to data\n",
    "data_dir = '/home/jimi/dissertacao/covid19/datasets/dataset_original/'\n",
    "train_dir = data_dir+'train/'\n",
    "test_dir = data_dir+'test/'\n",
    "val_dir = data_dir+'val/'\n",
    "\n",
    "normal_dir = data_dir+'normal/'\n",
    "pneumonia_dir = data_dir+'pneumonia/'\n",
    "covid_dir = data_dir+'covid/'\n",
    "\n",
    "len_covid = len([iq for iq in os.scandir(normal_dir)])\n",
    "len_normal = len([iq for iq in os.scandir(pneumonia_dir)])\n",
    "len_pneumonia = len([iq for iq in os.scandir(covid_dir)])\n",
    "\n",
    "len_train_dir = len([iq for iq in os.scandir(train_dir+'covid/')]) + len([iq for iq in os.scandir(train_dir+'normal/')]) + len([iq for iq in os.scandir(train_dir+'pneumonia/')])\n",
    "len_test_dir = len([iq for iq in os.scandir(test_dir+'covid/')]) + len([iq for iq in os.scandir(test_dir+'normal/')]) + len([iq for iq in os.scandir(test_dir+'pneumonia/')])\n",
    "len_val_dir = len([iq for iq in os.scandir(val_dir+'covid/')]) + len([iq for iq in os.scandir(val_dir+'normal/')]) + len([iq for iq in os.scandir(val_dir+'pneumonia/')])\n",
    "\n",
    "print('Files per classes')\n",
    "print(\"----\"*10)\n",
    "print(\"normal : \", len_covid)\n",
    "print(\"pneumonia : \", len_normal)\n",
    "print(\"covid : \", len_pneumonia)\n",
    "print(\"-\"*20)\n",
    "print('Train, test, validation')\n",
    "print(\"-\"*20)\n",
    "print(\"len_train_dir : \", len_train_dir)\n",
    "print(\"len_test_dir : \", len_test_dir)\n",
    "print(\"len_val_dir : \", len_val_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "#transforms.RandomRotation(degrees=(-5, 5)),\n",
    "#transforms.ColorJitter(brightness=.02),\n",
    "    \n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Squeezenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 0.6784 Acc: 0.6960\n",
      "val Loss: 0.3338 Acc: 0.8903\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 0.5255 Acc: 0.7727\n",
      "val Loss: 0.3854 Acc: 0.8655\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 0.5455 Acc: 0.7699\n",
      "val Loss: 0.4517 Acc: 0.8551\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 0.4545 Acc: 0.8298\n",
      "val Loss: 0.4065 Acc: 0.8655\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 0.4528 Acc: 0.8091\n",
      "val Loss: 0.3472 Acc: 0.8851\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 0.5047 Acc: 0.7917\n",
      "val Loss: 0.4269 Acc: 0.8538\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 0.5239 Acc: 0.7895\n",
      "val Loss: 0.3044 Acc: 0.9008\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 0.4880 Acc: 0.8012\n",
      "val Loss: 0.3382 Acc: 0.8916\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 0.4608 Acc: 0.8102\n",
      "val Loss: 0.3067 Acc: 0.9008\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 0.4606 Acc: 0.8186\n",
      "val Loss: 0.2587 Acc: 0.9112\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 0.4557 Acc: 0.8180\n",
      "val Loss: 0.3353 Acc: 0.8747\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 0.4507 Acc: 0.8163\n",
      "val Loss: 0.3033 Acc: 0.9034\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 0.4307 Acc: 0.8242\n",
      "val Loss: 0.2735 Acc: 0.9125\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 0.4404 Acc: 0.8287\n",
      "val Loss: 0.3261 Acc: 0.8969\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 0.4572 Acc: 0.8247\n",
      "val Loss: 0.2492 Acc: 0.9191\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 0.5160 Acc: 0.7990\n",
      "val Loss: 0.3501 Acc: 0.8956\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 0.4534 Acc: 0.8309\n",
      "val Loss: 0.2783 Acc: 0.9112\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 0.4507 Acc: 0.8270\n",
      "val Loss: 0.2800 Acc: 0.9060\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 0.4571 Acc: 0.8096\n",
      "val Loss: 0.4865 Acc: 0.8342\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 0.4275 Acc: 0.8264\n",
      "val Loss: 0.3049 Acc: 0.8916\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 0.4856 Acc: 0.8074\n",
      "val Loss: 0.2545 Acc: 0.9086\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 0.4307 Acc: 0.8281\n",
      "val Loss: 0.2680 Acc: 0.9086\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 0.4119 Acc: 0.8505\n",
      "val Loss: 0.3913 Acc: 0.8564\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 0.4564 Acc: 0.8242\n",
      "val Loss: 0.3541 Acc: 0.8695\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 0.4190 Acc: 0.8354\n",
      "val Loss: 0.3200 Acc: 0.8864\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 0.4461 Acc: 0.8203\n",
      "val Loss: 0.2815 Acc: 0.9086\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 0.4258 Acc: 0.8275\n",
      "val Loss: 0.2997 Acc: 0.9008\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 0.4348 Acc: 0.8320\n",
      "val Loss: 0.2658 Acc: 0.9164\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 0.4324 Acc: 0.8281\n",
      "val Loss: 0.3792 Acc: 0.8760\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 0.4156 Acc: 0.8331\n",
      "val Loss: 0.3627 Acc: 0.8812\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 0.4147 Acc: 0.8365\n",
      "val Loss: 0.2688 Acc: 0.9073\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 0.4607 Acc: 0.8135\n",
      "val Loss: 0.4955 Acc: 0.8329\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 0.4280 Acc: 0.8320\n",
      "val Loss: 0.2686 Acc: 0.9125\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 0.4700 Acc: 0.8231\n",
      "val Loss: 0.4907 Acc: 0.8407\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 0.4344 Acc: 0.8219\n",
      "val Loss: 1.0013 Acc: 0.6997\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 0.4398 Acc: 0.8298\n",
      "val Loss: 0.5322 Acc: 0.8381\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 0.4468 Acc: 0.8236\n",
      "val Loss: 0.2681 Acc: 0.9164\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 0.4445 Acc: 0.8354\n",
      "val Loss: 0.2517 Acc: 0.9243\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 0.4300 Acc: 0.8281\n",
      "val Loss: 0.3760 Acc: 0.8734\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 0.4265 Acc: 0.8281\n",
      "val Loss: 0.3566 Acc: 0.8799\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 0.4352 Acc: 0.8382\n",
      "val Loss: 0.2596 Acc: 0.9204\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 0.4523 Acc: 0.8169\n",
      "val Loss: 0.2966 Acc: 0.9034\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 0.4409 Acc: 0.8197\n",
      "val Loss: 0.3045 Acc: 0.8995\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 0.3975 Acc: 0.8483\n",
      "val Loss: 0.3090 Acc: 0.8930\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 0.4135 Acc: 0.8382\n",
      "val Loss: 0.3175 Acc: 0.8969\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 0.4567 Acc: 0.8247\n",
      "val Loss: 0.3047 Acc: 0.9086\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 0.3940 Acc: 0.8460\n",
      "val Loss: 0.2350 Acc: 0.9230\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 0.4523 Acc: 0.8303\n",
      "val Loss: 0.2491 Acc: 0.9178\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 0.4288 Acc: 0.8275\n",
      "val Loss: 0.2983 Acc: 0.9008\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 0.4402 Acc: 0.8292\n",
      "val Loss: 0.3389 Acc: 0.8864\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 0.3926 Acc: 0.8516\n",
      "val Loss: 0.2628 Acc: 0.9086\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 0.4285 Acc: 0.8259\n",
      "val Loss: 0.2413 Acc: 0.9217\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 0.3892 Acc: 0.8477\n",
      "val Loss: 0.3147 Acc: 0.8995\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 0.4148 Acc: 0.8354\n",
      "val Loss: 0.2896 Acc: 0.9099\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 0.4198 Acc: 0.8348\n",
      "val Loss: 0.2864 Acc: 0.9164\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 0.4509 Acc: 0.8253\n",
      "val Loss: 0.2985 Acc: 0.9060\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 0.4461 Acc: 0.8281\n",
      "val Loss: 0.2858 Acc: 0.9151\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 0.4438 Acc: 0.8264\n",
      "val Loss: 0.2608 Acc: 0.9204\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 0.3881 Acc: 0.8477\n",
      "val Loss: 0.2568 Acc: 0.9151\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 0.4242 Acc: 0.8298\n",
      "val Loss: 0.4251 Acc: 0.8486\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 0.4327 Acc: 0.8191\n",
      "val Loss: 0.2389 Acc: 0.9178\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 0.4428 Acc: 0.8236\n",
      "val Loss: 0.2817 Acc: 0.9060\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 0.3960 Acc: 0.8449\n",
      "val Loss: 0.2837 Acc: 0.9008\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 0.4235 Acc: 0.8348\n",
      "val Loss: 0.2489 Acc: 0.9151\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 0.4267 Acc: 0.8359\n",
      "val Loss: 0.2323 Acc: 0.9164\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 0.3831 Acc: 0.8561\n",
      "val Loss: 0.2610 Acc: 0.9230\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 0.4503 Acc: 0.8264\n",
      "val Loss: 0.2655 Acc: 0.9099\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 0.4056 Acc: 0.8460\n",
      "val Loss: 0.2565 Acc: 0.9191\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 0.4036 Acc: 0.8365\n",
      "val Loss: 0.3004 Acc: 0.9034\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 0.4075 Acc: 0.8382\n",
      "val Loss: 0.2758 Acc: 0.9191\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 0.4388 Acc: 0.8208\n",
      "val Loss: 0.2602 Acc: 0.9178\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 0.4209 Acc: 0.8337\n",
      "val Loss: 0.2297 Acc: 0.9230\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 0.4082 Acc: 0.8449\n",
      "val Loss: 0.2368 Acc: 0.9204\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 0.4419 Acc: 0.8292\n",
      "val Loss: 0.3110 Acc: 0.8969\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 0.3966 Acc: 0.8438\n",
      "val Loss: 0.2341 Acc: 0.9256\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 0.4260 Acc: 0.8393\n",
      "val Loss: 0.2357 Acc: 0.9243\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 0.4406 Acc: 0.8264\n",
      "val Loss: 0.3062 Acc: 0.9086\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 0.4591 Acc: 0.8225\n",
      "val Loss: 0.2499 Acc: 0.9112\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 0.4087 Acc: 0.8432\n",
      "val Loss: 0.2443 Acc: 0.9230\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 0.4472 Acc: 0.8236\n",
      "val Loss: 0.3771 Acc: 0.8747\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 0.4331 Acc: 0.8208\n",
      "val Loss: 0.2535 Acc: 0.9230\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 0.4585 Acc: 0.8208\n",
      "val Loss: 0.3336 Acc: 0.8838\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 0.4205 Acc: 0.8421\n",
      "val Loss: 0.2714 Acc: 0.9112\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 0.4228 Acc: 0.8348\n",
      "val Loss: 0.2364 Acc: 0.9282\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 0.3809 Acc: 0.8561\n",
      "val Loss: 0.2473 Acc: 0.9178\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 0.4657 Acc: 0.8298\n",
      "val Loss: 0.2538 Acc: 0.9230\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 0.3939 Acc: 0.8539\n",
      "val Loss: 0.3029 Acc: 0.9086\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 0.4170 Acc: 0.8320\n",
      "val Loss: 0.2304 Acc: 0.9256\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 0.3868 Acc: 0.8488\n",
      "val Loss: 0.4094 Acc: 0.8616\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 0.3970 Acc: 0.8522\n",
      "val Loss: 0.3222 Acc: 0.8903\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 0.4209 Acc: 0.8331\n",
      "val Loss: 0.3249 Acc: 0.9047\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 0.4042 Acc: 0.8438\n",
      "val Loss: 0.3459 Acc: 0.8903\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 0.4505 Acc: 0.8387\n",
      "val Loss: 0.4510 Acc: 0.8538\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 0.4495 Acc: 0.8203\n",
      "val Loss: 0.2712 Acc: 0.8995\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 0.4344 Acc: 0.8365\n",
      "val Loss: 0.2453 Acc: 0.9178\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 0.4161 Acc: 0.8438\n",
      "val Loss: 0.4063 Acc: 0.8708\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 0.4130 Acc: 0.8432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.3203 Acc: 0.8995\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 0.4369 Acc: 0.8292\n",
      "val Loss: 0.2254 Acc: 0.9217\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 0.4170 Acc: 0.8427\n",
      "val Loss: 0.2319 Acc: 0.9230\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 0.4291 Acc: 0.8399\n",
      "val Loss: 0.3099 Acc: 0.8982\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 0.3768 Acc: 0.8561\n",
      "val Loss: 0.2185 Acc: 0.9334\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 0.4495 Acc: 0.8309\n",
      "val Loss: 0.2529 Acc: 0.9217\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 0.4588 Acc: 0.8219\n",
      "val Loss: 0.3017 Acc: 0.9086\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 0.3888 Acc: 0.8477\n",
      "val Loss: 0.2371 Acc: 0.9230\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 0.4335 Acc: 0.8203\n",
      "val Loss: 0.2939 Acc: 0.9021\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 0.4044 Acc: 0.8410\n",
      "val Loss: 0.3466 Acc: 0.8773\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 0.4100 Acc: 0.8522\n",
      "val Loss: 0.2974 Acc: 0.9125\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 0.4217 Acc: 0.8320\n",
      "val Loss: 0.2362 Acc: 0.9178\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 0.4188 Acc: 0.8303\n",
      "val Loss: 0.3289 Acc: 0.8903\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 0.4465 Acc: 0.8259\n",
      "val Loss: 0.4426 Acc: 0.8420\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 0.3660 Acc: 0.8617\n",
      "val Loss: 0.2448 Acc: 0.9151\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 0.4494 Acc: 0.8270\n",
      "val Loss: 0.2902 Acc: 0.9151\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 0.4271 Acc: 0.8376\n",
      "val Loss: 0.3890 Acc: 0.8721\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 0.4273 Acc: 0.8371\n",
      "val Loss: 0.2528 Acc: 0.9256\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 0.4485 Acc: 0.8208\n",
      "val Loss: 0.5519 Acc: 0.8133\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 0.4264 Acc: 0.8477\n",
      "val Loss: 0.5196 Acc: 0.8277\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 0.4349 Acc: 0.8225\n",
      "val Loss: 0.3378 Acc: 0.8890\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 0.4397 Acc: 0.8309\n",
      "val Loss: 0.2919 Acc: 0.9047\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 0.4088 Acc: 0.8236\n",
      "val Loss: 0.2738 Acc: 0.9112\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 0.4121 Acc: 0.8415\n",
      "val Loss: 0.2786 Acc: 0.9151\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 0.4154 Acc: 0.8320\n",
      "val Loss: 0.3626 Acc: 0.8825\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 0.4369 Acc: 0.8281\n",
      "val Loss: 0.2732 Acc: 0.9164\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 0.4549 Acc: 0.8287\n",
      "val Loss: 0.3239 Acc: 0.8930\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 0.4294 Acc: 0.8315\n",
      "val Loss: 0.3796 Acc: 0.8851\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 0.4393 Acc: 0.8264\n",
      "val Loss: 0.2286 Acc: 0.9204\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 0.4281 Acc: 0.8376\n",
      "val Loss: 0.2920 Acc: 0.9047\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 0.4570 Acc: 0.8287\n",
      "val Loss: 0.3152 Acc: 0.8995\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 0.4069 Acc: 0.8421\n",
      "val Loss: 0.2868 Acc: 0.9125\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 0.4381 Acc: 0.8303\n",
      "val Loss: 0.4400 Acc: 0.8538\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 0.4218 Acc: 0.8359\n",
      "val Loss: 0.2894 Acc: 0.9034\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 0.4059 Acc: 0.8387\n",
      "val Loss: 0.3709 Acc: 0.8773\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 0.4166 Acc: 0.8371\n",
      "val Loss: 0.3177 Acc: 0.9060\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 0.4277 Acc: 0.8343\n",
      "val Loss: 0.2919 Acc: 0.9099\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 0.4187 Acc: 0.8359\n",
      "val Loss: 0.2325 Acc: 0.9256\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 0.4104 Acc: 0.8337\n",
      "val Loss: 0.2448 Acc: 0.9230\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 0.4763 Acc: 0.8191\n",
      "val Loss: 0.2816 Acc: 0.9125\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 0.4462 Acc: 0.8208\n",
      "val Loss: 0.2493 Acc: 0.9256\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 0.4223 Acc: 0.8287\n",
      "val Loss: 0.3384 Acc: 0.8969\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 0.4062 Acc: 0.8410\n",
      "val Loss: 0.4416 Acc: 0.8590\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 0.4700 Acc: 0.8253\n",
      "val Loss: 0.2658 Acc: 0.9047\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 0.4178 Acc: 0.8410\n",
      "val Loss: 0.3946 Acc: 0.8760\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 0.4107 Acc: 0.8365\n",
      "val Loss: 0.3217 Acc: 0.8969\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 0.4301 Acc: 0.8348\n",
      "val Loss: 0.2386 Acc: 0.9230\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 0.4431 Acc: 0.8298\n",
      "val Loss: 0.2432 Acc: 0.9230\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 0.4183 Acc: 0.8393\n",
      "val Loss: 0.2368 Acc: 0.9256\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 0.4282 Acc: 0.8326\n",
      "val Loss: 0.2348 Acc: 0.9230\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 0.4244 Acc: 0.8264\n",
      "val Loss: 0.3834 Acc: 0.8760\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 0.4116 Acc: 0.8382\n",
      "val Loss: 0.2368 Acc: 0.9204\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 0.4611 Acc: 0.8163\n",
      "val Loss: 0.2951 Acc: 0.9164\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 0.4239 Acc: 0.8432\n",
      "val Loss: 0.2634 Acc: 0.9204\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 0.4261 Acc: 0.8247\n",
      "val Loss: 0.3923 Acc: 0.8864\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 0.4188 Acc: 0.8348\n",
      "val Loss: 0.2343 Acc: 0.9256\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 0.4746 Acc: 0.8147\n",
      "val Loss: 0.2977 Acc: 0.9151\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 0.3894 Acc: 0.8567\n",
      "val Loss: 0.2504 Acc: 0.9243\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 0.4282 Acc: 0.8348\n",
      "val Loss: 0.2677 Acc: 0.9164\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 0.4391 Acc: 0.8275\n",
      "val Loss: 0.2376 Acc: 0.9204\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 0.4176 Acc: 0.8421\n",
      "val Loss: 0.3242 Acc: 0.8930\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 0.4042 Acc: 0.8421\n",
      "val Loss: 0.3514 Acc: 0.8903\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 0.4285 Acc: 0.8247\n",
      "val Loss: 0.3659 Acc: 0.8838\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 0.4154 Acc: 0.8376\n",
      "val Loss: 0.4378 Acc: 0.8577\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 0.4048 Acc: 0.8410\n",
      "val Loss: 0.5357 Acc: 0.8277\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 0.3929 Acc: 0.8455\n",
      "val Loss: 0.4286 Acc: 0.8564\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 0.4710 Acc: 0.8091\n",
      "val Loss: 0.6192 Acc: 0.8107\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 0.4293 Acc: 0.8298\n",
      "val Loss: 0.3573 Acc: 0.8890\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 0.4155 Acc: 0.8359\n",
      "val Loss: 0.2987 Acc: 0.9073\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 0.3927 Acc: 0.8488\n",
      "val Loss: 0.3715 Acc: 0.8812\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 0.4024 Acc: 0.8432\n",
      "val Loss: 0.3013 Acc: 0.9021\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 0.4630 Acc: 0.8253\n",
      "val Loss: 0.4227 Acc: 0.8708\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 0.4351 Acc: 0.8348\n",
      "val Loss: 0.2780 Acc: 0.9164\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 0.4022 Acc: 0.8421\n",
      "val Loss: 0.3969 Acc: 0.8786\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 0.4388 Acc: 0.8292\n",
      "val Loss: 0.2784 Acc: 0.9073\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 0.3954 Acc: 0.8298\n",
      "val Loss: 0.2651 Acc: 0.9099\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 0.4102 Acc: 0.8331\n",
      "val Loss: 0.2692 Acc: 0.9138\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 0.4413 Acc: 0.8259\n",
      "val Loss: 0.2828 Acc: 0.9086\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 0.4214 Acc: 0.8371\n",
      "val Loss: 0.2831 Acc: 0.9099\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 0.4494 Acc: 0.8074\n",
      "val Loss: 0.3720 Acc: 0.8773\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 0.3908 Acc: 0.8410\n",
      "val Loss: 0.2679 Acc: 0.9138\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 0.4474 Acc: 0.8326\n",
      "val Loss: 0.3677 Acc: 0.8734\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 0.4091 Acc: 0.8415\n",
      "val Loss: 0.2631 Acc: 0.9112\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 0.4369 Acc: 0.8247\n",
      "val Loss: 0.2781 Acc: 0.9178\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 0.4417 Acc: 0.8343\n",
      "val Loss: 0.2475 Acc: 0.9178\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 0.4355 Acc: 0.8303\n",
      "val Loss: 0.2495 Acc: 0.9230\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 0.3944 Acc: 0.8471\n",
      "val Loss: 0.2241 Acc: 0.9217\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 0.3955 Acc: 0.8516\n",
      "val Loss: 0.2231 Acc: 0.9243\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 0.3868 Acc: 0.8488\n",
      "val Loss: 0.4122 Acc: 0.8721\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 0.5031 Acc: 0.8024\n",
      "val Loss: 0.3247 Acc: 0.9034\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 0.3923 Acc: 0.8415\n",
      "val Loss: 0.2865 Acc: 0.9125\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 0.4224 Acc: 0.8331\n",
      "val Loss: 0.3768 Acc: 0.8838\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 0.4259 Acc: 0.8287\n",
      "val Loss: 0.2972 Acc: 0.9099\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 0.4223 Acc: 0.8365\n",
      "val Loss: 0.4610 Acc: 0.8460\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 0.3760 Acc: 0.8516\n",
      "val Loss: 0.3029 Acc: 0.9073\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 0.4125 Acc: 0.8466\n",
      "val Loss: 0.2582 Acc: 0.9164\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.4224 Acc: 0.8387\n",
      "val Loss: 0.3685 Acc: 0.8695\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 0.4207 Acc: 0.8382\n",
      "val Loss: 0.2711 Acc: 0.9099\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 0.4098 Acc: 0.8393\n",
      "val Loss: 0.2497 Acc: 0.9217\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 0.4355 Acc: 0.8376\n",
      "val Loss: 0.2687 Acc: 0.9164\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 0.4028 Acc: 0.8488\n",
      "val Loss: 0.2545 Acc: 0.9164\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 0.4589 Acc: 0.8231\n",
      "val Loss: 0.2993 Acc: 0.9073\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 0.4119 Acc: 0.8393\n",
      "val Loss: 0.4108 Acc: 0.8655\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 0.4171 Acc: 0.8354\n",
      "val Loss: 0.3119 Acc: 0.9047\n",
      "\n",
      "Training complete in 126m 10s\n",
      "Best val Acc: 0.933420\n"
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, \n",
    "                             optimizer_ft, num_epochs=num_epochs, \n",
    "                             is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
