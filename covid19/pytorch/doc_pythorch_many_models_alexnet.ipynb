{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.5.0\n",
      "Torchvision Version:  0.6.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = '/home/jimi/dissertacao/covid19/datasets/dataset_original/'\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"alexnet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 3\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 8\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 100\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 0.8002 Acc: 0.7626\n",
      "val Loss: 0.6218 Acc: 0.8244\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 0.9440 Acc: 0.7744\n",
      "val Loss: 1.8850 Acc: 0.7421\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.8969 Acc: 0.7879\n",
      "val Loss: 0.4048 Acc: 0.9098\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.7726 Acc: 0.8029\n",
      "val Loss: 0.4402 Acc: 0.8956\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.7789 Acc: 0.8045\n",
      "val Loss: 0.6625 Acc: 0.8829\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.9008 Acc: 0.8108\n",
      "val Loss: 0.4128 Acc: 0.9003\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.7765 Acc: 0.8263\n",
      "val Loss: 0.3703 Acc: 0.9051\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.8122 Acc: 0.8116\n",
      "val Loss: 0.2979 Acc: 0.9193\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.9301 Acc: 0.8069\n",
      "val Loss: 0.3714 Acc: 0.9098\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.9884 Acc: 0.7986\n",
      "val Loss: 0.5029 Acc: 0.8813\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.8038 Acc: 0.8203\n",
      "val Loss: 0.3722 Acc: 0.9241\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.8949 Acc: 0.8065\n",
      "val Loss: 0.3193 Acc: 0.9098\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.8518 Acc: 0.8152\n",
      "val Loss: 0.6085 Acc: 0.8924\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.8725 Acc: 0.8041\n",
      "val Loss: 0.5222 Acc: 0.8908\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.8348 Acc: 0.8152\n",
      "val Loss: 0.6725 Acc: 0.8829\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 1.0424 Acc: 0.8069\n",
      "val Loss: 0.5726 Acc: 0.9035\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.9648 Acc: 0.8037\n",
      "val Loss: 0.3846 Acc: 0.9209\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.8764 Acc: 0.8172\n",
      "val Loss: 0.5902 Acc: 0.9019\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.7374 Acc: 0.8326\n",
      "val Loss: 0.5948 Acc: 0.8845\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.9278 Acc: 0.7998\n",
      "val Loss: 0.9358 Acc: 0.8608\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 0.7866 Acc: 0.8287\n",
      "val Loss: 1.1042 Acc: 0.7753\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.7527 Acc: 0.8314\n",
      "val Loss: 0.5715 Acc: 0.8892\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.9193 Acc: 0.8089\n",
      "val Loss: 0.8495 Acc: 0.8655\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.7603 Acc: 0.8346\n",
      "val Loss: 0.9054 Acc: 0.8434\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.8489 Acc: 0.8207\n",
      "val Loss: 0.4581 Acc: 0.9256\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.7313 Acc: 0.8263\n",
      "val Loss: 0.5068 Acc: 0.8956\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.8734 Acc: 0.8128\n",
      "val Loss: 0.3100 Acc: 0.9288\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.9626 Acc: 0.8160\n",
      "val Loss: 1.0339 Acc: 0.7927\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 1.0531 Acc: 0.8124\n",
      "val Loss: 0.6077 Acc: 0.8940\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.7720 Acc: 0.8350\n",
      "val Loss: 0.3870 Acc: 0.9019\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 0.7949 Acc: 0.8318\n",
      "val Loss: 0.5982 Acc: 0.8956\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.9050 Acc: 0.8168\n",
      "val Loss: 0.6404 Acc: 0.8861\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.9288 Acc: 0.8207\n",
      "val Loss: 1.0102 Acc: 0.8560\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 1.0638 Acc: 0.8033\n",
      "val Loss: 0.5479 Acc: 0.8861\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 1.0201 Acc: 0.8207\n",
      "val Loss: 0.3305 Acc: 0.9193\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.8866 Acc: 0.8195\n",
      "val Loss: 0.5680 Acc: 0.8861\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.8524 Acc: 0.8247\n",
      "val Loss: 0.9591 Acc: 0.8133\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 1.0486 Acc: 0.8231\n",
      "val Loss: 0.6061 Acc: 0.9003\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.9447 Acc: 0.8203\n",
      "val Loss: 0.7243 Acc: 0.8845\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.7126 Acc: 0.8472\n",
      "val Loss: 0.6509 Acc: 0.8940\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 0.8144 Acc: 0.8255\n",
      "val Loss: 1.6827 Acc: 0.7120\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.9133 Acc: 0.8219\n",
      "val Loss: 0.6077 Acc: 0.9051\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.8338 Acc: 0.8199\n",
      "val Loss: 0.4087 Acc: 0.9209\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.8449 Acc: 0.8374\n",
      "val Loss: 0.4088 Acc: 0.9241\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.8523 Acc: 0.8243\n",
      "val Loss: 1.1018 Acc: 0.8592\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 1.0347 Acc: 0.8160\n",
      "val Loss: 0.8330 Acc: 0.8845\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.7358 Acc: 0.8433\n",
      "val Loss: 0.7447 Acc: 0.8734\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.8854 Acc: 0.8199\n",
      "val Loss: 0.6396 Acc: 0.8924\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.7280 Acc: 0.8441\n",
      "val Loss: 0.3928 Acc: 0.9225\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 0.7849 Acc: 0.8389\n",
      "val Loss: 0.4942 Acc: 0.9066\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 0.8695 Acc: 0.8374\n",
      "val Loss: 0.4245 Acc: 0.9098\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 0.7643 Acc: 0.8259\n",
      "val Loss: 0.9203 Acc: 0.8544\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 0.7744 Acc: 0.8378\n",
      "val Loss: 0.3572 Acc: 0.9098\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 0.9223 Acc: 0.8180\n",
      "val Loss: 0.7899 Acc: 0.8829\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 0.8418 Acc: 0.8219\n",
      "val Loss: 0.9904 Acc: 0.8576\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 0.7805 Acc: 0.8290\n",
      "val Loss: 0.4038 Acc: 0.9114\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 0.8760 Acc: 0.8271\n",
      "val Loss: 0.3809 Acc: 0.9161\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 0.9640 Acc: 0.8195\n",
      "val Loss: 0.4471 Acc: 0.8845\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 0.9077 Acc: 0.8247\n",
      "val Loss: 0.8078 Acc: 0.9003\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 0.9008 Acc: 0.8211\n",
      "val Loss: 0.4193 Acc: 0.9098\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 0.8371 Acc: 0.8370\n",
      "val Loss: 0.7603 Acc: 0.8797\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 0.8229 Acc: 0.8358\n",
      "val Loss: 1.3792 Acc: 0.8149\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 0.8877 Acc: 0.8195\n",
      "val Loss: 2.3567 Acc: 0.7310\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 1.0946 Acc: 0.8041\n",
      "val Loss: 0.4566 Acc: 0.8972\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 0.9813 Acc: 0.8184\n",
      "val Loss: 1.0846 Acc: 0.8481\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 0.9457 Acc: 0.8271\n",
      "val Loss: 0.4046 Acc: 0.9209\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 0.7850 Acc: 0.8310\n",
      "val Loss: 0.7054 Acc: 0.8813\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 0.8120 Acc: 0.8366\n",
      "val Loss: 0.5848 Acc: 0.9066\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 0.8527 Acc: 0.8251\n",
      "val Loss: 1.5389 Acc: 0.8228\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 0.8792 Acc: 0.8227\n",
      "val Loss: 0.5166 Acc: 0.9177\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 0.8823 Acc: 0.8366\n",
      "val Loss: 1.6037 Acc: 0.7737\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 0.9051 Acc: 0.8350\n",
      "val Loss: 0.3680 Acc: 0.9098\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 0.8085 Acc: 0.8298\n",
      "val Loss: 1.1691 Acc: 0.8386\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 0.9592 Acc: 0.8302\n",
      "val Loss: 0.4801 Acc: 0.9177\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 0.9773 Acc: 0.8314\n",
      "val Loss: 0.9477 Acc: 0.8513\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 0.8005 Acc: 0.8397\n",
      "val Loss: 0.5733 Acc: 0.8987\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 0.7365 Acc: 0.8433\n",
      "val Loss: 0.4196 Acc: 0.8782\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 0.9412 Acc: 0.8195\n",
      "val Loss: 0.9621 Acc: 0.8560\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 0.7394 Acc: 0.8445\n",
      "val Loss: 0.9173 Acc: 0.8750\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 0.7753 Acc: 0.8338\n",
      "val Loss: 0.9550 Acc: 0.8734\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 0.9148 Acc: 0.8259\n",
      "val Loss: 0.4753 Acc: 0.9082\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 0.6918 Acc: 0.8338\n",
      "val Loss: 0.4066 Acc: 0.9241\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 0.8390 Acc: 0.8298\n",
      "val Loss: 0.6969 Acc: 0.8623\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 0.9212 Acc: 0.8342\n",
      "val Loss: 0.6026 Acc: 0.8908\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 0.7576 Acc: 0.8374\n",
      "val Loss: 0.3337 Acc: 0.9256\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 0.7682 Acc: 0.8271\n",
      "val Loss: 0.2651 Acc: 0.9320\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 0.8818 Acc: 0.8235\n",
      "val Loss: 0.3606 Acc: 0.9177\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 0.7762 Acc: 0.8318\n",
      "val Loss: 0.5033 Acc: 0.8718\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 0.6765 Acc: 0.8480\n",
      "val Loss: 0.6502 Acc: 0.9066\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 0.7900 Acc: 0.8247\n",
      "val Loss: 0.2744 Acc: 0.9272\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 0.7875 Acc: 0.8342\n",
      "val Loss: 0.3082 Acc: 0.9256\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 0.7038 Acc: 0.8370\n",
      "val Loss: 0.6550 Acc: 0.8861\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 0.9428 Acc: 0.8184\n",
      "val Loss: 0.4362 Acc: 0.9019\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 0.8089 Acc: 0.8318\n",
      "val Loss: 0.8290 Acc: 0.8829\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 0.8311 Acc: 0.8314\n",
      "val Loss: 0.3785 Acc: 0.9177\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 0.8736 Acc: 0.8263\n",
      "val Loss: 0.4222 Acc: 0.9130\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 0.8814 Acc: 0.8290\n",
      "val Loss: 0.3513 Acc: 0.9209\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 0.8551 Acc: 0.8310\n",
      "val Loss: 1.6331 Acc: 0.8149\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.8051 Acc: 0.8469\n",
      "val Loss: 1.6051 Acc: 0.7959\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 0.7606 Acc: 0.8405\n",
      "val Loss: 0.5331 Acc: 0.8877\n",
      "\n",
      "Training complete in 121m 47s\n",
      "Best val Acc: 0.931962\n"
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, \n",
    "                             optimizer_ft, num_epochs=num_epochs, \n",
    "                             is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
